node_1|node_2|edge|chunk_id|node_3|node_4|level
artificial intelligence|european commission|The proposal for a Regulation laying down harmonised rules on artificial intelligence is initiated by the EUROPEAN COMMISSION, which recognizes the economic and societal benefits that Artificial Intelligence can bring across various industries and social activities.|a9ac3ec195684dd7925e593b470a6cf0|||
artificial intelligence|high-impact sectors|The use of artificial intelligence is especially needed in high-impact sectors, such as climate change, environment and health, the public sector, finance, mobility, home affairs and agriculture.|a9ac3ec195684dd7925e593b470a6cf0|||
ai technologies|socially and environmentally beneficial outcomes|The same elements and techniques that power the socio-economic benefits of AI can also bring about socially and environmentally beneficial outcomes, especially in high-impact sectors.|a9ac3ec195684dd7925e593b470a6cf0|||
european commission|white paper on artificial intelligence - a european approach to excellence and trust|The European Commission published a white paper on artificial intelligence that focuses on excellence and trust, as requested by the European Parliament.|680e602964f548e0b4c02964d07d4586|||
european council|white paper on artificial intelligence - a european approach to excellence and trust|The European Council called for a 'sense of urgency' to address emerging trends, including issues such as artificial intelligence, while ensuring a high level of data protection, digital rights, and ethical standards.|680e602964f548e0b4c02964d07d4586|||
european parliament|white paper on artificial intelligence - a european approach to excellence and trust|The European Parliament specifically requested that the European Commission publish a white paper on artificial intelligence, with a focus on excellence and trust.|680e602964f548e0b4c02964d07d4586|||
european council|review of existing relevant legislation|In its 2019 Conclusions on the Coordinated Plan on the development and use of artificial intelligence Made in Europe, the European Council called for a review of the existing relevant legislation to make it fit for purpose for the new opportunities and challenges raised by AI.|680e602964f548e0b4c02964d07d4586|||
european council|determination of high-risk ai applications|The European Council called for a clear determination of the AI applications that should be considered high-risk.|680e602964f548e0b4c02964d07d4586|||
european parliament|ethical principles protection|The European Parliament requested that the European Commission ensure the protection of ethical principles while addressing emerging trends, including issues such as artificial intelligence.|680e602964f548e0b4c02964d07d4586|||
european council|opacity, complexity, bias, and partially autonomous behaviour of ai|In its most recent Conclusions from 21 October 2020, the European Council called for addressing the opacity, complexity, bias, and partially autonomous behaviour of artificial intelligence.|680e602964f548e0b4c02964d07d4586|||
compatibility with fundamental rights|ai systems|certain AI systems require compatibility with fundamental rights to ensure their compatibility with fundamental rights and facilitate the enforcement of legal rules|054628088e3a46899d3d2ef31e119b9b|||
european parliament|ai|The European Parliament has adopted a number of resolutions related to AI, including on ethics, liability, copyright, AI in criminal matters and AI in education, culture and the audio-visual sector.|054628088e3a46899d3d2ef31e119b9b|||
commission|legislative action|The EP Resolution on a Framework of Ethical Aspects of Artificial Intelligence, Robotics and Related Technologies recommends to the Commission to propose legislative action to harness the opportunities and benefits of AI, but also to ensure protection of ethical principles.|054628088e3a46899d3d2ef31e119b9b|||
european parliament resolution of 20 october 2020 with recommendations to the commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020/2012(inl)|council of the european union, artificial intelligence b) conclusions on the coordinated plan on artificial intelligence-adoption 6177/19, 2019|The Council's conclusions on the coordinated plan for AI and the European Parliament's recommendations are related as they both discuss the development and implementation of AI technology.|7902fe386ba54b2fa2e1b4351c5cfc55|||
european parliament resolution of 20 october 2020 with recommendations to the commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020/2012(inl)|european council, special meeting of the european council (1and 2 october 2020) ­ conclusions euco 13/20, 2020|The conclusions from the European Council's special meeting and the recommendations made by the European Parliament both relate to the development of AI and its impact on society.|7902fe386ba54b2fa2e1b4351c5cfc55|||
european parliament resolution of 20 october 2020 with recommendations to the commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020/2012(inl)|european council, european council meeting (19 october 2017) ­ conclusion euco 14/17, 2017|The European Council's conclusions from their meeting in 2017 and the European Parliament's recommendations both touch upon the development of AI technology.|7902fe386ba54b2fa2e1b4351c5cfc55|||
european parliament resolution of 20 october 2020 with recommendations to the commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020/2012(inl)|council of the european union, presidency conclusions - the charter of fundamental rights in the context of artificial intelligence and digital change, 11481/20, 2020|The Council's presidency conclusions regarding AI and the protection of fundamental rights are related to the European Parliament's recommendations for a framework of ethical aspects of AI.|7902fe386ba54b2fa2e1b4351c5cfc55|||
european parliament resolution of 20 october 2020 with recommendations to the commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020/2012(inl)|european parliament resolution of 20 october 2020 on intellectual property rights for the development of artificial intelligence technologies, 2020/2015(ini)|The European Parliament's recommendations regarding IP rights and AI are related as they both discuss the legal framework surrounding the development of this technology.|7902fe386ba54b2fa2e1b4351c5cfc55|||
european parliament resolution of 20 october 2020 with recommendations to the commission on a framework of ethical aspects of artificial intelligence, robotics and related technologies, 2020/2012(inl)|european parliament draft report, artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters, 2020/2016(ini)|The European Parliament's recommendations regarding AI in criminal law and the draft report on this topic are related as they both address the potential impact of AI technology on the justice system.|7902fe386ba54b2fa2e1b4351c5cfc55|||
ai systems|lawful, safe and trustworthy ai applications|This proposal aims to facilitate the development of a single market for lawful, safe and trustworthy AI applications by ensuring legal certainty for investors and innovators, preventing market fragmentation, and enhancing governance and effective enforcement of existing law on fundamental rights and safety requirements applicable to AI systems.|6ec1993cb6834efb971aca75a23413c4|||
ai systems|existing law on fundamental rights and union values|This proposal ensures that AI systems placed on the Union market and used are safe and respect existing law on fundamental rights and Union values.|6ec1993cb6834efb971aca75a23413c4|||
proposal|european parliament draft report|The proposed regulatory framework on Artificial Intelligence takes into account the aforementioned resolution of the European Parliament in full respect of proportionality, subsidiarity and better law making principles.|6ec1993cb6834efb971aca75a23413c4|||
digital education action plan|ethical guidelines in ai and data usage in education|The Commission Communication COM(2020) 624 final foresees the development of ethical guidelines in AI and Data usage in education as part of the Digital Education Action Plan 2021-2027: Resetting education and training for the digital age.|6ec1993cb6834efb971aca75a23413c4|||
commission communication|european parliament draft report|The Commission Communication COM(2020) 624 final takes into account the aforementioned resolution of the European Parliament in full respect of proportionality, subsidiarity and better law making principles.|6ec1993cb6834efb971aca75a23413c4|||
proposal|ai systems placed on the union market and used|This proposal ensures that AI systems placed on the Union market and used are safe and respect existing law on fundamental rights and Union values.|6ec1993cb6834efb971aca75a23413c4|||
proposal|single market for lawful, safe and trustworthy ai applications|This proposal aims to facilitate the development of a single market for lawful, safe and trustworthy AI applications by ensuring legal certainty for investors and innovators, preventing market fragmentation, and enhancing governance and effective enforcement of existing law on fundamental rights and safety requirements applicable to AI systems.|6ec1993cb6834efb971aca75a23413c4|||
ai systems|harmonised rules for development, placement on the market and use of ai systems|proposal presents a balanced and proportionate horizontal regulatory approach to AI that sets harmonized rules for the development, placement on the market and use of AI systems in the Union following a proportional risk-based approach.|9cb733692b2f49dba4696c5f6dc1754e|||
ai systems|prohibited ai practices|the proposal sets harmonised rules for the development, placement on the market and use of AI systems in the Union following a proportional risk-based approach. Certain particularly harmful AI practices are prohibited as|9cb733692b2f49dba4696c5f6dc1754e|||
ai systems|ai practices linked to risks and problems|the proposal sets a robust and flexible legal framework. On the one hand, it is comprehensive and future-proof in its fundamental regulatory choices, including the principle-based requirements that AI systems should comply with. On the other hand, it puts in place a proportionate regulatory system centred on a well-defined risk-based regulatory approach that does not create unnecessary restrictions to trade, whereby legal intervention is tailored to those concrete situations where there is a justified cause for concern or where such concern can reasonably be anticipated in the near future.|9cb733692b2f49dba4696c5f6dc1754e|||
ai systems|market fragmentation|applications and prevent market fragmentation.|9cb733692b2f49dba4696c5f6dc1754e|||
ai practices|union values|certain particularly harmful AI practices are prohibited as contravening Union values|326ddb07962f4bd2a4814e9b41c94002|||
high-risk ai systems|significant risks to the health and safety or fundamental rights of persons|defined by a solid risk methodology in the proposal|326ddb07962f4bd2a4814e9b41c94002|||
horizontal mandatory requirements for trustworthy ai|high-risk ai systems|have to comply with before being placed on the Union market|326ddb07962f4bd2a4814e9b41c94002|||
conformity assessment procedures|high-risk ai systems|before being placed on the Union market|326ddb07962f4bd2a4814e9b41c94002|||
providers and users of those systems|safety and respect of existing legislation protecting fundamental rights throughout the whole ai systems' lifecycle|ensured by predictable, proportionate and clear obligations proposed in the proposal|326ddb07962f4bd2a4814e9b41c94002|||
specific ai systems|only minimum transparency obligations are proposed|in particular when chatbots or `deep fakes' are used|326ddb07962f4bd2a4814e9b41c94002|||
ai regulatory sandboxes|additional measures proposed to support innovation|-|326ddb07962f4bd2a4814e9b41c94002|||
union level|establishment of a european artificial intelligence board|-|326ddb07962f4bd2a4814e9b41c94002|||
small and medium-sized enterprises (smes)|consistency with existing policy provisions|In the proposal, measures to reduce regulatory burden for SMEs and start-ups are consistent with existing Union legislation applicable to sectors where high-risk AI systems are used or likely to be used in the near future. This consistency ensures full harmony with existing secondary Union legislation on data protection, consumer protection, non-discrimination, and gender equality.|49b2e8dc205848d4ac4ca19b9563aa56|||
consistency with existing policy provisions|the eu charter of fundamental rights|The proposal ensures consistency with the EU Charter of Fundamental Rights, which is an existing secondary Union legislation.|49b2e8dc205848d4ac4ca19b9563aa56|||
consistency with existing policy provisions|general data protection regulation (regulation (eu) 2016/679)|The proposal is without prejudice and complements the General Data Protection Regulation, which is an existing secondary Union legislation on data protection.|49b2e8dc205848d4ac4ca19b9563aa56|||
consistency with existing policy provisions|law enforcement directive (directive (eu) 2016/680)|The proposal is without prejudice and complements the Law Enforcement Directive, which is an existing secondary Union legislation on data protection.|49b2e8dc205848d4ac4ca19b9563aa56|||
consistency with existing policy provisions|high-risk ai systems|The proposal ensures consistency with existing Union legislation applicable to sectors where high-risk AI systems are used or likely to be used in the near future.|49b2e8dc205848d4ac4ca19b9563aa56|||
consistency with existing policy provisions|existing union law on non-discrimination|The proposal complements existing Union law on non-discrimination with specific requirements that aim to minimise the risk of algorithmic discrimination, in particular in relation to the design and the quality of data sets used for the development of AI systems.|49b2e8dc205848d4ac4ca19b9563aa56|||
high-risk ai systems which are safety components of products|union competition law|without prejudice to the application of Union competition law|c8adc3476bc54958a8d866add4bf73f4|||
high-risk ai systems related to products covered by the new legislative framework (nlf) legislation|requirements for ai systems set out in this proposal|will be checked as part of the existing conformity assessment procedures under the relevant NLF legislation|c8adc3476bc54958a8d866add4bf73f4|||
safety risks specific to ai systems|requirements for ai systems set out in this proposal|meant to be covered by the requirements of this proposal|c8adc3476bc54958a8d866add4bf73f4|||
overall safety of the final product|nlf legislation|aims at ensuring|c8adc3476bc54958a8d866add4bf73f4|||
high-risk ai systems related to products covered by relevant old approach legislation (e.g. aviation, cars)|this proposal would not directly apply||c8adc3476bc54958a8d866add4bf73f4|||
ex-ante essential requirements for high-risk ai systems|union's financial services legislation|should be taken into account when adopting relevant implementing or delegated legislation under those acts.|8232cd735e644d41a5a3272a7a6a7ef7|||
authorities responsible for the supervision of the union's financial services legislation|competent authorities for supervising the requirements in this proposal to ensure a coherent enforcement of the obligations under this proposal and the union's financial services legislation where ai systems are implicitly regulated||8232cd735e644d41a5a3272a7a6a7ef7|||
conformity assessment procedure|procedural obligations under this proposal are integrated into the procedures under directive 2013/36/eu on access to the activity of credit institutions and the prudential supervision||8232cd735e644d41a5a3272a7a6a7ef7|||
directive 2013/36/eu|credit institutions and investment firms, amending directive 2002/87/ec and repealing directives 2006/48/ec and 2006/49/ec text with eea relevance||8232cd735e644d41a5a3272a7a6a7ef7|||
ai systems|eu-lisa managed ai systems|The proposal will not apply to eu-LISA managed AI systems that have been placed on the market or put into service before one year has elapsed from the date of application of this Regulation, unless the replacement or amendment of those legal acts leads to a significant change in the design or intended purpose of the AI system or AI systems concerned.|482a5b2f5f514568827c2101c4975257|||
ai systems|white paper on ai|The proposal is part of a wider comprehensive package of measures that address problems posed by the development and use of AI, as examined in the White Paper on AI.|482a5b2f5f514568827c2101c4975257|||
ai systems|sectoral product legislation|Consistency is therefore ensured with other ongoing or planned initiatives of the Commission that also aim to address those problems, including the revision of sectoral product legislation (e.g. The Machinery Directive, the General Product Safety Directive).|482a5b2f5f514568827c2101c4975257|||
proposal|union legislation on services|The proposal is also consistent with the applicable Union legislation on services, including on intermediary services regulated by the e-Commerce Directive 2000/31/EC and the Commission's recent proposal for the Digital Services Act (DSA).|482a5b2f5f514568827c2101c4975257|||
ai systems|initiatives addressing liability issues related to new technologies|The proposal is coherent with the Commission's overall digital strategy in its contribution to promoting technology that works for people, one of the three main pillars of policy orientation and objectives announced in the Communication `Shaping Europe's digital future'. These initiatives will build on and complement this proposal in order to bring legal clarity and foster the development of an ecosystem of trust in AI in Europe.|7e5e4987e1054c5c90b2c2be5a712b31|||
ai systems|product legislation (e.g. the machinery directive, the general product safety directive)|The proposal is coherent with the Commission's overall digital strategy in its contribution to promoting technology that works for people, one of the three main pillars of policy orientation and objectives announced in the Communication `Shaping Europe's digital future'. This proposal ensures AI is developed in ways that respect people's rights and earn their trust.|7e5e4987e1054c5c90b2c2be5a712b31|||
ai systems|union's role to help shape global norms and standards|The proposal strengthens significantly the Union's role to help shape global norms and standards and promote trustworthy AI that is consistent with Union values and interests.|7e5e4987e1054c5c90b2c2be5a712b31|||
ai systems|data governance act|The promotion of AI-driven innovation is closely linked to the Data Governance Act, which will establish trusted mechanisms and services for the re-use, sharing and pooling of data that are essential for the development of data-driven AI models of high quality.|7e5e4987e1054c5c90b2c2be5a712b31|||
ai systems|open data directive|The promotion of AI-driven innovation is closely linked to other initiatives under the EU strategy for data, which will establish trusted mechanisms and services for the re-use, sharing and pooling of data that are essential for the development of data-driven AI models of high quality.|7e5e4987e1054c5c90b2c2be5a712b31|||
ai|union values and interests|standards and promote trustworthy AI that is consistent with Union values and interests|bc8b80c539734c3195717af91be4247d|||
directive on electronic commerce|proposal for a regulation of the european parliament and of the council on a single market for digital services (digital services act) and amending directive 2000/31/ec|See Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL on a Single Market For Digital Services (Digital Services Act) and amending Directive 2000/31/EC COM/2020/825 final.|bc8b80c539734c3195717af91be4247d|||
communication from the commission, shaping europe's digital future, com/2020/67 final.|2030 digital compass: the european way for the digital decade.|Communication from the Commission, Shaping Europe's Digital Future, COM/2020/67 final.|bc8b80c539734c3195717af91be4247d|||
proposal for a regulation on european data governance (data governance act) com/2020/767|directive (eu) 2019/1024 of the european parliament and of the council of 20 june 2019 on open data and the re-use of public sector information, pe/28/2019/rev/1|Proposal for a Regulation on European data governance (Data Governance Act) COM/2020/767.|bc8b80c539734c3195717af91be4247d|||
commission communication, a european strategy for data com/2020/66 final.|directive (eu) 2019/1024 of the european parliament and of the council of 20 june 2019 on open data and the re-use of public sector information, pe/28/2019/rev/1|Commission Communication, A European strategy for data COM/2020/66 final.|bc8b80c539734c3195717af91be4247d|||
article 114 tfeu|eu digital single market strategy|The legal basis for this proposal is primarily Article 114 TFEU, which provides for the adoption of measures to ensure the establishment and functioning of the internal market. This proposal is a core part of the EU digital single market strategy.|22a8c8b3b3be4c3db0c0d523e9b0c746|||
eu digital single market strategy|proper functioning of the internal market|The primary objective of this proposal is to ensure the proper functioning of the internal market by setting harmonised rules for AI technologies and systems.|22a8c8b3b3be4c3db0c0d523e9b0c746|||
member states|national rules|Some Member States are already considering national rules to ensure that AI is safe and developed and used in compliance with fundamental rights obligations.|22a8c8b3b3be4c3db0c0d523e9b0c746|||
ai products and services|fragmentation of the internal market|This will likely lead to a fragmentation of the internal market on essential elements regarding the requirements for AI products and services, their marketing, their use, the liability and the supervision by public authorities.|22a8c8b3b3be4c3db0c0d523e9b0c746|||
ai products and services|legal certainty|Given the wide circulation of products and services across borders, these two problems can be best solved through EU harmonizing legislation to ensure legal certainty for both providers and users of AI systems on how existing and new rules will apply to those systems in the Union.|22a8c8b3b3be4c3db0c0d523e9b0c746|||
ai systems|eu harmonizing legislation|The proposal defines common mandatory requirements applicable to the design and development of certain AI systems before they are placed on the market that will be further operationalised through harmonised technical standards. This highlights the relationship between AI systems and EU harmonizing legislation in addressing the two problems of cross-border design and development of AI systems.|570fd013c7e64adf8ea9138239c880ac|||
ai systems|harmonized technical standards|The proposal operationalizes common mandatory requirements applicable to the design and development of certain AI systems through harmonised technical standards. This indicates that harmonized technical standards are necessary for the effective implementation of common mandatory requirements for AI systems.|570fd013c7e64adf8ea9138239c880ac|||
ai systems|seamless circulation of products and services across the eu|An emerging patchwork of potentially divergent national rules will hamper the seamless circulation of products and services related to AI systems across the EU. This suggests that the lack of harmonized national rules may negatively impact the free movement of AI-related products and services within the EU.|570fd013c7e64adf8ea9138239c880ac|||
ai systems|non-exclusive competence|The nature of AI, which often relies on large and varied datasets and which may be embedded in any product or service circulating freely within the internal market, entails that the objectives of this proposal cannot be effectively achieved by Member States alone. This implies that the competence to regulate AI systems falls outside the exclusive sphere of Member States.|570fd013c7e64adf8ea9138239c880ac|||
ai systems|tfeu article 16|Considering that this proposal contains certain specific rules on the protection of individuals with regard to the processing of personal data, notably restrictions of the use of AI systems for `real-time' remote biometric identification in publicly accessible spaces for the purpose of law enforcement, it is appropriate to base this regulation, in as far as those specific rules are concerned, on Article 16 of the TFEU.|570fd013c7e64adf8ea9138239c880ac|||
ai systems|single market|The objectives of this proposal can be better achieved at Union level to avoid a further fragmentation of the Single Market into potentially contradictory national frameworks preventing the free circulation of goods and services embedding AI.|f0546d788db8404f846155193afaddc1|||
high risks to fundamental rights and safety|regulatory burdens|The proposal builds on existing legal frameworks and is proportionate and necessary to achieve its objectives, since it follows a risk-based approach and imposes regulatory burdens only when an AI system is likely to pose high risks to fundamental rights and safety.|f0546d788db8404f846155193afaddc1|||
non-high-risk ai systems|limited transparency obligations|For other, non-high-risk AI systems, only very limited transparency obligations are imposed,|f0546d788db8404f846155193afaddc1|||
ai systems|high-risk ai systems|For high-risk AI systems, the requirements of high quality data, documentation and traceability, transparency, human oversight, accuracy and robustness, are strictly necessary to mitigate the risks to fundamental rights and safety posed by AI and that are not covered by other existing legal frameworks.|89ae8e5826e04c38a0480ffd256ff5f1|||
ai systems|other, non-high-risk ai systems|Only very limited transparency obligations are imposed for these systems, as indicated in terms of the provision of information to flag the use of an AI system when interacting with humans.|89ae8e5826e04c38a0480ffd256ff5f1|||
ai systems|prohibition of certain harmful ai-enabled practices|This proposal aims to prohibit certain harmful AI-enabled practices, although the specifics are not explicitly mentioned in this context.|89ae8e5826e04c38a0480ffd256ff5f1|||
definition of ai|ai systems|The proposed regulation will introduce a harmonized set of core requirements with regard to AI systems, which will include a definition of AI as outlined in this context.|89ae8e5826e04c38a0480ffd256ff5f1|||
lawful, safe and trustworthy ai systems|ai systems|The proposed regulation aims to introduce core requirements for lawful, safe and trustworthy AI systems as highlighted in this context.|89ae8e5826e04c38a0480ffd256ff5f1|||
single market for lawful, safe and trustworthy ai systems|ai systems|The proposed regulation will facilitate the development of a single market for lawful, safe and trustworthy AI systems by introducing a harmonized set of core requirements as noted in this context.|89ae8e5826e04c38a0480ffd256ff5f1|||
direct applicability|regulation|The proposed regulation will have direct applicability, which is in accordance with Article 288 TFEU, as mentioned in this context.|89ae8e5826e04c38a0480ffd256ff5f1|||
legal instrument|choice of the instrument|The proposed regulation will be implemented through a specific legal instrument known as the choice of the instrument, which is discussed in this context.|89ae8e5826e04c38a0480ffd256ff5f1|||
consultation results|stakeholders|stakeholders provided 1215 contributions, including responses from commercial and non-commercial organizations, social partners, experts, academics, citizens, research institutions, public authorities, consumer's organizations, non-governmental organizations, trade unions, and business associations. The consultation results are available on the Commission's website.|25802a41e26944979f0f70e804d556f0|||
business and industry representatives|eu-27|84% of the 352 business and industry replies came from the EU-27. The rest were business associations.|25802a41e26944979f0f70e804d556f0|||
micro, small, and medium-sized enterprises|business and industry representatives|222 of the 352 business and industry representatives were companies and business representatives, 41.5% of which were micro, small, and medium-sized enterprises.|25802a41e26944979f0f70e804d556f0|||
en|academic/research institutions|EN had 7 academic/research institutions as respondents.|25802a41e26944979f0f70e804d556f0|||
trade unions|civil society's voices|Civil society's voices were represented by 160 respondents, including 22 trade unions.|25802a41e26944979f0f70e804d556f0|||
citizens|individuals|406 responses came from individuals, of which 92% were from the EU.|25802a41e26944979f0f70e804d556f0|||
free text option|respondents|Between 81 and 598 of the respondents used the free text option to insert comments.|25802a41e26944979f0f70e804d556f0|||
position papers|respondents|Over 450 position papers were submitted through the EU Survey website, either in addition to questionnaire answers or as stand-alone contributions.|25802a41e26944979f0f70e804d556f0|||
eu survey website|position papers|Position papers were submitted through the EU Survey website.|25802a41e26944979f0f70e804d556f0|||
need for action|legislative gaps|Overall, there is a general agreement amongst stakeholders on a need for action and several stakeholders warn the Commission to avoid duplication, conflicting obligations and overregulation. A large majority of stakeholders agree that legislative gaps exist or that new legislation is needed.|7ab4ef56578848099e6fff292e96498c|||
need for action|new legislation|Overall, there is a general agreement amongst stakeholders on a need for action and a large majority of stakeholders agree that legislative gaps exist or that new legislation is needed.|7ab4ef56578848099e6fff292e96498c|||
need for action|ai|Stakeholders mostly requested a narrow, clear and precise definition for AI.|7ab4ef56578848099e6fff292e96498c|||
risk|high-risk|Most of the respondents are explicitly in favour of the risk-based approach. Using a risk-based framework was considered a better option than blanket regulation of all AI systems.|7ab4ef56578848099e6fff292e96498c|||
harm|high-risk|Risks also should be calculated taking into account the impact on rights and safety.|7ab4ef56578848099e6fff292e96498c|||
low-risk|ai|Stakeholders mostly requested a narrow, clear and precise definition for AI.|7ab4ef56578848099e6fff292e96498c|||
remote biometric identification|ai|Most of the respondents are explicitly in favour of the risk-based approach. Using a risk-based framework was considered a better option than blanket regulation of all AI systems.|7ab4ef56578848099e6fff292e96498c|||
enforcement models|ex-ante risk self-assessment|More than 50%, especially from the business associations, were in favour of a combination of an ex-ante risk self-assessment and an ex-post enforcement for high-risk AI systems.|7ab4ef56578848099e6fff292e96498c|||
enforcement models|ex-post enforcement|More than 50%, especially from the business associations, were in favour of a combination of an ex-ante risk self-assessment and an ex-post enforcement for high-risk AI systems.|7ab4ef56578848099e6fff292e96498c|||
enforcement models|legislative gaps|Several stakeholders warn the Commission to avoid duplication, conflicting obligations and overregulation.|7ab4ef56578848099e6fff292e96498c|||
ai|technology neutral|Stakeholders also highlighted that besides the clarification of the term of AI, it is important to define `risk', `high-risk', `low-risk', `remote biometric identification' and `harm'. Most of the respondents are explicitly in favour of the risk-based approach. Using a risk-based framework was considered a better option than blanket regulation of all AI systems.|7ab4ef56578848099e6fff292e96498c|||
ai|proportionate regulatory framework|Stakeholders also highlighted that besides the clarification of the term of AI, it is important to define `risk', `high-risk', `low-risk', `remote biometric identification' and `harm'. Most of the respondents are explicitly in favour of the risk-based approach. Using a risk-based framework was considered a better option than blanket regulation of all AI systems.|7ab4ef56578848099e6fff292e96498c|||
regulatory sandboxes|ai|Regulatory sandboxes could be very useful for the promotion of AI and are welcomed by certain stakeholders, especially the Business Associations.|7ab4ef56578848099e6fff292e96498c|||
collection and use of expertise|legislative gaps|Among those who formulated their opinion on the enforcement models, more than 50%, especially from the business associations, were in favour of a combination of an ex-ante risk self-assessment and an ex-post enforcement for high-risk AI systems.|7ab4ef56578848099e6fff292e96498c|||
high-level expert group on ai|european commission|The High-Level Expert Group on AI advised the European Commission on implementing its strategy for Artificial Intelligence in 2018.|67dff0d0eca948168520a396ec816e83|||
hleg ethics guidelines for trustworthy ai|commission's strategy on artificial intelligence|The Commission endorsed the key requirements set out in the HLEG ethics guidelines for Trustworthy AI in 2019, which had been revised based on stakeholder submissions.|67dff0d0eca948168520a396ec816e83|||
altai|ai development and use principles|The Assessment List for Trustworthy Artificial Intelligence (ALTAI) made the requirements operational in a piloting process with over 350 organisations, based on value-oriented principles developed by many private and public organisations in Europe and beyond.|67dff0d0eca948168520a396ec816e83|||
academics|businesses|The proposal builds on two years of analysis and close involvement of stakeholders, including academics and businesses, in developing the strategy for Artificial Intelligence.|67dff0d0eca948168520a396ec816e83|||
social partners|non-governmental organisations|The proposal also involved social partners and non-governmental organisations in the development process.|67dff0d0eca948168520a396ec816e83|||
ai alliance|stakeholders|approximately 4000 stakeholders, including over 450 additional position papers, were part of the AI Alliance formed to debate the technological and societal implications of AI, culminating in a yearly AI Assembly.|0f5ac43f487549ae9f2f3793d59c4b36|||
white paper on ai|stakeholders|The White Paper on AI further developed this inclusive approach, inciting comments from more than 1250 stakeholders, including over 450 additional position papers.|0f5ac43f487549ae9f2f3793d59c4b36|||
commission's regulatory scrutiny board|impact assessment|The Commission conducted an impact assessment for this proposal examined by the Commission's Regulatory Scrutiny Board. A meeting with the Regulatory Scrutiny Board was held on 16 December 2020, which was followed by a negative opinion.|0f5ac43f487549ae9f2f3793d59c4b36|||
impact assessment|commission's regulatory scrutiny board|After substantial revision of the impact assessment to address the comments and a resubmission of the impact assessment, the Regulatory Scrutiny Board issued a positive opinion.|0f5ac43f487549ae9f2f3793d59c4b36|||
policy options|preferred option|the Commission examined four policy options to achieve the general objective of ensuring the proper functioning of the single market by creating the conditions for the development and use of trustworthy AI in the Union. Among these, the preferred option is option 3+, a regulatory framework for high-risk AI systems.|e9bbb1e705fd4e0d8b5b8fba386d8317|||
preferred option|eu legislative instrument following a proportionate risk-based approach|Option 3+ combines the EU legislative instrument following a proportionate risk-based approach with codes of conduct for non-high-risk AI systems.|e9bbb1e705fd4e0d8b5b8fba386d8317|||
preferred option|mandatory requirements for all ai systems|However, it still involves mandatory requirements for all AI systems, irrespective of the risk they pose.|e9bbb1e705fd4e0d8b5b8fba386d8317|||
eu legislative instrument following a proportionate risk-based approach|high-risk ai systems|The EU legislative instrument following a proportionate risk-based approach is applied specifically to high-risk AI systems.|e9bbb1e705fd4e0d8b5b8fba386d8317|||
eu legislative instrument setting up a voluntary labelling scheme|voluntary labelling scheme|Option 1 proposes an EU legislative instrument setting up a voluntary labelling scheme.|e9bbb1e705fd4e0d8b5b8fba386d8317|||
"sectoral, ""ad-hoc"" approach"|sectoral approach|Option 2 suggests a sectoral, 'ad-hoc' approach.|e9bbb1e705fd4e0d8b5b8fba386d8317|||
rights|fundamental rights|The preferred option is considered suitable to address in the most effective way the objectives of this proposal as it limits the risks of violation of fundamental rights. This demonstrates that the key concept 'rights' is related to 'fundamental rights'.|5f77b1a2df5c4c0aab18730a0c7a29ed|||
preferred option|restricted yet effective set of actions from ai developers and users|The preferred option requires a restricted yet effective set of actions from AI developers and users, which limits the risks of violation of fundamental rights and fosters effective supervision and enforcement by targeting high-risk AI systems.|5f77b1a2df5c4c0aab18730a0c7a29ed|||
high-risk ai systems|regulatory framework for high-risk ai systems|The preferred option introduces mandatory requirements, including data, documentation and traceability, provision of information and transparency, human oversight and robustness and accuracy, for high-risk AI systems. This demonstrates that the key concept 'high-risk AI systems' is related to a 'regulatory framework for high-risk AI systems'.|5f77b1a2df5c4c0aab18730a0c7a29ed|||
companies|codes of conduct for other ai systems|The preferred option allows companies that introduced codes of conduct for other AI systems to follow a code of conduct voluntarily. This demonstrates that the key concept 'companies' is related to 'codes of conduct for other AI systems'.|5f77b1a2df5c4c0aab18730a0c7a29ed|||
european commission|inception impact assessment for a proposal for a legal act of the european parliament and the council laying down requirements for artificial intelligence.|The context mentions that for details of all the consultations that have been carried out, one can refer to Annex 2 of the impact assessment conducted by the European Commission.|5f77b1a2df5c4c0aab18730a0c7a29ed|||
ai alliance|multi-stakeholder forum launched in june 2018|The context mentions that the AI Alliance is a multi-stakeholder forum launched by an unspecified organization in June 2018.|5f77b1a2df5c4c0aab18730a0c7a29ed|||
ai applications|high risk for safety and fundamental rights of citizens|Developers or users of AI applications that pose a high risk to safety and fundamental rights of citizens are required to conform with specific requirements as part of our preferred option.|8a5408ea4ef645a09e980ff69bbb1a5c|||
high risk for safety and fundamental rights of citizens|specific requirements|Our preferred option aims to address the high risks associated with AI applications that impact safety and fundamental rights of citizens by implementing specific requirements for their development and use.|8a5408ea4ef645a09e980ff69bbb1a5c|||
high risk for safety and fundamental rights of citizens|regulatory sandboxes|To support compliance for SMEs, our preferred option includes provisions for the creation of regulatory sandboxes, which provide a controlled environment for testing AI applications with high risks to safety and fundamental rights of citizens.|8a5408ea4ef645a09e980ff69bbb1a5c|||
high risk for safety and fundamental rights of citizens|sme interests|Our preferred option obliges consideration of SME interests when setting fees related to conformity assessment, in order to reduce compliance costs for small and medium-sized enterprises.|8a5408ea4ef645a09e980ff69bbb1a5c|||
high risk for safety and fundamental rights of citizens|compliance costs|By targeting requirements only to systems where there is a high risk that such violations could occur, our preferred option keeps compliance costs to a minimum, avoiding an unnecessary slowing of uptake due to higher prices and compliance costs.|8a5408ea4ef645a09e980ff69bbb1a5c|||
ai applications|trust in ai|Higher trust in AI resulting from our preferred option leads to increased demand for AI applications, as well as an expanded array of available offers due to legal certainty.|8a5408ea4ef645a09e980ff69bbb1a5c|||
ai applications|single market for ai|The absence of obstacles to the cross-border movement of AI systems under our preferred option is likely to contribute to the flourishing of the European Union's fast-growing AI ecosystem, resulting in increased digital autonomy.|8a5408ea4ef645a09e980ff69bbb1a5c|||
high-risk ai|ai users|Annual cost for ensuring human oversight where appropriate depends on the use case and amounts to approximately EUR 5000 to EUR 8000 per year for high-risk AI systems.|8952a5ce9f1d4e18ad3956c0de300f95|||
high-risk ai|suppliers|Verification costs could amount to another EUR 3000 to EUR 7500 for high-risk AI systems.|8952a5ce9f1d4e18ad3956c0de300f95|||
ai users|individuals/citizens|Policy options could have different impacts on stakeholders, including economic operators/businesses, conformity assessment bodies, standardisation bodies and other public bodies, individuals/citizens, and researchers. These impacts are explained in detail in Annex 3 of the Impact assessment supporting this proposal.|8952a5ce9f1d4e18ad3956c0de300f95|||
ai users|businesses or public authorities|For AI applications not classified as high risk, minimal obligations of information would apply.|8952a5ce9f1d4e18ad3956c0de300f95|||
high-risk ai|ai users|Costs for suppliers of high-risk AI could amount to approximately EUR 6000 to EUR 7000 for the supply of an average high-risk AI system by 2025.|8952a5ce9f1d4e18ad3956c0de300f95|||
high-risk ai|ai users|Additional costs for verification could amount to another EUR 3000 to EUR 7500 for high-risk AI systems.|8952a5ce9f1d4e18ad3956c0de300f95|||
ai users|conformity assessment bodies, standardisation bodies and other public bodies|Costs would be at most as high as for high-risk AI systems, but most probably lower if businesses or public authorities choose to follow a code of conduct for suitable requirements and ensure that their AI systems are trustworthy.|8952a5ce9f1d4e18ad3956c0de300f95|||
ai users|individuals/citizens|Policy options could have different impacts on stakeholders, including economic operators/businesses, conformity assessment bodies, standardisation bodies and other public bodies, individuals/citizens, and researchers. These impacts are explained in detail in Annex 3 of the Impact assessment supporting this proposal.|8952a5ce9f1d4e18ad3956c0de300f95|||
ai users|researchers|Policy options could have different impacts on stakeholders, including economic operators/businesses, conformity assessment bodies, standardisation bodies and other public bodies, individuals/citizens, and researchers. These impacts are explained in detail in Annex 3 of the Impact assessment supporting this proposal.|8952a5ce9f1d4e18ad3956c0de300f95|||
ai users|businesses or public authorities|For AI applications not classified as high risk, minimal obligations of information would apply.|8952a5ce9f1d4e18ad3956c0de300f95|||
high-risk ai systems|union market|This proposal lays down obligations that will apply to providers of high-risk AI systems and will ensure legal certainty and prevent obstacles to the cross-border provision of AI-related services and products on the Union market.|80e161ae9d7043d68dc311987e20bb9a|||
providers of high-risk ai systems|companies using ai|This proposal will promote trust among customers of companies using AI by creating legal certainty and ensuring no obstacles to the cross-border provision of AI-related services and products emerge.|80e161ae9d7043d68dc311987e20bb9a|||
national public administrations|public trust in the use of ai|This proposal will promote public trust in the use of AI and strengthen enforcement mechanisms by introducing a European coordination mechanism, providing for appropriate capacities, and facilitating audits of high-risk AI systems with new requirements for documentation, traceability, and transparency.|80e161ae9d7043d68dc311987e20bb9a|||
union market|en legislation applicable to products and services|This proposal will bring further clarity and simplify the enforcement of the new rules by ensuring full consistency with existing sectoral Union legislation applicable to products and services.|80e161ae9d7043d68dc311987e20bb9a|||
ai systems|eu charter of fundamental rights|The use of AI with its specific characteristics can adversely affect a number of fundamental rights enshrined in the EU Charter of Fundamental Rights. This proposal seeks to ensure a high level of protection for those fundamental rights and aims to address various sources of risks through a clearly defined risk-based approach.|3fe04addd2784cdda4aafe5666e28af8|||
ai systems|right to human dignity|The use of AI with its specific characteristics can adversely affect the right to human dignity (Article 1) enshrined in the EU Charter of Fundamental Rights.|3fe04addd2784cdda4aafe5666e28af8|||
ai systems|respect for private life and protection of personal data|The use of AI with its specific characteristics can adversely affect respect for private life and protection of personal data (Articles 7 and 8) enshrined in the EU Charter of Fundamental Rights.|3fe04addd2784cdda4aafe5666e28af8|||
ai systems|non-discrimination|The use of AI with its specific characteristics can adversely affect non-discrimination (Article 21) enshrined in the EU Charter of Fundamental Rights.|3fe04addd2784cdda4aafe5666e28af8|||
ai systems|equality between women and men|The use of AI with its specific characteristics can adversely affect equality between women and men (Article 23) enshrined in the EU Charter of Fundamental Rights.|3fe04addd2784cdda4aafe5666e28af8|||
ai systems|rights to freedom of expression|The use of AI with its specific characteristics can have a chilling effect on the rights to freedom of expression (Article 11) enshrined in the EU Charter of Fundamental Rights.|3fe04addd2784cdda4aafe5666e28af8|||
ai systems|rights of defence and the presumption of innocence|The use of AI with its specific characteristics can affect the rights of defence and the presumption of innocence (Articles 47 and 48) enshrined in the EU Charter of Fundamental Rights.|3fe04addd2784cdda4aafe5666e28af8|||
rights of workers to fair and just working conditions|article 31|The proposal will positively affect the rights of a number of special groups, including the rights of workers to fair and just working conditions as mentioned in Article 31.|5ef40c7d3cc444eda5bc8f302a33e914|||
rights of workers to fair and just working conditions|high level of consumer protection|The proposal will positively affect the rights of a number of special groups, such as the workers' rights to fair and just working conditions and a high level of consumer protection (Article 28).|5ef40c7d3cc444eda5bc8f302a33e914|||
rights of children|article 24|The proposal will positively affect the rights of a number of special groups, such as the rights of the child (Article 24).|5ef40c7d3cc444eda5bc8f302a33e914|||
integration of persons with disabilities|article 26|The proposal will positively affect the rights of a number of special groups, such as the integration of persons with disabilities (Article 26).|5ef40c7d3cc444eda5bc8f302a33e914|||
high level of environmental protection|article 37|The proposal will positively affect a high level of environmental protection and the improvement of the quality of the environment (Article 37) which is also relevant, including in relation to the health and safety of people.|5ef40c7d3cc444eda5bc8f302a33e914|||
ex ante testing|risk management|The obligations for ex ante testing, risk management and human oversight will also facilitate the respect of other fundamental rights by minimising the risk of erroneous or biased AI-assisted decisions in critical areas such as education and training, employment, important services, law enforcement and the judiciary.|5ef40c7d3cc444eda5bc8f302a33e914|human oversight||
infringements of fundamental rights|effective redress for affected persons|In case infringements of fundamental rights still happen, effective redress for affected persons will be made possible by ensuring transparency and traceability of the AI systems coupled with strong ex post controls.|5ef40c7d3cc444eda5bc8f302a33e914|||
freedom to conduct business|article 16|This proposal imposes some restrictions on the freedom to conduct business (Article 16)|5ef40c7d3cc444eda5bc8f302a33e914|||
freedom of art and science|article 13|This proposal imposes some restrictions on the freedom to conduct business (Article 16) and the freedom of art and science (Article 13)|5ef40c7d3cc444eda5bc8f302a33e914|||
high-risk ai technology|compliance with overriding reasons of public interest|Ensure compliance with overriding reasons of public interest such as health, safety, consumer protection and the protection of other fundamental rights (`responsible innovation') when high-risk AI technology is developed and|5ef40c7d3cc444eda5bc8f302a33e914|||
fundamental rights|responsible innovation|When high-risk AI technology is developed and used, restrictions will be placed on fundamental rights to prevent and mitigate serious safety risks and likely infringements of fundamental rights. These restrictions will be proportionate and limited to the minimum necessary.|a986618b0d834d3fbcb2c5308a79803e|||
increased transparency obligations|right to protection of intellectual property|The increased transparency obligations will not disproportionately affect the right to protection of intellectual property, since they will be limited only to the minimum necessary information for individuals to exercise their right to an effective remedy and to the necessary transparency towards supervision and enforcement authorities, in line with their mandates.|a986618b0d834d3fbcb2c5308a79803e|||
supervisory authorities|member states|Member States will have to designate supervisory authorities in charge of implementing the legislative requirements. Their supervisory function could build on existing arrangements.|a986618b0d834d3fbcb2c5308a79803e|||
ai providers|public eu-wide database|AI providers will be obliged to provide meaningful information about their systems and the conformity assessment carried out on those systems to feed this database, which will enable competent authorities, users, and other interested people to verify if the high-risk AI system complies with the requirements laid down in the proposal and to exercise enhanced oversight over those AI systems posing high risks to fundamental rights.|1d80d05100d54a4c989d85b8b38227c4|||
commission|monitoring and evaluation mechanism|The Commission will be in charge of monitoring the effects of the proposal, establishing a system for registering stand-alone high-risk AI applications in a public EU-wide database, which will also enable competent authorities, users, and other interested people to verify if the high-risk AI system complies with the requirements laid down in the proposal and to exercise enhanced oversight over those AI systems posing high risks to fundamental rights.|1d80d05100d54a4c989d85b8b38227c4|||
member state|pre-existing structure|Depending on the pre-existing structure in each Member State, this could amount to 1 to 25 Full Time Equivalents per Member State.|1d80d05100d54a4c989d85b8b38227c4|||
financial statement|proposal|A detailed overview of the costs involved is provided in the `financial statement' linked to this proposal.|1d80d05100d54a4c989d85b8b38227c4|||
ai systems|conformity assessment|The regulation covers the placing on the market, putting into service and use of AI systems, which undergo conformity assessment as specified in Title I.|552ca9af21204cc7b0ae9f644b82bcbb|||
ai providers|national competent authorities|AI providers are obliged to inform national competent authorities about serious incidents or malfunctioning that constitute a breach of fundamental rights obligations as soon as they become aware of them, as well as any recalls or withdrawals of AI systems from the market.|552ca9af21204cc7b0ae9f644b82bcbb|||
national competent authorities|commission|National competent authorities will investigate the incidents/or malfunctioning, collect all the necessary information and regularly transmit it to the Commission with adequate metadata.|552ca9af21204cc7b0ae9f644b82bcbb|||
commission|market for ai|The Commission will complement this information on the incidents by a comprehensive analysis of the overall market for AI.|552ca9af21204cc7b0ae9f644b82bcbb|||
prohibited ai practices|manipulation of persons through subliminal techniques beyond their consciousness|Title II establishes a list of prohibited AI in the regulation, which follows a risk-based approach. This prohibition covers AI systems whose use is considered unacceptable as it contravenes Union values and has a significant potential to manipulate persons through subliminal techniques beyond their consciousness.|e13476fb7337411183d0d5abb695adf6|||
prohibited ai practices|exploitation of vulnerabilities of specific vulnerable groups|Title II also prohibits AI systems that have a significant potential to exploit the vulnerabilities of specific vulnerable groups such as children or persons with disabilities in order to materially distort their behaviour in a manner that is likely to cause them or another person psychological or physical harm.|e13476fb7337411183d0d5abb695adf6|||
person|psychological or physical harm|A person can suffer from psychological or physical harm due to manipulative or exploitative practices that might be facilitated by AI systems.|aa75f2e2235c465eafffbe5a557b5bb2|||
ai systems|manipulative or exploitative practices affecting adults|AI systems can facilitate manipulative or exploitative practices affecting adults.|aa75f2e2235c465eafffbe5a557b5bb2|||
adults|profiling or other practices that might affect their behaviour|Adults may be subject to profiling or other practices that might affect their behaviour due to AI systems.|aa75f2e2235c465eafffbe5a557b5bb2|||
digital service legislation|natural persons are properly informed and have free choice not to be subject to profiling or other practices|Digital service legislation guarantees that natural persons are properly informed and have free choice not to be subject to profiling or other practices.|aa75f2e2235c465eafffbe5a557b5bb2|||
ai-based social scoring|general purposes done by public authorities|The proposal prohibits AI-based social scoring for general purposes done by public authorities.|aa75f2e2235c465eafffbe5a557b5bb2|||
real time' remote biometric identification systems|law enforcement in publicly accessible spaces|The use of real time' remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement is prohibited unless certain limited exceptions apply.|aa75f2e2235c465eafffbe5a557b5bb2|||
ai systems|high risk to the health and safety or fundamental rights of natural persons|AI systems that create a high risk to the health and safety or fundamental rights of natural persons are permitted on the European market subject to compliance with certain mandatory requirements and an ex-ante conformity assessment.|aa75f2e2235c465eafffbe5a557b5bb2|||
high-risk ai systems|title iii|High-risk AI systems are covered by Title III of the proposal.|aa75f2e2235c465eafffbe5a557b5bb2|||
intended purpose|mandatory requirements and an ex-ante conformity assessment|The classification of an AI system as high-risk is based on its intended purpose, and it requires compliance with mandatory requirements and an ex-ante conformity assessment.|aa75f2e2235c465eafffbe5a557b5bb2|||
ai systems intended to be used as safety component of products that are subject to third party ex-ante conformity assessment|high-risk ai systems listed in annex iii|Category of high-risk AI systems according to Chapter 1 of Title III|7fc5736ae4b640e6825f9ff1bbd66158|||
ai systems intended to be used as safety component of products that are subject to third party ex-ante conformity assessment|high-risk ai systems whose risks have already materialised or are likely to materialise in the near future|Expansion criteria for list of high-risk AI systems applied by Commission|7fc5736ae4b640e6825f9ff1bbd66158|||
data and data governance requirements|legal requirements for high-risk ai systems as outlined in chapter 2|Relationship between data management and regulation of high-risk AI systems|7fc5736ae4b640e6825f9ff1bbd66158|||
documentation and recording keeping requirements|legal requirements for high-risk ai systems as outlined in chapter 2|Relationship between documentation management and regulation of high-risk AI systems|7fc5736ae4b640e6825f9ff1bbd66158|||
transparency and provision of information to users requirements|legal requirements for high-risk ai systems as outlined in chapter 2|Relationship between transparency and user communication requirements and regulation of high-risk AI systems|7fc5736ae4b640e6825f9ff1bbd66158|||
human oversight requirements|legal requirements for high-risk ai systems as outlined in chapter 2|Relationship between human intervention and regulation of high-risk AI systems|7fc5736ae4b640e6825f9ff1bbd66158|||
robustness requirements|legal requirements for high-risk ai systems as outlined in chapter 2|Relationship between robustness and regulation of high-risk AI systems|7fc5736ae4b640e6825f9ff1bbd66158|||
accuracy requirements|legal requirements for high-risk ai systems as outlined in chapter 2|Relationship between accuracy and regulation of high-risk AI systems|7fc5736ae4b640e6825f9ff1bbd66158|||
security requirements|legal requirements for high-risk ai systems as outlined in chapter 2|Relationship between security and regulation of high-risk AI systems|7fc5736ae4b640e6825f9ff1bbd66158|||
ai framework|international recommendations and principles|The proposed AI framework is largely consistent with other international recommendations and principles adopted by the EU's trade partners.|8f460846c6144bd1b7b72b980455fee0|||
high-risk ai systems|providers of high-risk ai systems|Chapter 3 places clear horizontal obligations on providers of high-risk AI systems.|8f460846c6144bd1b7b72b980455fee0|||
ai framework|state-of-the-art and technological and scientific progress|Chapter 3 allows providers of AI systems to choose the technical solutions to achieve compliance with international requirements, taking into account the state-of-the-art and technological and scientific progress in this field.|8f460846c6144bd1b7b72b980455fee0|||
notified bodies|independent third parties|Chapter 4 sets the framework for notified bodies to be involved as independent third parties across the AI value chain.|8f460846c6144bd1b7b72b980455fee0|||
users and other participants|providers of high-risk ai systems|Chapter 3 places proportionate obligations on users and other participants across the AI value chain, such as importers, distributors, authorized representatives.|8f460846c6144bd1b7b72b980455fee0|||
stand-alone high-risk ai systems|annex iii|stand-alone high-risk AI systems are referred to in Annex III of the regulation|9b956293105745b48a99a27a34944e5d|||
stand-alone high-risk ai systems|remote biometric identification systems|both types of high-risk AI systems differ in their conformity assessment procedures, as remote biometric identification systems will be subject to third party conformity assessment|9b956293105745b48a99a27a34944e5d|||
high-risk ai systems|safety components of products regulated under the new legislative framework legislation|both types of high-risk AI systems will be subject to the same ex-ante and ex-post compliance and enforcement mechanisms, as safety components of products regulated under the New Legislative Framework legislation|9b956293105745b48a99a27a34944e5d|||
high-risk ai systems|new legislative framework legislation|the conformity assessment approach for high-risk AI systems aims to ensure compliance not only with the requirements established by sectorial legislation, but also with the requirements established by this regulation|9b956293105745b48a99a27a34944e5d|||
providers|internal control checks|for stand-alone remote biometric identification systems, providers will be subject to internal control checks as part of the compliance and enforcement system established for these types of AI systems|9b956293105745b48a99a27a34944e5d|||
conformity assessment procedures|chapter 4|Chapter 4 sets the framework for notified bodies to be involved in conformity assessment procedures for high-risk AI systems|9b956293105745b48a99a27a34944e5d|||
conformity assessment procedures|chapter 5|Chapter 5 explains in detail the conformity assessment procedures for each type of high-risk AI system|9b956293105745b48a99a27a34944e5d|||
ai sector|regulatory intervention|The regulatory intervention is in an early phase for the AI sector, making an effective and reasonable solution for conformity assessment internal checks and strong ex-post enforcement a viable option.|2cd82811fd714f9896f3ce37c464d3c9|||
ai sector|innovation|The AI sector is very innovative.|2cd82811fd714f9896f3ce37c464d3c9|||
high-risk ai systems|regulation|An assessment through internal checks for `stand-alone' high-risk AI systems would require a full, effective and properly documented ex ante compliance with all requirements of the regulation.|2cd82811fd714f9896f3ce37c464d3c9|||
high-risk ai systems|compliance|An assessment through internal checks for `stand-alone' high-risk AI systems would require a full, effective and properly documented ex ante compliance with all requirements of the regulation.|2cd82811fd714f9896f3ce37c464d3c9|||
high-risk ai systems|quality management|An assessment through internal checks for `stand-alone' high-risk AI systems would require compliance with robust quality and risk management systems.|2cd82811fd714f9896f3ce37c464d3c9|||
high-risk ai systems|risk management|An assessment through internal checks for `stand-alone' high-risk AI systems would require compliance with robust quality and risk management systems.|2cd82811fd714f9896f3ce37c464d3c9|||
high-risk ai systems|post-market monitoring|An assessment through internal checks for `stand-alone' high-risk AI systems would require post-market monitoring.|2cd82811fd714f9896f3ce37c464d3c9|||
ai systems|safety components of products|For reasons of consistency with the existing product safety legislation, the conformity assessments of AI systems that are safety components of products will follow a system with third party conformity assessment procedures already established under the relevant sectoral product safety legislation.|2cd82811fd714f9896f3ce37c464d3c9|||
ai systems|substantial modifications|New ex ante re-assessments of the conformity will be needed in case of substantial modifications to the AI systems (and notably changes which go beyond what is)|2cd82811fd714f9896f3ce37c464d3c9|||
database|eu|After the provider has performed the relevant conformity assessment, it should register those stand-alone high-risk AI systems in an EU database that will be managed by the Commission to increase public transparency and oversight and strengthen ex post supervision by competent authorities.|2cd82811fd714f9896f3ce37c464d3c9|||
substantial modifications to ai systems|pre-determined by the provider in its technical documentation and checked at the moment of ex-ante conformity assessment|The edge between these nodes represents the situation where substantial modifications beyond what is pre-determined may occur, highlighting the importance of checking for conformity during the ex-ante assessment.|de7af1825a5344c4903d045a2546a72a|||
certain ai systems|used to detect emotions or determine association with (social) categories based on biometric data|These nodes are related as certain AI systems may be used to detect emotions or categorize individuals based on their biometric data.|de7af1825a5344c4903d045a2546a72a|||
certain ai systems|generate or manipulate content ('deep fakes')|The relation between these nodes indicates that some AI systems are capable of generating or manipulating content, specifically 'deep fakes'.|de7af1825a5344c4903d045a2546a72a|||
title iv|certain ai systems|Title IV applies to certain AI systems due to the specific risks they pose, particularly related to manipulation and the recognition of emotions or characteristics through automated means.|de7af1825a5344c4903d045a2546a72a|||
persons interacting with an ai system|people informed of their interaction with the ai system|When persons interact with an AI system, they should be informed of this circumstance, as indicated by Title IV.|de7af1825a5344c4903d045a2546a72a|||
title iv|ai systems generating or manipulating content ('deep fakes')|Under Title IV, there is an obligation to disclose that the content generated by AI systems is automated, with exceptions for legitimate purposes.|de7af1825a5344c4903d045a2546a72a|||
title v|innovation-friendly legal framework|Title V aims to create a legal framework that is friendly towards innovation.|de7af1825a5344c4903d045a2546a72a|||
title vii|eu-wide database for stand-alone high-risk ai systems|facilitates monitoring work of Commission and national authorities through establishment|d12a98f44ff0408694ee35474c2701bf|||
title vii|providers of ai systems|required to register their systems before placing them on the market or otherwise putting them into service|d12a98f44ff0408694ee35474c2701bf|||
market surveillance authorities|title viii|control the market and investigate compliance with obligations and requirements for all high-risk AI systems already placed on the market|d12a98f44ff0408694ee35474c2701bf|||
providers of ai systems|market surveillance authorities|subject to market surveillance authorities' control and investigation on compliance with obligations and requirements for all high-risk AI systems already placed on the market|d12a98f44ff0408694ee35474c2701bf|||
providers of ai systems|post-market monitoring and reporting obligations|set out by Title VIII|d12a98f44ff0408694ee35474c2701bf|||
title vii|ex-post enforcement|ensures that once the AI system has been put on the market, public authorities have powers and resources to monitor compliance|d12a98f44ff0408694ee35474c2701bf|||
codes of conduct|voluntary application of mandatory requirements for non-high-risk ai systems|Title IX in the proposed regulation creates a framework for encouraging providers to apply voluntarily the mandatory requirements for non-high-risk AI systems through codes of conduct.|50271ac83f944bcf8e5d67c86851e576|||
existing supervision and enforcement authorities|request market surveillance authorities to organise testing of high-risk ai systems through technical means|When necessary for their mandate, existing supervision and enforcement authorities in the proposed regulation have the power to request market surveillance authorities to organise testing of high-risk AI systems through technical means.|50271ac83f944bcf8e5d67c86851e576|||
public authorities|intervene in case ai systems generate unexpected risks|In the proposed regulation, public authorities have the powers and resources to intervene in case AI systems generate unexpected risks.|50271ac83f944bcf8e5d67c86851e576|||
public authorities|monitor compliance of operators with their relevant obligations under the regulation|The proposed regulation empowers public authorities to monitor compliance of operators with their relevant obligations.|50271ac83f944bcf8e5d67c86851e576|||
existing sectorial authorities|trusted also with the powers to monitor and enforce the provisions of the regulation|In the proposed regulation, Member States may appoint existing sectorial authorities, who would be entrusted also with the powers to monitor and enforce the provisions of the regulation.|50271ac83f944bcf8e5d67c86851e576|||
existing supervision and enforcement authorities|request and access any documentation maintained following this regulation|Existing supervision and enforcement authorities in the proposed regulation will also have the power to request and access any documentation maintained following this regulation.|50271ac83f944bcf8e5d67c86851e576|||
high-risk ai systems|organise testing of high-risk ai systems through technical means|Market surveillance authorities in the proposed regulation have the responsibility to organise testing of high-risk AI systems through technical means.|50271ac83f944bcf8e5d67c86851e576|||
title x|confidentiality|Title X emphasizes the obligation of all parties to respect the confidentiality of information and data.|9b38aaf38b314d0f96c998cce5aa0ab6|||
title x|effective, proportionate, and dissuasive penalties for infringements of the provisions.|Title X also includes measures to ensure the effective implementation of the regulation through effective, proportionate, and dissuasive penalties for infringements of the provisions.|9b38aaf38b314d0f96c998cce5aa0ab6|||
title xi|delegation and implementing powers|The proposal empowers the Commission to adopt, where appropriate, implementing acts to ensure uniform application of the regulation or delegated acts to update or complement the lists in Annexes I to VII.|9b38aaf38b314d0f96c998cce5aa0ab6|||
annex iii|update|There is an obligation for the Commission to assess regularly the need for an update of Annex III|9b38aaf38b314d0f96c998cce5aa0ab6|||
title xii|regular reports|Title XII contains an obligation for the Commission to prepare regular reports on the evaluation and review of the regulation.|9b38aaf38b314d0f96c998cce5aa0ab6|||
union values|improvement of the functioning of the internal market|purpose of this regulation is to lay down a uniform legal framework in particular for the development, marketing and use of artificial intelligence in conformity with union values which improves the functioning of the internal market|1a3da920f72b46a9b3b23d9610493b17|||
union values|artificial intelligence|conformity with union values in the development, marketing and use of artificial intelligence|1a3da920f72b46a9b3b23d9610493b17|||
internal market|uniform legal framework|this regulation lays down a uniform legal framework for the development, marketing and use of artificial intelligence in conformity with union values which improves the functioning of the internal market|1a3da920f72b46a9b3b23d9610493b17|||
european commission|proposal|after transmission of the draft legislative act to the national parliaments, having regard to the proposal from the european commission|1a3da920f72b46a9b3b23d9610493b17|||
european economic and social committee|opinion|having regard to the opinion of the european economic and social committee|1a3da920f72b46a9b3b23d9610493b17|||
committee of the regions|opinion|having regard to the opinion of the committee of the regions|1a3da920f72b46a9b3b23d9610493b17|||
european parliament|european council|the european parliament and the council of the european union, acting in accordance with the ordinary legislative procedure,|1a3da920f72b46a9b3b23d9610493b17|||
artificial intelligence systems (ais)|high level of protection|Pursuing a number of overriding reasons of public interest, such as a high level of protection of health, safety and fundamental rights, this Regulation ensures the free movement of AI-based goods and services cross-border, preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by this Regulation.|aa480a4d8b474efdab31dad27ec1bbe9|||
ai systems|uniform obligations for operators|Certain Member States have already explored the adoption of national rules to ensure that artificial intelligence is safe and is developed and used in compliance with fundamental rights obligations. By laying down uniform obligations for operators, this Regulation ensures a consistent and high level of protection throughout the Union, preventing divergences hampering the free circulation of AI systems and related products and services within the internal market.|aa480a4d8b474efdab31dad27ec1bbe9|||
ai systems|uniform protection of overriding reasons of public interest and of rights of persons throughout the internal market based on article 114 of the treaty on the functioning of the european union|The Regulation guarantees the uniform protection of overriding reasons of public interest and of rights of persons throughout the internal market based on Article 114 of the Treaty on the Functioning of the European Union, preventing the free circulation of AI systems and related products and services within the internal market from being hampered by divergences.|aa480a4d8b474efdab31dad27ec1bbe9|||
ai systems|free movement of ai-based goods and services cross-border|The Regulation ensures the free movement of AI-based goods and services cross-border, preventing Member States from imposing restrictions on the development, marketing and use of AI systems, unless explicitly authorised by this Regulation.|aa480a4d8b474efdab31dad27ec1bbe9|||
ai systems|development, marketing and use of ai systems|Unless explicitly authorised by this Regulation, Member States cannot impose restrictions on the development, marketing and use of AI systems.|aa480a4d8b474efdab31dad27ec1bbe9|||
ai systems|internal market|The Regulation prevents Member States from imposing restrictions on the development, marketing and use of AI systems that would hinder the free circulation of AI systems and related products and services within the internal market.|aa480a4d8b474efdab31dad27ec1bbe9|||
article 114 of the treaty on the functioning of the european union (tfeu)|regulation|Based on Article 114 of the Treaty on the Functioning of the European Union (TFEU), this Regulation has been introduced.|026fb885345345ecb7db48f0809f5417|||
article 16 of the tfeu|specific rules on the protection of individuals with regard to the processing of personal data concerning|If the Regulation contains specific rules on the protection of individuals with regard to the processing of personal data concerning, it is appropriate to base those specific rules on Article 16 of the TFEU.|026fb885345345ecb7db48f0809f5417|||
real-time|remote biometric identification|The use of AI systems for `real-time' remote biometric identification in publicly accessible spaces for the purpose of law enforcement is restricted.|026fb885345345ecb7db48f0809f5417|||
artificial intelligence|benefits|Artificial intelligence can provide key competitive advantages to companies and support socially and environmentally beneficial outcomes in various industries and social activities.|026fb885345345ecb7db48f0809f5417|||
prediction|optimisation|[{'node_1': 'Artificial intelligence', 'node_2': 'Prediction'}, {'node_1': 'Artificial intelligence', 'node_2': 'Optimisation'}, {'node_1': 'Artificial intelligence', 'node_2': 'Resource allocation'}]|026fb885345345ecb7db48f0809f5417|Resource allocation||
individuals|organisations|[{'node_1': 'Digital solutions', 'node_2': 'Individuals'}, {'node_1': 'Digital solutions', 'node_2': 'Organisations'}]|026fb885345345ecb7db48f0809f5417|||
healthcare|farming|[{'node_1': 'Artificial intelligence', 'node_2': 'Healthcare'}, {'node_1': 'Artificial intelligence', 'node_2': 'Farming'}, {'node_1': 'Artificial intelligence', 'node_2': 'Education and training'}, {'node_1': 'Artificial intelligence', 'node_2': 'Infrastructure management'}]|026fb885345345ecb7db48f0809f5417|Education and training|Infrastructure management|
artificial intelligence|union legal framework laying down harmonised rules on artificial intelligence|A Union legal framework laying down harmonised rules on artificial intelligence is therefore needed to foster the development, use and uptake of artificial intelligence in the internal market that at the same time meets a high level of protection of public interests, such as health and safety and the protection of fundamental rights, as recognised and protected by Union law. To achieve that objective, rules regulating the placing on the market and putting into service of certain AI systems should be laid down, thus ensuring the smooth functioning of the internal market and allowing those systems to benefit from the principle of free movement of goods and services.|18fb40e95c1c4aa1b670aa8e408d4470|||
artificial intelligence|public interests|At the same time, depending on the circumstances regarding its specific application and use, artificial intelligence may generate risks and cause harm to public interests and rights that are protected by Union law. Such harm might be material or immaterial.|18fb40e95c1c4aa1b670aa8e408d4470|||
union|global leader in the development of secure, trustworthy and ethical artificial intelligence|As stated by the European Council33,|18fb40e95c1c4aa1b670aa8e408d4470|||
intelligence|ai system|The definition of AI system should be based on the key functional characteristics of intelligence, which is the ability for a given set of human-defined objectives to generate outputs that influence the environment in both physical and digital dimensions. This is in line with the request by the European Parliament for ethical principles to be ensured in this context.|f3499bb5ddc1452c9e7b64b7c92112c2|||
ai system|european council|The European Council has requested that AI systems ensure the protection of ethical principles, as specifically mentioned in their conclusions from October 2020.|f3499bb5ddc1452c9e7b64b7c92112c2|||
ai system|european parliament|The European Parliament has also requested the definition of AI system to provide legal certainty and accommodate future technological developments, while ensuring flexibility for its use as a stand-alone basis or as a component in products.|f3499bb5ddc1452c9e7b64b7c92112c2|||
ai system|techniques and approaches used for its development|The definition of AI system should be complemented by a list of specific techniques and approaches used for its development, which should be kept up-to-date in the light of market and technological developments.|f3499bb5ddc1452c9e7b64b7c92112c2|||
regulation (eu) 2016/679|article 4(14)|The notion of biometric data used in this Regulation is in line with and should be interpreted consistently with the notion of biometric data as defined in Article 4(14) of Regulation (EU) 2016/679|35bfc874b7924036b9d073b5fc35289a|||
regulation (eu) 2018/1725|article 3(18)|The notion of biometric data used in this Regulation is in line with and should be interpreted consistently with the notion of biometric data as defined in Article 4(14) of Regulation (EU) 2016/679 and Article 3(18) of Regulation (EU) 2018/1725|35bfc874b7924036b9d073b5fc35289a|||
directive (eu) 2016/680|article 3(13)|The notion of biometric data used in this Regulation is in line with and should be interpreted consistently with the notion of biometric data as defined in Article 4(14) of Regulation (EU) 2016/679 and Article 3(18) of Regulation (EU) 2018/1725 and Article 3(13) of Directive (EU) 2016/680|35bfc874b7924036b9d073b5fc35289a|||
article 3(18)|regulation (eu) 2018/1725|The notion of remote biometric identification system should be defined functionally, as an AI system intended for the identification of natural persons at a distance through the comparison of a person's biometric data with the biometric data contained in a reference database, and without prior knowledge whether the targeted person will be present and can be identified, irrespective of the particular technology, processes or types of biometric data used. This notion is defined consistently with Article 3(18) of Regulation (EU) 2018/1725|35bfc874b7924036b9d073b5fc35289a|||
article 4(14)|regulation (eu) 2016/679|The notion of biometric data used in this Regulation is also referred to as 'remote biometric identification system' as defined in Article 3(18) of Regulation (EU) 2018/1725|35bfc874b7924036b9d073b5fc35289a|||
article 3(13)|directive (eu) 2016/680|The notion of biometric data used in this Regulation is also referred to as 'remote biometric identification system' as defined in Article 3(18) of Regulation (EU) 2018/1725 and Article 3(13) of Directive (EU) 2016/680|35bfc874b7924036b9d073b5fc35289a|||
real-time|ai systems|In the case of `real-time' systems, the capturing of biometric data, the comparison and the identification occur all instantaneously, near-instantaneously or in any event without a significant delay. This involves material, such as video footage generated by a camera or other device with similar functionality.|e89c370d91bf4ff6bf08ea1e9f3acf7f|||
real-time|live|In the case of `real-time' systems, involves material, such as video footage generated by a camera or other device with similar functionality, that is `live' or `near-`live'|e89c370d91bf4ff6bf08ea1e9f3acf7f|||
post|biometric data|In the case of `post' systems, in contrast, the biometric data have already been captured and the comparison and identification occur only after a significant delay.|e89c370d91bf4ff6bf08ea1e9f3acf7f|||
ai systems|biometric data|This involves material, such as pictures or video footage generated by closed circuit television cameras or private devices, which has been generated before the use of the system in respect of the natural persons concerned.|e89c370d91bf4ff6bf08ea1e9f3acf7f|||
publicly accessible space|physical place|For the purposes of this Regulation the notion of publicly accessible space should be understood as referring to any physical place that is accessible to the public, irrespective of whether the place in question is privately or publicly owned. Therefore, the notion does not cover places that are private in nature and normally not freely accessed by the public.|e89c370d91bf4ff6bf08ea1e9f3acf7f|||
prevention, investigation, detection or prosecution of criminal offences|execution of criminal penalties|purposes for which data can be transferred under this Regulation.|e77354c4684e45bb8c156242eded4443|||
publicly accessible|spaces such as streets, relevant parts of government buildings and most transport infrastructure|examples of publicly accessible spaces.|e77354c4684e45bb8c156242eded4443|||
cinemas, theatres, shops and shopping centres|normally also publicly accessible|other examples of publicly accessible spaces.|e77354c4684e45bb8c156242eded4443|||
providers of ai systems|users of ai systems established within the union|groups to which the rules established by this Regulation apply.|e77354c4684e45bb8c156242eded4443|||
established within the union|established in a third country|distinction based on whether AI systems providers are established in the EU or in a third country.|e77354c4684e45bb8c156242eded4443|||
ai systems established within the union|providers and users of ai systems that are established in a third country|The output produced by these systems is used in the Union, even if the systems themselves are not physically present in the Union.|fb64b1622b8d498a84d92b058689fdfc|||
ai systems|operators established outside the union|In circumstances where an operator established in the Union contracts certain services to an operator established outside the Union, the AI system used by the latter falls within the scope of this Regulation if its effects impact natural persons located in the Union.|fb64b1622b8d498a84d92b058689fdfc|||
ai systems|high-risk ai systems|The activity to be performed by an AI system can qualify as high-risk, and this is a factor that determines whether it falls within the scope of this Regulation.|fb64b1622b8d498a84d92b058689fdfc|||
ai systems|digital nature|Certain AI systems should fall within the scope of this Regulation even when they are neither placed on the market, nor put into service, nor used in the Union, as a result of their digital nature.|fb64b1622b8d498a84d92b058689fdfc|||
high-risk ai systems|public interests as regards health, safety and fundamental rights|should be established to ensure a consistent and high level of protection for all high-risk AI systems due to the potential impact on public interests related to health, safety, and fundamental rights.|dcd461eddd324c09bc854c3691b2e5b0|||
ai systems exclusively developed or used for military purposes|common foreign and security policy regulated under title v of the treaty on the european union (teu)|should be excluded from the scope of this Regulation where that use falls under the exclusive remit of the Common Foreign and Security Policy regulated under Title V of the Treaty on the European Union (TEU).|dcd461eddd324c09bc854c3691b2e5b0|||
union institutions, offices, bodies and agencies|this regulation|should apply to when acting as a provider or user of an AI system.|dcd461eddd324c09bc854c3691b2e5b0|||
international agreements concluded at national or european level for law enforcement and judicial cooperation with the union or with its member states|public authorities of a third country and international organisations|when acting in the framework of international agreements concluded at national or European level for law enforcement and judicial cooperation with the Union or with its Member States, this Regulation should not apply.|dcd461eddd324c09bc854c3691b2e5b0|||
intermediary service providers set out in directive 2000/31/ec of the european parliament and of the council [as amended by the digital services act]|this regulation|should be without prejudice to the provisions regarding the liability of intermediary service providers set out in Directive 2000/31/EC of the European Parliament and of the Council [as amended by the Digital Services Act].|dcd461eddd324c09bc854c3691b2e5b0|||
high-risk ai systems|charter of fundamental rights of the european union (the charter)|Should be consistent with|cecaebbdcb084aac8c4c2487fbf995a5|||
high-risk ai systems|non-discriminatory and in line with the union's international trade commitments|Should be|cecaebbdcb084aac8c4c2487fbf995a5|||
a clearly defined risk-based approach|high-risk ai systems|Should follow|cecaebbdcb084aac8c4c2487fbf995a5|||
certain ai systems|transparency obligations|Should lay down|cecaebbdcb084aac8c4c2487fbf995a5|||
prohibited artificial intelligence practices|union values of respect for human dignity, freedom, equality, democracy and the rule of law and union fundamental rights|Should contradict|cecaebbdcb084aac8c4c2487fbf995a5|||
ai systems for 'real-time' remote biometric identification|heightened risks for the rights and freedoms of natural persons|The use of AI systems for 'real-time' remote biometric identification of natural persons in publicly accessible spaces for the purpose of law enforcement is considered particularly intrusive in the rights and freedoms of the concerned persons, to the extent that it may affect the private life of a large part of the population, evoke a feeling of constant surveillance and indirectly dissuade the exercise of the freedom of assembly and other fundamental rights. This operation of such systems operating in 'real-time' carries heightened risks for the rights and freedoms of the persons that are concerned by law enforcement activities.|6940723db83f4c21a5ccbfcf37d57709|||
ai systems for 'real-time' remote biometric identification|limited opportunities for further checks or corrections|The immediacy of the impact and the limited opportunities for further checks or corrections in relation to the use of such systems operating in 'real-time' carry heightened risks for the rights and freedoms of the persons that are concerned by law enforcement activities.|6940723db83f4c21a5ccbfcf37d57709|||
ai systems for 'real-time' remote biometric identification|law enforcement activities|The use of AI systems for 'real-time' remote biometric identification of natural persons in publicly accessible spaces is related to law enforcement activities.|6940723db83f4c21a5ccbfcf37d57709|||
natural persons|ai systems for 'real-time' remote biometric identification|The use of AI systems for 'real-time' remote biometric identification of natural persons in publicly accessible spaces for the purpose of law enforcement is related to natural persons.|6940723db83f4c21a5ccbfcf37d57709|||
natural persons|heightened risks for the rights and freedoms of natural persons|The use of AI systems for 'real-time' remote biometric identification of natural persons in publicly accessible spaces for the purpose of law enforcement is related to heightened risks for the rights and freedoms of natural persons.|6940723db83f4c21a5ccbfcf37d57709|||
natural persons|limited opportunities for further checks or corrections|The immediacy of the impact and the limited opportunities for further checks or corrections in relation to the use of such systems operating in 'real-time' are related to natural persons.|6940723db83f4c21a5ccbfcf37d57709|||
natural persons|ai systems evaluating or classifying trustworthiness based on social behaviour|Such AI systems evaluate or classify the trustworthiness of natural persons based on their social behaviour in multiple contexts or known or predicted personal or personality characteristics.|6940723db83f4c21a5ccbfcf37d57709|||
natural persons|social score obtained from such ai systems|The social score obtained from such AI systems may lead to the detrimental or unfavourable treatment of natural persons or whole groups thereof in social contexts, which are unrelated to the context in which the data was originally generated or collected or to a detrimental treatment that is disproportionate or unjustified to the gravity of their social behaviour.|6940723db83f4c21a5ccbfcf37d57709|||
social score obtained from such ai systems|detrimental or unfavourable treatment of natural persons or whole groups thereof in social contexts|The social score obtained from such AI systems may lead to the detrimental or unfavourable treatment of natural persons or whole groups thereof in social contexts, which are unrelated to the context in which the data was originally generated or collected or to a detrimental treatment that is disproportionate or unjustified to the gravity of their social behaviour.|6940723db83f4c21a5ccbfcf37d57709|||
detrimental or unfavourable treatment of natural persons or whole groups thereof in social contexts|social behaviour|The detrimental or unfavourable treatment of natural persons or whole groups thereof in social contexts is related to social behaviour.|6940723db83f4c21a5ccbfcf37d57709|||
detrimental or unfavourable treatment of natural persons or whole groups thereof in social contexts|values of equality and justice|Such AI systems may violate the values of equality and justice by leading to the detrimental or unfavourable treatment of natural persons or whole groups thereof in social contexts, which are unrelated to the context in which the data was originally generated or collected or to a detrimental treatment that is disproportionate or unjustified to the gravity of their social behaviour.|6940723db83f4c21a5ccbfcf37d57709|||
ai systems evaluating or classifying trustworthiness based on personal or personality characteristics|personal or personality characteristics|AI systems evaluating or classifying the trustworthiness of natural persons based on their personal or personality characteristics are related to personal or personality characteristics.|6940723db83f4c21a5ccbfcf37d57709|||
social behaviour in multiple contexts|social behaviour|Social behaviour in multiple contexts is a form of social behaviour.|6940723db83f4c21a5ccbfcf37d57709|||
law enforcement activities|rights and freedoms of the persons concerned|The use of those systems for the purpose of law enforcement should therefore be prohibited, except in three exhaustively listed and narrowly defined situations, where the use is strictly necessary to achieve a substantial public interest, the importance of which outweighs the risks.|5ad86b02e9264e1b98b65770af7922aa|||
potential victims of crime|search for potential victims of crime|involve the search for potential victims of crime,|5ad86b02e9264e1b98b65770af7922aa|||
missing children|search for potential victims of crime|including missing children;|5ad86b02e9264e1b98b65770af7922aa|||
certain threats to the life or physical safety of natural persons or of a terrorist attack|use is strictly necessary to achieve a substantial public interest|certain threats to the life or physical safety of natural persons or of a terrorist attack;|5ad86b02e9264e1b98b65770af7922aa|||
criminal offences referred to in council framework decision 2002/584/jha38|detection, localisation, identification or prosecution of perpetrators or suspects of the criminal offences referred to in council framework decision 2002/584/jha38 |and as they are defined in the law of that Member State.|5ad86b02e9264e1b98b65770af7922aa|||
council framework decision 2002/584/jha38|criminal offences referred to in council framework decision 2002/584/jha38 |if those criminal offences are punishable in the Member State concerned by a custodial sentence or a detention order for a maximum period of at least three years|5ad86b02e9264e1b98b65770af7922aa|||
member state concerned|council framework decision 2002/584/jha38|as they are defined in the law of that Member State.|5ad86b02e9264e1b98b65770af7922aa|||
custodial sentence or detention order for a maximum period of at least three years|council framework decision 2002/584/jha38|and as they are defined in the law of that Member State.|5ad86b02e9264e1b98b65770af7922aa|||
national law|council framework decision 2002/584/jha38|as they are defined in the law of that Member State.|5ad86b02e9264e1b98b65770af7922aa|||
criminal offences|council framework decision 2002/584/jha38|listed in the Council Framework Decision|5ad86b02e9264e1b98b65770af7922aa|||
real-time' remote biometric identification system|law enforcement authority|used by law enforcement authority for the purpose of law enforcement in publicly accessible spaces subject to an express and specific authorisation by a judicial authority or by an independent administrative authority of a Member State. Such authorisation should be obtained prior to the use, except in duly justified situations of urgency where the need to use the systems in question is such as to make it effectively and objectively impossible to obtain an authorisation before commencing the use.|747f9405cbf64a05842afb3b1c4f5f04|||
real-time' remote biometric identification system|member state|allowed to be used in the territory of a Member State in accordance with this Regulation only where and in as far as the Member State allows such use.|747f9405cbf64a05842afb3b1c4f5f04|||
real-time' remote biometric identification system|exhaustive framework set by this regulation|falls within the exhaustive framework set by this Regulation.|747f9405cbf64a05842afb3b1c4f5f04|||
law enforcement authority|independent administrative authority of a member state|both can provide an express and specific authorisation for the use of 'real-time' remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement.|747f9405cbf64a05842afb3b1c4f5f04|||
law enforcement authority|judicial authority|both can provide an express and specific authorisation for the use of 'real-time' remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement.|747f9405cbf64a05842afb3b1c4f5f04|||
law enforcement authority|situations of urgency|in situations where the need to use the systems in question is such as to make it effectively and objectively impossible to obtain an authorisation before commencing the use, law enforcement authorities can still use 'real-time' remote biometric identification systems for the purpose of law enforcement.|747f9405cbf64a05842afb3b1c4f5f04|||
law enforcement authority|appropriate safeguards and conditions|should apply to the use of 'real-time' remote biometric identification systems in situations of urgency, as determined in national law.|747f9405cbf64a05842afb3b1c4f5f04|||
law enforcement authority|seeks to obtain an authorisation as soon as possible|should seek to obtain an express and specific authorisation for the use of 'real-time' remote biometric identification systems in situations where it was not possible to request it earlier due to urgent circumstances.|747f9405cbf64a05842afb3b1c4f5f04|||
regulation|member state|Regulation should only be possible where and in as far as the Member State in question has decided to expressly provide for the possibility to authorise such use in its Council Framework Decision 2002/584/JHA of 13 June 2002 on the European arrest warrant and the surrender procedures between Member States (OJ L 190, 18.7.2002, p. 1).|49ec74d61f9c4f92a7f45bafa7fd7a27|||
regulation|member state|Consequently, Member States remain free under this Regulation not to provide for such a possibility at all or to only provide for such a possibility in respect of some of the objectives capable of justifying authorised use identified in this Regulation.|49ec74d61f9c4f92a7f45bafa7fd7a27|||
ai systems|natural persons|The use of AI systems for `real-time' remote biometric identification of natural persons in publicly accessible spaces for the purpose of law enforcement necessarily involves the processing of biometric data.|49ec74d61f9c4f92a7f45bafa7fd7a27|||
rules of this regulation|rules on the processing of biometric data contained in article 10 of directive (eu) 2016/680|The rules of this Regulation that prohibit, subject to certain exceptions, such use, which are based on Article 16 TFEU, should apply as lex specialis in respect of the rules on the processing of biometric data contained in Article 10 of Directive (EU) 2016/680, thus regulating such use and the processing of biometric data involved in an exhaustive manner.|49ec74d61f9c4f92a7f45bafa7fd7a27|||
rules of this regulation|rules on the processing of biometric data contained in article 10 of directive (eu) 2016/680|Therefore, such use and processing should only be possible in as far as it is compatible with the framework|49ec74d61f9c4f92a7f45bafa7fd7a27|||
real-time' remote biometric identification systems|law enforcement as regulated by this regulation|The use of 'real-time' remote biometric identification systems for purposes other than law enforcement is not covered by the specific framework regarding such use for the purpose of law enforcement set by this Regulation.|2e394909fb1a4244a820c6eb9ce25ea1|||
ai systems for biometric identification|not in connection to the use of 'real-time' remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement as regulated by this regulation|Any processing of biometric data and other personal data involved in the use of AI systems for biometric identification, other than in connection to the use of 'real-time' remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement as regulated by this Regulation, should not be subject to the specific framework regarding such use for the purpose of law enforcement set by this Regulation.|2e394909fb1a4244a820c6eb9ce25ea1|||
competent authorities|publicly accessible spaces|used by competent authorities for purposes other than law enforcement in publicly accessible spaces|b2748a2566504fe7a06859ea2387f62c|||
article 9(1) of regulation (eu) 2016/679|competent authorities|should continue to comply with all requirements resulting from Article 9(1) of Regulation (EU) 2016/679 when used by competent authorities|b2748a2566504fe7a06859ea2387f62c|||
article 10(1) of regulation (eu) 2018/1725|competent authorities|should continue to comply with all requirements resulting from Article 10(1) of Regulation (EU) 2018/1725 when used by competent authorities|b2748a2566504fe7a06859ea2387f62c|||
article 10 of directive (eu) 2016/680|competent authorities|should continue to comply with all requirements resulting from Article 10 of Directive (EU) 2016/680 when used by competent authorities|b2748a2566504fe7a06859ea2387f62c|||
ireland|rules laid down in article 5(1), point (d), (2) and (3) of this regulation adopted on the basis of article 16 of the tfeu which relate to the processing of personal data by member states when carrying out activities falling within the scope of chapter 4 or chapter 5 of title v of part three of the tfeu|Ireland is not bound by|b2748a2566504fe7a06859ea2387f62c|||
ireland|rules governing the forms of judicial cooperation in criminal matters or police cooperation which require compliance with the provisions laid down on the basis of article 16 of the tfeu|Ireland is not bound by|b2748a2566504fe7a06859ea2387f62c|||
denmark|rules laid down in article 5(1), point (d), (2) and (3) of this regulation adopted on the basis of article 16 of the tfeu, or subject to their application|Denmark is not bound by or subject to their application|b2748a2566504fe7a06859ea2387f62c|||
high-risk ai systems|union market|High-risk AI systems should only be placed on the Union market if they comply with certain mandatory requirements to ensure they do not pose unacceptable risks to important Union public interests as recognized and protected by Union law.|d7052e6ea4a34a61a17b62c49d185099|||
high-risk ai systems|put into service|High-risk AI systems should only be put into service if they comply with certain mandatory requirements to ensure they do not pose unacceptable risks to important Union public interests as recognized and protected by Union law.|d7052e6ea4a34a61a17b62c49d185099|||
union market|high-risk ai systems available in the union or whose output is otherwise used in the union|High-risk AI systems should only be placed on the Union market if they do not pose unacceptable risks to important Union public interests as recognized and protected by Union law, and AI systems identified as high-risk should be limited to those that have a significant harmful impact on health, safety and fundamental rights of persons in the Union and such limitation minimizes any potential restriction to international trade, if any.|d7052e6ea4a34a61a17b62c49d185099|||
high-risk ai systems|union law|High-risk AI systems should only be placed on the Union market or put into service if they comply with certain mandatory requirements to ensure that they do not pose unacceptable risks to important Union public interests as recognized and protected by Union law.|d7052e6ea4a34a61a17b62c49d185099|||
important union public interests|union law|Important Union public interests, as recognized and protected by Union law, should be taken into account when regulating high-risk AI systems.|d7052e6ea4a34a61a17b62c49d185099|||
high-risk ai systems|adverse outcomes to health and safety of persons|High-risk AI systems could produce adverse outcomes to health and safety of persons, particularly when such systems operate as components of products.|d7052e6ea4a34a61a17b62c49d185099|||
union market|safe and otherwise compliant products|The objectives of Union harmonisation legislation should ensure that only safe and otherwise compliant products find their way into the Union market.|d7052e6ea4a34a61a17b62c49d185099|||
ai systems|fundamental rights protected by the charter|The extent of the adverse impact caused by AI systems on fundamental rights protected by the Charter is of particular relevance when classifying an AI system as high-risk.|e8b5ddb98493465d850e5abd37ab3701|||
high-risk ai systems|fundamental rights protected by the charter|When classifying an AI system as high-risk, it is important to consider the extent of the adverse impact caused on fundamental rights protected by the Charter.|e8b5ddb98493465d850e5abd37ab3701|||
fundamental rights protected by the charter|children|Article 24 of the EU Charter and the United Nations Convention on the Rights of the Child outline specific rights for children.|e8b5ddb98493465d850e5abd37ab3701|||
products|safety risks generated by digital components, including ai systems|It is important to prevent and mitigate safety risks generated by a product's digital components, including AI systems.|e8b5ddb98493465d850e5abd37ab3701|||
robots|safe operation in complex environments|Increasingly autonomous robots should be able to safely operate and perform their functions in complex environments.|e8b5ddb98493465d850e5abd37ab3701|||
diagnostic systems|reliability and accuracy|In the health sector, increasingly sophisticated diagnostic systems and systems supporting human decisions should be reliable and accurate.|e8b5ddb98493465d850e5abd37ab3701|||
united nations convention on the rights of the child (further elaborated in the uncrc general comment no. 25 as regards the digital environment)|children's vulnerabilities|requirement for consideration and provision of protection and care necessary for their well-being|90d3ee7491d0428f9d74fca7d8bc58d9|||
high-risk ai systems|products or systems falling within the scope of regulation (ec) no 300/2008 of the european parliament and of the council|safety components|90d3ee7491d0428f9d74fca7d8bc58d9|||
high-risk ai systems|products or systems falling within the scope of regulation (ec) no 300/2008 of the european parliament and of the council|falling within the scope|90d3ee7491d0428f9d74fca7d8bc58d9|||
high-risk ai systems|directive 2014/90/eu of the european parliament and of the council|falling within the scope|90d3ee7491d0428f9d74fca7d8bc58d9|||
high-risk ai systems|regulation (eu) no 167/2013 of the european parliament and of the council|falling within the scope|90d3ee7491d0428f9d74fca7d8bc58d9|||
regulation (eu) no 168/2013|two- or three-wheel vehicles and quadricycles|Approval and market surveillance of|445a83f9560248ae9eced5d8b5ce1787|||
directive (eu) 2016/797|european parliament and council42|Passed by|445a83f9560248ae9eced5d8b5ce1787|||
regulation (eu) 2018/858|european parliament and council43|Passed by|445a83f9560248ae9eced5d8b5ce1787|||
regulation (eu) 2018/1139|european parliament and council44|Passed by|445a83f9560248ae9eced5d8b5ce1787|||
regulation (eu) 2019/2144|european parliament and council46|Passed by|445a83f9560248ae9eced5d8b5ce1787|||
high-risk ai systems|commission|Takes into account mandatory requirements for when adopting relevant future delegated or implementing acts on the basis of certain Union harmonisation|445a83f9560248ae9eced5d8b5ce1787|||
ai systems|products falling within the scope of certain union harmonisation|Are safety components of products, or which are themselves products,|445a83f9560248ae9eced5d8b5ce1787|||
ai systems|high-risk products under this regulation|products with safety component as AI systems should be classified as high-risk pursuant to this Regulation, but the product itself may not be considered high-risk under relevant Union harmonisation legislation|7d865bd902b648ec82c93703d22d548e|||
machinery|high-risk products under this regulation|products falling within the scope of certain Union harmonisation legislation, specifically machinery, are classified as high-risk under this Regulation if they undergo conformity assessment procedure with a third-party conformity assessment body pursuant to relevant Union harmonisation legislation|7d865bd902b648ec82c93703d22d548e|||
toys|high-risk products under this regulation|products falling within the scope of certain Union harmonisation legislation, specifically toys, are classified as high-risk under this Regulation if they undergo conformity assessment procedure with a third-party conformity assessment body pursuant to relevant Union harmonisation legislation|7d865bd902b648ec82c93703d22d548e|||
lifts|high-risk products under this regulation|products falling within the scope of certain Union harmonisation legislation, specifically lifts, are classified as high-risk under this Regulation if they undergo conformity assessment procedure with a third-party conformity assessment body pursuant to relevant Union harmonisation legislation|7d865bd902b648ec82c93703d22d548e|||
equipment and protective systems intended for use in potentially explosive atmospheres|high-risk products under this regulation|products falling within the scope of certain Union harmonisation legislation, specifically equipment and protective systems intended for use in potentially explosive atmospheres, are classified as high-risk under this Regulation if they undergo conformity assessment procedure with a third-party conformity assessment body pursuant to relevant Union harmonisation legislation|7d865bd902b648ec82c93703d22d548e|||
radio equipment|high-risk products under this regulation|products falling within the scope of certain Union harmonisation legislation, specifically radio equipment, are classified as high-risk under this Regulation if they undergo conformity assessment procedure with a third-party conformity assessment body pursuant to relevant Union harmonisation legislation|7d865bd902b648ec82c93703d22d548e|||
pressure equipment|high-risk products under this regulation|products falling within the scope of certain Union harmonisation legislation, specifically pressure equipment, are classified as high-risk under this Regulation if they undergo conformity assessment procedure with a third-party conformity assessment body pursuant to relevant Union harmonisation legislation|7d865bd902b648ec82c93703d22d548e|||
recreational craft equipment|high-risk products under this regulation|products falling within the scope of certain Union harmonisation legislation, specifically recreational craft equipment, are classified as high-risk under this Regulation if they undergo conformity assessment procedure with a third-party conformity assessment body pursuant to relevant Union harmonisation legislation|7d865bd902b648ec82c93703d22d548e|||
cableway installations|high-risk products under this regulation|products falling within the scope of certain Union harmonisation legislation, specifically cableway installations, are classified as high-risk under this Regulation if they undergo conformity assessment procedure with a third-party conformity assessment body pursuant to relevant Union harmonisation legislation|7d865bd902b648ec82c93703d22d548e|||
appliances burning gaseous fuels|high-risk products under this regulation|products falling within the scope of certain Union harmonisation legislation, specifically appliances burning gaseous fuels, are classified as high-risk under this Regulation if they undergo conformity assessment procedure with a third-party conformity assessment body pursuant to relevant Union harmonisation legislation|7d865bd902b648ec82c93703d22d548e|||
medical devices|high-risk products under this regulation|products falling within the scope of certain Union harmonisation legislation, specifically medical devices, are classified as high-risk under this Regulation if they undergo conformity assessment procedure with a third-party conformity assessment body pursuant to relevant Union harmonisation legislation|7d865bd902b648ec82c93703d22d548e|||
in vitro diagnostic medical devices|high-risk products under this regulation|products falling within the scope of certain Union harmonisation legislation, specifically in vitro diagnostic medical devices, are classified as high-risk under this Regulation if they undergo conformity assessment procedure with a third-party conformity assessment body pursuant to relevant Union harmonisation legislation|7d865bd902b648ec82c93703d22d548e|||
ai systems|high-risk products under the criteria established in the relevant union harmonisation legislation that applies to the product|classification of an AI system as high-risk pursuant to this Regulation should not necessarily mean that the product whose safety component is the AI system, or the AI system itself as a product, is considered `high-risk' under the criteria established in the relevant Union harmonisation legislation that applies to the product|7d865bd902b648ec82c93703d22d548e|||
regulation (eu) 2019/2144|motor vehicles and their trailers|amending Regulation (EU) 2018/858 of the European Parliament and of the Council as regards their general safety and the protection of vehicle occupants and vulnerable road users|ac9eb430a4c744a49fd8b664c0731abf|||
regulation (eu) 2019/2144|systems, components and separate technical units intended for such vehicles|amending Regulation (EU) 2018/858 of the European Parliament and of the Council|ac9eb430a4c744a49fd8b664c0731abf|||
regulation (eu) 2018/1139|civil aviation|establishing a European Union Aviation Safety Agency and amending Regulations (EC) No 2111/2005, (EC) No 1008/2008, (EU) No 996/2010, (EU) No 376/2014 and Directives 2014/30/EU and 2014/53/EU of the European Parliament and of the Council|ac9eb430a4c744a49fd8b664c0731abf|||
regulation (eu) 2018/858|motor vehicles and their trailers|approval and market surveillance of motor vehicles and their trailers, amending Regulations (EC) No 715/2007 and (EC) No 595/2009 and repealing Directive 2007/46/EC|ac9eb430a4c744a49fd8b664c0731abf|||
regulation (eu) 2018/858|systems, components and separate technical units intended for such vehicles|approval and market surveillance of motor vehicles and their trailers, amending Regulations (EC) No 715/2007 and (EC) No 595/2009|ac9eb430a4c744a49fd8b664c0731abf|||
regulation (eu) 2018/1139|european union aviation safety agency|establishing a European Union Aviation Safety Agency|ac9eb430a4c744a49fd8b664c0731abf|||
regulation (ec) no 715/2007|motor vehicles and their trailers|approval and market surveillance of motor vehicles and their trailers, amended by Regulation (EU) 2018/858|ac9eb430a4c744a49fd8b664c0731abf|||
regulation (ec) no 715/2007|separate technical units intended for such vehicles|approval and market surveillance of motor vehicles and their trailers, amended by Regulation (EU) 2018/858|ac9eb430a4c744a49fd8b664c0731abf|||
regulation (ec) no 595/2009|motor vehicles and their trailers|type-approval of motor vehicles with respect to noise, amended by Regulation (EU) 2018/858|ac9eb430a4c744a49fd8b664c0731abf|||
regulation (ec) no 595/2009|separate technical units intended for such vehicles|type-approval of motor vehicles with respect to noise, amended by Regulation (EU) 2018/858|ac9eb430a4c744a49fd8b664c0731abf|||
directive 2007/46/ec|motor vehicles and their trailers|repealed by Regulation (EU) 2018/858|ac9eb430a4c744a49fd8b664c0731abf|||
occupants and vulnerable road users|regulation (eu) 2018/858 of the european parliament and council|amending Regulation (EU) 2018/858 of the European Parliament and Council, which is related to occupants and vulnerable road users.|e8d33c99230c419392f417a33685f21b|||
regulation (ec) no 78/2009|european parliament and council|repealing Regulations (EC) No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of the European Parliament and of the Council.|e8d33c99230c419392f417a33685f21b|||
commission regulations|(ec) no 631/2009, (eu) no 406/2010, (eu) no 672/2010, (eu) no 1003/2010, (eu) no 1005/2010, (eu) no 1008/2010, (eu) no 1009/2010, (eu) no 19/2011, (eu) no 109/2011, (eu) no 458/2011, (eu) no 65/2012, (eu) no 130/2012, (eu) no 347/2012, (eu) no 351/2012 and (eu) no 1230/2012|repealing Regulations (EC) No 78/2009, (EC) No 79/2009 and (EC) No 661/2009 of the European Parliament and of the Council and Commission Regulations.|e8d33c99230c419392f417a33685f21b|||
regulation (eu) 2017/746|council47|where a third-party conformity assessment is provided for medium-risk and high-risk products.|e8d33c99230c419392f417a33685f21b|||
high-risk ai systems|stand-alone ai systems|meaning high-risk AI systems other than those that are safety components of products, or which are themselves products.|e8d33c99230c419392f417a33685f21b|||
high-risk|harm to the health and safety or the fundamental rights of persons|it is appropriate to classify them as high-risk if, in the light of their intended purpose, they pose a high risk of harm to the health and safety or the fundamental rights of persons, taking into account both the severity of the possible harm and its probability of occurrence and they are used in a number of specifically pre-defined areas specified in the Regulation.|e8d33c99230c419392f417a33685f21b|||
ai systems intended for the remote biometric identification of natural persons|real-time' and 'post' remote biometric identification systems|should be classified as high-risk due to technical inaccuracies leading to biased results and discriminatory effects, particularly relevant for age, ethnicity, sex or disabilities.|57e2bb188c18433ba44c86f3cff8b4df|||
ai systems intended for the remote biometric identification of natural persons|real-time' and 'post' remote biometric identification systems|should be subject to specific requirements on logging capabilities and human oversight due to risks they pose.|57e2bb188c18433ba44c86f3cff8b4df|||
ai systems intended for the management and operation of critical infrastructure|ai systems intended to be used as safety components in the management and operation of road traffic and the supply of water, gas, heating and electricity|should be classified as high-risk due to failure or malfunctioning that may put at risk the life and health of persons at large scale and lead to appreciable disruptions in the ordinary conduct of social and economic activities.|57e2bb188c18433ba44c86f3cff8b4df|||
ai systems used in education or vocational training|ai systems used in determining access or assigning persons to educational and vocational training institutions or to evaluate|should be classified as high-risk envisaged also for any future amendments of the list of high-risk AI systems due to its methodology and criteria.|57e2bb188c18433ba44c86f3cff8b4df|||
persons|educational and vocational training institutions|should be considered for assigning to, as part of or as a precondition for their education. These systems may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of discrimination when improperly designed and used.|d99d20fb182f4fbf9ccdc395b693336f|||
persons|tests|should be considered for evaluating on, as part of or as a precondition for their education. Improperly designed and used tests may violate the right to education and training as well as the right not to be discriminated against and perpetuate historical patterns of discrimination.|d99d20fb182f4fbf9ccdc395b693336f|||
ai systems|high-risk|used in employment, workers management and access to self-employment, notably for the recruitment and selection of persons, for making decisions on promotion and termination and for task allocation, monitoring or evaluation of persons in work-related contractual relationships should also be classified as high-risk.|d99d20fb182f4fbf9ccdc395b693336f|||
ai systems|future career prospects and livelihoods of these persons|may appreciably impact.|d99d20fb182f4fbf9ccdc395b693336f|||
employees and persons providing services through platforms|should not be considered users within the meaning of this regulation.|Throughout the recruitment process and in the 47 Regulation (EU) 2017/745 of the European Parliament and of the Council of 5 April 2017 on medical devices, these persons should be considered for assigning to educational and vocational training institutions or to evaluate persons on tests as part of or as a precondition for their education.|d99d20fb182f4fbf9ccdc395b693336f|||
ai systems|discrimination against women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation|AI systems may perpetuate historical patterns of discrimination by monitoring the performance and behaviour of these persons.|64bd05f4c5394a2c8da1d1e934134323|||
ai systems|data protection and privacy rights|AI systems used to monitor the performance and behaviour of persons may also impact their rights to data protection and privacy.|64bd05f4c5394a2c8da1d1e934134323|||
ai systems|access to essential private and public services and benefits necessary for people to fully participate in society or to improve one's standard of living|In particular, AI systems used to evaluate the credit score or creditworthiness of persons deserve special consideration.|64bd05f4c5394a2c8da1d1e934134323|||
eu regulation 2017/745|regulation (ec) no 178/2002|EU Regulation 2017/745 amends Directive 2001/83/EC and repeals Council Directives 90/385/EEC and 93/42/EEC, while Commission Decision 2010/227/EU is also repealed.|64bd05f4c5394a2c8da1d1e934134323|||
eu regulation 2017/746|directive 98/79/ec|EU Regulation 2017/746 repeals Directive 98/79/EC and Commission Decision 2010/227/EU.|64bd05f4c5394a2c8da1d1e934134323|||
historical patterns of discrimination|women, certain age groups, persons with disabilities, or persons of certain racial or ethnic origins or sexual orientation|Historical patterns of discrimination may be perpetuated by AI systems used to monitor the performance and behaviour of these persons.|64bd05f4c5394a2c8da1d1e934134323|||
discrimination|high-risk|AI systems classified as high-risk when entailing significant risk to legal and natural persons, particularly in cases of power imbalance and potential impacts on fundamental rights guaranteed in the Charter. This applies to actions by law enforcement authorities involving certain uses of AI systems that may lead to surveillance, arrest or deprivation of liberty.|28470b58d38a485287c04d97a52d84f9|||
human dignity|high-risk|AI systems classified as high-risk when potentially infringing upon human dignity. This could occur if the AI system is not trained with high quality data, does not meet adequate requirements in terms of accuracy or robustness, or is not properly designed and tested before being put on the market or otherwise put into service.|28470b58d38a485287c04d97a52d84f9|||
effective remedy|high-risk|AI systems classified as high-risk when failing to provide an effective remedy in cases of infringement on fundamental rights guaranteed in the Charter. This could result from actions by law enforcement authorities involving certain uses of AI systems that may lead to surveillance, arrest or deprivation of liberty.|28470b58d38a485287c04d97a52d84f9|||
innovative approaches|compliant and safe ai systems|AI systems that stand to benefit from a wider use in the public administration should be classified as high-risk if they entail a significant degree of power imbalance or adverse impacts on fundamental rights guaranteed in the Charter. However, this Regulation should not hamper the development and use of compliant and safe AI systems.|28470b58d38a485287c04d97a52d84f9|||
ai systems|dispatching or establishing priority in dispatching emergency first response services|AI systems used to dispatch or establish priority in the dispatching of emergency first response services should be classified as high-risk since they make decisions in very critical situations for the life and health of persons and their property.|28470b58d38a485287c04d97a52d84f9|||
law enforcement authorities|ai systems involving certain uses|Actions by law enforcement authorities involving certain uses of AI systems are characterized by a significant degree of power imbalance and may lead to surveillance, arrest or deprivation of liberty as well as other adverse impacts on fundamental rights guaranteed in the Charter.|28470b58d38a485287c04d97a52d84f9|||
natural persons|adverse impacts on fundamental rights guaranteed in the charter|AI systems should not entail a high risk to legal and natural persons if they do not infringe upon fundamental rights guaranteed in the Charter, particularly in cases of power imbalance and potential impacts on these rights.|28470b58d38a485287c04d97a52d84f9|||
charter|significant degree of power imbalance|Actions by law enforcement authorities involving certain uses of AI systems are characterized by a significant degree of power imbalance and may lead to surveillance, arrest or deprivation of liberty as well as other adverse impacts on fundamental rights guaranteed in the Charter.|28470b58d38a485287c04d97a52d84f9|||
right to an effective remedy|fair|AI systems should provide an effective remedy in cases of infringement on fundamental rights guaranteed in the Charter, particularly in actions by law enforcement authorities involving certain uses of AI systems that may lead to surveillance, arrest or deprivation of liberty.|28470b58d38a485287c04d97a52d84f9|||
right to a fair|effective remedy|AI systems should ensure an effective remedy and a fair treatment in cases of infringement on fundamental rights guaranteed in the Charter, particularly in actions by law enforcement authorities involving certain uses of AI systems that may lead to surveillance, arrest or deprivation of liberty.|28470b58d38a485287c04d97a52d84f9|||
natural persons entering the territory of a member state or applying for visa or asylum|relevant documents of natural persons|for verifying the authenticity of|8b1babafc2204184bc187940e665946e|||
natural persons entering the territory of a member state or applying for visa or asylum|competent public authorities for the examination of applications for asylum, visa and residence permits and associated complaints|for assisting|8b1babafc2204184bc187940e665946e|||
ai systems in the area of migration, asylum and border control management covered by this regulation|directive 2013/32/eu of the european parliament and of the council49|should comply with the relevant procedural requirements set by|8b1babafc2204184bc187940e665946e|||
ai systems in the area of migration, asylum and border control management covered by this regulation|regulation (ec) no 810/2009 of the european parliament and of the council50|and other relevant legislation.|8b1babafc2204184bc187940e665946e|||
ai systems intended for the administration of justice and democratic processes|potential biases, errors and opacity|to address the risks of|8b1babafc2204184bc187940e665946e|||
ai systems intended to assist judicial authorities in researching and interpreting facts and the law and in applying the law to a concrete set of facts|democracy, rule of law, individual freedoms as well as the right to an effective remedy and to a fair trial.||8b1babafc2204184bc187940e665946e|||
ai systems intended for purely ancillary administrative activities that do not affect the actual|qualification should not extend, however, to||8b1babafc2204184bc187940e665946e|||
high risk ai system|lawful use under other acts of union law or national law compatible with union law|The fact that an AI system is classified as high risk under this Regulation should not be interpreted as indicating that the use of the system is necessarily lawful under other acts of Union law or national law compatible with Union law, such as on the protection of personal data, on the use of polygraphs and similar tools or other systems to detect the emotional state of natural persons. Any such use should continue to occur solely in accordance with the applicable requirements resulting from the Charter and from the applicable acts of secondary Union law and national law.|4797b590afe043db88aa8525fcb0cb0b|||
high risk ai system|applicable requirements resulting from the charter and from the applicable acts of secondary union law and national law|This Regulation should not be understood as providing for the legal ground for processing of personal data.|4797b590afe043db88aa8525fcb0cb0b|||
intended for purely ancillary administrative activities|actual administration of justice in individual cases|Such as anonymisation or pseudonymisation of judicial decisions, documents or data, communication between personnel, administrative tasks or allocation of resources.|4797b590afe043db88aa8525fcb0cb0b|||
directive 2013/32/eu|community code on visas (visa code)|Regulation (EC) No 810/2009 of the European Parliament and of the Council.|4797b590afe043db88aa8525fcb0cb0b|||
high-risk ai systems|union market|should not be understood as providing for the legal ground for processing of personal data, including special categories of personal data, where relevant.|fedec73a5cac4eaa80689262f1586ea8|||
high-risk ai systems|users and affected persons|should mitigate the risks from high-risk AI systems placed or otherwise put into service on the Union market for users and affected persons, taking into account the intended purpose of the use of the system and according to the risk management system to be established by the provider.|fedec73a5cac4eaa80689262f1586ea8|||
high-risk ai systems|union law|should not be understood as providing for the legal ground for processing of personal data, including special categories of personal data, where relevant.|fedec73a5cac4eaa80689262f1586ea8|||
high-risk ai systems|discrimination prohibited by union law|high data quality is essential for the performance of many AI systems, especially when techniques involving the training of models are used, with a view to ensure that the high-risk AI system performs as intended and safely and it does not become the source of discrimination prohibited by Union law.|fedec73a5cac4eaa80689262f1586ea8|||
high-risk ai systems|appropriate data governance|high quality training, validation and testing data sets require the implementation of appropriate data governance and|fedec73a5cac4eaa80689262f1586ea8|||
high-risk ai systems|data governance and management practices|requirement for implementing appropriate data governance and management practices in developing high-risk AI systems|42f70410934147f2b04050c4b2b8e69d|||
training, validation and testing data sets|high-risk ai systems|need for sufficient relevance, representation, freedom from errors and completeness of training, validation and testing data sets in view of the intended purpose of high-risk AI systems|42f70410934147f2b04050c4b2b8e69d|||
training, validation and testing data sets|statistical properties|requirement for appropriate statistical properties, including as regards persons or groups of persons in training, validation and testing data sets|42f70410934147f2b04050c4b2b8e69d|||
training, validation and testing data sets|specific geographical, behavioural or functional setting or context|need to take into account the features, characteristics or elements that are particular to the specific geographical, behavioural or functional setting or context in training, validation and testing data sets|42f70410934147f2b04050c4b2b8e69d|||
providers|special categories of personal data|ability to process special categories of personal data for bias monitoring, detection and correction in relation to high-risk AI systems as a matter of substantial public interest|42f70410934147f2b04050c4b2b8e69d|||
certain actors|high-quality datasets within their respective fields of activities|ability to access and use high-quality datasets related to high-risk AI systems for development by certain actors, such as providers, notified bodies, digital innovation hubs, testing experimentation facilities and researchers|42f70410934147f2b04050c4b2b8e69d|||
european common data spaces established by the commission|high-risk ai systems|Having information on how high-risk AI systems have been developed and how they perform throughout their lifecycle is essential to verify compliance with the requirements under this Regulation. This requires keeping records and the availability of a technical documentation, containing information which is necessary to assess the compliance of the AI system with the relevant requirements. European common data spaces established by the Commission facilitate non-discriminatory access to high quality data for the training, validation and testing of AI systems, thus contributing to the verification of compliance with the requirements under this Regulation.|98d889238bfb4f3f98ae515d407c9053|||
high-risk ai systems|relevant competent authorities, including sectoral ones|Providing or supporting the access to data may also support the provision of high-quality data for the training, validation and testing of AI systems.|98d889238bfb4f3f98ae515d407c9053|||
compliance of the ai system with the relevant requirements|general characteristics, capabilities and limitations of the system|represents the inherent relationship between the AI system's compliance with the relevant requirements and its general characteristics, capabilities and limitations. This relationship signifies that an AI system's conformity to the requisite regulatory standards is intrinsically linked to its fundamental attributes.|e91c71902afa474cb120f0049dcb7d68|||
general characteristics, capabilities and limitations of the system|algorithms, data, training, testing and validation processes used|represents the causal relationship between an AI system's fundamental attributes and its underlying algorithms, data, training, testing and validation processes. This relation denotes that an AI system's core functionalities and procedures are inextricably linked to its essential characteristics.|e91c71902afa474cb120f0049dcb7d68|||
algorithms, data, training, testing and validation processes used|relevant risk management system|represents the causal relationship between an AI system's underlying algorithms, data, training, testing, and validation processes and its pertinent risk management system. This relation indicates that an AI system's fundamental functionalities and procedures are intrinsically linked to its relevant risk management system.|e91c71902afa474cb120f0049dcb7d68|||
technical documentation|general characteristics, capabilities and limitations of the system|represents the causal relationship between an AI system's technical documentation and its fundamental attributes. This relation signifies that an AI system's technical documentation is inextricably linked to its essential characteristics.|e91c71902afa474cb120f0049dcb7d68|||
technical documentation|algorithms, data, training, testing and validation processes used|represents the causal relationship between an AI system's technical documentation and its underlying algorithms, data, training, testing, and validation processes. This relation denotes that an AI system's technical documentation is intrinsically linked to its fundamental functionalities and procedures.|e91c71902afa474cb120f0049dcb7d68|||
technical documentation|relevant risk management system|represents the causal relationship between an AI system's technical documentation and its pertinent risk management system. This relation signifies that an AI system's technical documentation is intrinsically linked to its relevant risk management system.|e91c71902afa474cb120f0049dcb7d68|||
users|transparency of high-risk ai systems|represents the causal relationship between users and the degree of transparency required for high-risk AI systems. This relation denotes that the users' interaction with high-risk AI systems is intrinsically linked to their level of comprehensibility and complexity.|e91c71902afa474cb120f0049dcb7d68|||
high-risk ai systems|relevant documentation and instructions for use|represents the causal relationship between high-risk AI systems and their accompanying documentation and instructions for use. This relation denotes that high-risk AI systems' functionality and performance are intrinsically linked to their relevant documentation and instructions for use.|e91c71902afa474cb120f0049dcb7d68|||
high-risk ai systems|concise and clear information including risks to fundamental rights and discrimination|represents the causal relationship between high-risk AI systems and their concise and clear information regarding potential risks to fundamental rights and discrimination. This relation signifies that high-risk AI systems' functionality and performance are intrinsically linked to their pertinent risks to fundamental rights and discrimination.|e91c71902afa474cb120f0049dcb7d68|||
provider of the system|human oversight measures|represents the causal relationship between the provider of the system and the human oversight measures required for high-risk AI systems. This relation denotes that the providers' responsibility for high-risk AI systems is intrinsically linked to their pertinent human oversight measures.|e91c71902afa474cb120f0049dcb7d68|||
human oversight|natural persons|represents the causal relationship between human oversight and the natural persons assigned to oversee high-risk AI systems' functioning. This relation denotes that human oversight's effectiveness is intrinsically linked to the involvement of natural persons.|e91c71902afa474cb120f0049dcb7d68|||
human oversight|in-built operational constraints that cannot be overridden by the system itself|represents the causal relationship between human oversight and the in-built operational constraints required for high-risk AI systems. This relation denotes that human oversight's effectiveness is intrinsically linked to the pertinent in-built operational constraints.|e91c71902afa474cb120f0049dcb7d68|||
human oversight|responsive to the human operator|represents the causal relationship between human oversight and the high-risk AI systems' responsiveness to human operators. This relation signifies that human oversight's effectiveness is intrinsically linked to the pertinent responsiveness to human operators.|e91c71902afa474cb120f0049dcb7d68|||
natural persons to whom human oversight has been assigned|high-risk ai systems|These individuals have the necessary competence, training, and authority to oversee high-risk AI systems.|382ad8fd65024094a9b91805bb420047|||
high-risk ai systems|appropriate level of accuracy, robustness, and cybersecurity|High-risk AI systems should meet an appropriate level of accuracy, robustness, and cybersecurity in accordance with the generally acknowledged state of the art.|382ad8fd65024094a9b91805bb420047|||
high-risk ai systems|level of accuracy and accuracy metrics|The level of accuracy and accuracy metrics should be communicated to the users of high-risk AI systems.|382ad8fd65024094a9b91805bb420047|||
technical robustness|high-risk ai systems|High-risk AI systems should be resilient against risks connected to the limitations of the system, as well as against malicious actions that may compromise their security and result in harmful or otherwise undesirable behavior.|382ad8fd65024094a9b91805bb420047|||
cybersecurity|high-risk ai systems|Cybersecurity plays a crucial role in ensuring that high-risk AI systems are resilient against attempts to alter their use, behavior, performance, or compromise their security properties by malicious third parties exploiting the system's vulnerabilities.|382ad8fd65024094a9b91805bb420047|||
cyberattacks|ai specific assets|Cyberattacks against AI systems can leverage AI-specific assets, such as training data sets or trained models, or exploit vulnerabilities in the system.|382ad8fd65024094a9b91805bb420047|||
high-risk ai systems|union harmonisation legislation|rules applicable to the placing on the market, putting into service and use of high-risk AI systems should be laid down consistently with Regulation (EC) No 765/2008 of the European Parliament and of the Council setting out the requirements for accreditation and the market surveillance of products, Decision No 768/2008/EC on a common framework for the marketing of products and Regulation (EU) 2019/1020 of the European Parliament and of the Council on market surveillance and compliance of products (`New Legislative Framework for the marketing of products').|a74c433783f14db286a584fa128c984c|||
high-risk ai systems|providers|a specific natural or legal person, defined as the provider, takes the responsibility for the placing on the market or putting into service of a high-risk AI system, regardless of whether that natural or legal person is the person who designed or developed the system.|a74c433783f14db286a584fa128c984c|||
union harmonisation legislation|regulation (ec) no 765/2008|rules applicable to the placing on the market, putting into service and use of high-risk AI systems should be laid down consistently with Regulation (EC) No 765/2008 of the European Parliament and of the Council setting out the requirements for accreditation and the market surveillance of products.|a74c433783f14db286a584fa128c984c|||
union harmonisation legislation|decision no 768/2008/ec|rules applicable to the placing on the market, putting into service and use of high-risk AI systems should be laid down consistently with Decision No 768/2008/EC of the European Parliament and of the Council on a common framework for the marketing of products.|a74c433783f14db286a584fa128c984c|||
union harmonisation legislation|regulation (eu) 2019/1020|rules applicable to the placing on the market, putting into service and use of high-risk AI systems should be laid down consistently with Regulation (EU) 2019/1020 of the European Parliament and of the Council on market surveillance and compliance of products (`New Legislative Framework for the marketing of products').|a74c433783f14db286a584fa128c984c|||
high-risk ai systems|cybersecurity|suitable measures should therefore be taken by the providers of high-risk AI systems, also taking into account as appropriate the underlying ICT infrastructure, to ensure a level of cybersecurity appropriate to the risks.|a74c433783f14db286a584fa128c984c|||
high-risk ai systems|ict infrastructure|suitable measures should therefore be taken by the providers of high-risk AI systems, also taking into account as appropriate the underlying ICT infrastructure.|a74c433783f14db286a584fa128c984c|||
high-risk ai systems|security vulnerabilities|sets (e.g. Data poisoning) or trained models (e.g. Adversarial attacks), or exploit vulnerabilities in the AI system's digital assets or the underlying ICT infrastructure.|a74c433783f14db286a584fa128c984c|||
union harmonisation legislation|new legislative framework for the marketing of products'|rules applicable to the placing on the market, putting into service and use of high-risk AI systems should be laid down consistently with Regulation (EU) 2019/1020 of the European Parliament and of the Council on market surveillance and compliance of products (`New Legislative Framework for the marketing of products').|a74c433783f14db286a584fa128c984c|||
provider|natural or legal person|The provider should establish a sound quality management system, ensure the accomplishment of the required conformity assessment procedure, draw up the relevant documentation and establish a robust post-market monitoring system for natural or legal persons. This indicates that the terms 'provider' and 'natural or legal person' are related in the context.|5b70a90f04f84a95a305684a0442c056|||
high-risk ai system|public authority|Public authorities which put into service high-risk AI systems for their own use may adopt and implement the rules for the quality management system as part of the quality management system adopted at a national or regional level, as appropriate, taking into account the specificities of the sector and the competences and organisation of the public authority in question.|5b70a90f04f84a95a305684a0442c056|||
manufacturer|final product as defined under the relevant new legislative framework legislation|Where a high-risk AI system that is a safety component of a product which is covered by a relevant New Legislative Framework sectorial legislation is not placed on the market or put into service independently from the product, the manufacturer of the final product as defined under the relevant New Legislative Framework legislation should comply with the obligations of the provider established in this Regulation and notably ensure that the AI system embedded in the final product complies with the requirements of this Regulation.|5b70a90f04f84a95a305684a0442c056|||
relevant economic operators|importers, distributors|specific obligations should be set to ensure legal certainty and facilitate regulatory compliance by those relevant operators|1cf3f0afb01f4253857df57e478bb4a2|||
providers established outside the union|authorised representative established in the union|by written mandate, appoint|1cf3f0afb01f4253857df57e478bb4a2|||
ai systems|person established in the union|under all circumstances, a person established in the Union can provide authorities with all the necessary information on the compliance of an AI system.|1cf3f0afb01f4253857df57e478bb4a2|||
accreditation and market surveillance|regulation (ec) no 765/2008 of the european parliament and of the council|setting out the requirements for accreditation and market surveillance relating to the marketing of products and repealing Regulation (EEC) No 339/93|1cf3f0afb01f4253857df57e478bb4a2|||
common framework for the marketing of products|decision no 768/2008/ec of the european parliament and of the council|on a common framework for the marketing of products, and repealing Council Decision 93/465/EEC|1cf3f0afb01f4253857df57e478bb4a2|||
market surveillance and compliance of products|regulation (eu) 2019/1020 of the european parliament and of the council|of 20 June 2019 on market surveillance and compliance of products and amending Directive 2004/42/EC and Regulations (EC)|1cf3f0afb01f4253857df57e478bb4a2|||
ai systems|users|Given the nature of AI systems and the risks to safety and fundamental rights possibly associated with their use, it is appropriate to set specific responsibilities for users. Users should in particular use high-risk AI systems in accordance with the instructions of use and certain other obligations should be provided for with regard to monitoring of the functioning of the AI systems and with regard to record-keeping, as appropriate.|9bc77a6a0e504fb3a67217a52ea92b89|||
users|natural or legal person, public authority, agency or other body|The user of the AI system should be the natural or legal person, public authority, agency or other body under whose authority the AI system is operated except where the use is made in the course of a personal non-professional activity.|9bc77a6a0e504fb3a67217a52ea92b89|||
ai systems|relevant third parties|In the light of the complexity of the artificial intelligence value chain, relevant third parties, notably the ones involved in the sale and the supply of software, software tools and components, pre-trained models and data, or providers of network services,|9bc77a6a0e504fb3a67217a52ea92b89|||
high-risk ai systems|conformity assessment|Prior to their placing on the market or putting into service, high-risk AI systems should undergo a conformity assessment.|4870966a47bf482abedf57f1c77e7d7a|||
high-risk ai systems|union harmonisation legislation|High-risk AI systems related to products covered by existing Union harmonisation legislation following the New Legislative Framework approach should also comply with the requirements of this Regulation as part of their conformity assessment already foreseen under that legislation.|4870966a47bf482abedf57f1c77e7d7a|||
standardisation|compliance|Standardisation should play a key role to provide technical solutions for providers to ensure compliance with this Regulation.|4870966a47bf482abedf57f1c77e7d7a|||
harmonised standards|compliance|Compliance with harmonised standards as defined in Regulation (EU) No 1025/2012 of the European Parliament and of the Council should be a means for providers to demonstrate conformity with the requirements of this Regulation.|4870966a47bf482abedf57f1c77e7d7a|||
common technical specifications|compliance|The Commission could adopt common technical specifications in areas where no harmonised standards exist or where they are insufficient.|4870966a47bf482abedf57f1c77e7d7a|||
ai systems|high-risk ai systems other than those related to products|In an initial phase of application of this Regulation, the conformity assessment for high-risk AI systems other than those related to products should be carried out by the provider under its own responsibility, as opposed to third-party conformity assessment.|cac3544aeb8942caba05a5b02cd3688f|||
ai systems intended to be used for remote biometric identification of persons|notified bodies|For AI systems intended to be used for remote biometric identification of persons, the involvement of a notified body in the conformity assessment should be foreseen, to the extent they are not prohibited.|cac3544aeb8942caba05a5b02cd3688f|||
notified bodies|national competent authorities|In order to carry out third-party conformity assessment for AI systems intended to be used for remote biometric identification of persons, notified bodies should be designated under this Regulation by the national competent authorities.|cac3544aeb8942caba05a5b02cd3688f|||
ai systems|substantial modification|An AI system undergoes a new conformity assessment whenever a change occurs which may affect the conformity of the AI system, in line with the commonly established notion of substantial modification for products regulated by Union harmonisation legislation.|cac3544aeb8942caba05a5b02cd3688f|||
ai systems which continue to 'learn'|changes to the algorithm and its performance|in AI systems which continue to 'learn' after being placed on the market or put into service, it is necessary to provide rules establishing that changes to the algorithm and its performance that have been pre-determined by the provider and assessed at the moment of the conformity assessment should not constitute a substantial modification.|cac4d1d0feb04acd8f924a7aa23de22d|||
conformity with this regulation|high-risk ai systems|High-risk AI systems should bear the CE marking to indicate their conformity with this Regulation so that they can move freely within the internal market.|cac4d1d0feb04acd8f924a7aa23de22d|||
placing on the market or putting into service of high-risk ai systems|member states|Member States should not create unjustified obstacles to the placing on the market or putting into service of high-risk AI systems that comply with the requirements laid down in this Regulation and bear the CE marking.|cac4d1d0feb04acd8f924a7aa23de22d|||
placing on the market or putting into service of ai systems|exceptional reasons of public security or protection of life and health of natural persons and the protection of industrial and commercial property|Under exceptional reasons of public security or protection of life and health of natural persons and the protection of industrial and commercial property, Member States could authorise the placing on the market or putting into service of AI systems which have not undergone a conformity assessment.|cac4d1d0feb04acd8f924a7aa23de22d|||
ai systems|high-risk ai systems|In order to facilitate the work of the Commission and the Member States in the artificial intelligence field as well as to increase the transparency towards the public, providers of high-risk AI systems should be required to register their high-risk AI system in a EU database, established and managed by the Commission. The Commission should be the controller of that database.|e6de69c6498b498dbe7a941b724a2da1|||
high-risk ai systems|ai systems related to products falling within the scope of relevant existing union harmonisation legislation|The procedure for setting the database should include the elaboration of functional specifications by the Commission and an independent audit report, in order to ensure the full functionality of the database when deployed.|e6de69c6498b498dbe7a941b724a2da1|||
ai systems|certain ai systems intended to interact with natural persons or to generate content|Certain AI systems intended to interact with natural persons or to generate content may pose specific risks of impersonation or deception irrespective of whether they qualify as high-risk or not.|e6de69c6498b498dbe7a941b724a2da1|||
certain ai systems intended to interact with natural persons or to generate content|specific transparency obligations|In certain circumstances, the use of these systems should therefore be subject to specific transparency obligations without prejudice to the|e6de69c6498b498dbe7a941b724a2da1|||
artificial intelligence|rapidly developing family of technologies|Artificial intelligence is a rapidly developing family of technologies that requires novel forms of regulatory oversight and a safe space for experimentation, while ensuring responsible innovation and integration of appropriate safeguards and risk mitigation measures.|da26b9f0a13b436695d00dd12d0fa8dc|||
novel forms of regulatory oversight|artificial intelligence|To ensure a legal framework that is innovation-friendly, future-proof and resilient to disruption, national competent authorities from one or more Member States should be encouraged to establish artificial intelligence regulatory sandboxes, as well as other forms of regulatory support mechanisms for the responsible development and testing of AI applications.|da26b9f0a13b436695d00dd12d0fa8dc|||
artificial intelligence|novel forms of regulatory oversight|To ensure a legal framework that is innovation-friendly, future-proof and resilient to disruption, national competent authorities from one or more Member States should be encouraged to establish artificial intelligence regulatory sandboxes, as well as other forms of regulatory support mechanisms for the responsible development and testing of AI applications.|da26b9f0a13b436695d00dd12d0fa8dc|||
national competent authorities|artificial intelligence|National competent authorities from one or more Member States should be encouraged to establish artificial intelligence regulatory sandboxes, as well as other forms of regulatory support mechanisms for the responsible development and testing of AI applications.|da26b9f0a13b436695d00dd12d0fa8dc|||
responsible innovation|artificial intelligence|Artificial intelligence should be developed and deployed in accordance with values, principles and applicable law concerning fundamental rights and democratic values, such as dignity, security, data protection, non-discrimination and equality, as well as principles of transparency, fairness and accountability.|da26b9f0a13b436695d00dd12d0fa8dc|||
responsible innovation|artificial intelligence|To ensure a legal framework that is innovation-friendly, future-proof and resilient to disruption, national competent authorities from one or more Member States should be encouraged to establish artificial intelligence regulatory sandboxes, as well as other forms of regulatory support mechanisms for the responsible development and testing of AI applications.|da26b9f0a13b436695d00dd12d0fa8dc|||
risk mitigation measures|artificial intelligence|Artificial intelligence should be developed and deployed in accordance with values, principles and applicable law concerning fundamental rights and democratic values, such as dignity, security, data protection, non-discrimination and equality, as well as principles of transparency, fairness and accountability.|da26b9f0a13b436695d00dd12d0fa8dc|||
risk mitigation measures|artificial intelligence|To ensure a legal framework that is innovation-friendly, future-proof and resilient to disruption, national competent authorities from one or more Member States should be encouraged to establish artificial intelligence regulatory sandboxes, as well as other forms of regulatory support mechanisms for the responsible development and testing of AI applications.|da26b9f0a13b436695d00dd12d0fa8dc|||
users|ai system|Natural persons should be notified that they are interacting with an AI system, unless this is obvious from the circumstances and the context of use.|da26b9f0a13b436695d00dd12d0fa8dc|||
natural persons|emotion recognition system|Natural persons should be notified when they are exposed to an emotion recognition system or a biometric categorisation system.|da26b9f0a13b436695d00dd12d0fa8dc|||
natural persons|ai system generates or manipulates image, audio or video content that appreciably resembles existing persons, places or events and would falsely appear to a person to be authentic|Users, who use an AI system to generate or manipulate image, audio or video content that appreciably resembles existing persons, places or events and would falsely appear to a person to be authentic, should disclose that the content has been artificially created or manipulated by labelling the artificial intelligence output accordingly and disclosing its artificial origin.|da26b9f0a13b436695d00dd12d0fa8dc|||
users|ai system generates or manipulates image, audio or video content that appreciably resembles existing persons, places or events and would falsely appear to a person to be authentic|Users, who use an AI system to generate or manipulate image, audio or video content that appreciably resembles existing persons, places or events and would falsely appear to a person to be authentic, should disclose that the content has been artificially created or manipulated by labelling the artificial intelligence output accordingly and disclosing its artificial origin.|da26b9f0a13b436695d00dd12d0fa8dc|||
users|ai system|Natural persons should be notified that they are interacting with an AI system, unless this is obvious from the circumstances and the context of use.|da26b9f0a13b436695d00dd12d0fa8dc|||
ai regulatory sandboxes|member states|Member States should be encouraged to establish AI regulatory sandboxes to facilitate the development and testing of innovative AI systems under strict regulatory oversight before these systems are placed on the market or otherwise put into service. (72)|117a72c80380466fa6b0cbd53d41d64d|||
ai innovation|ai regulatory sandboxes|The objectives of the regulatory sandboxes should be to foster AI innovation by establishing a controlled experimentation and testing environment in the development and pre-marketing phase with a view to ensuring compliance of the innovative AI systems with this Regulation and other relevant Union and Member States legislation; (72)|117a72c80380466fa6b0cbd53d41d64d|||
ai systems|innovative ai systems|These systems are placed on the market or otherwise put into service. (72)|117a72c80380466fa6b0cbd53d41d64d|||
legal certainty|ai innovation|enhance legal certainty for innovators and the competent authorities' oversight and understanding of the opportunities, emerging risks and the impacts of AI use. (72)|117a72c80380466fa6b0cbd53d41d64d|||
small and medium enterprises|ai innovation|accelerate access to markets, including by removing barriers for small and medium enterprises (SMEs) and start-ups. (72)|117a72c80380466fa6b0cbd53d41d64d|||
common rules|ai regulatory sandboxes' implementation|To ensure uniform implementation across the Union and economies of scale, it is appropriate to establish common rules for the regulatory sandboxes' implementation and a framework for cooperation between the relevant authorities involved in the supervision of the sandboxes. (72)|117a72c80380466fa6b0cbd53d41d64d|||
personal data|ai systems|The use of personal data collected for other purposes for developing certain AI systems in the public interest within the AI regulatory sandbox, in line with Article 6(4) of Regulation (EU) 2016/679, and Article 6 of Regulation. (72)|117a72c80380466fa6b0cbd53d41d64d|||
member states|operators|Member States should develop initiatives targeted at operators, including awareness raising and information communication.|22b614d9271c43edaadaf03f8dfd0e90|||
small-scale providers|notified bodies|when Notified Bodies set conformity assessment fees, the specific interests and needs of small-scale providers shall be taken into account|22b614d9271c43edaadaf03f8dfd0e90|||
documentation|communication with authorities|Translation costs related to mandatory documentation and communication with authorities may constitute a significant cost for providers and other operators, notably those of a smaller scale.|22b614d9271c43edaadaf03f8dfd0e90|||
one language|relevant providers' documentation and for communication with operators|Member States should possibly ensure that one of the languages determined and accepted by them for relevant providers' documentation and for communication with operators is one which is broadly understood by the largest possible number of cross-border users.|22b614d9271c43edaadaf03f8dfd0e90|||
ai-on demand platform|european digital innovation hubs|The AI-on demand platform, the European Digital Innovation Hubs and the Testing and Experimentation Facilities established by the Commission and the Member States at national or EU level should possibly contribute to the implementation of this Regulation.|22b614d9271c43edaadaf03f8dfd0e90|Testing and Experimentation Facilities||
providers|notified bodies|In order to minimise the risks to implementation resulting from lack of knowledge and expertise in the market as well as to facilitate compliance of providers and notified bodies with their obligations under this Regulation, the AI-on demand platform, the European Digital Innovation Hubs and the Testing and Experimentation Facilities established by the Commission and the Member States at national or EU level should possibly contribute to the implementation of this Regulation.|22b614d9271c43edaadaf03f8dfd0e90|||
mission and fields of competence|union harmonisation legislation|Providers and notified bodies may provide technical and scientific support related to mission and fields of competence as well as Union harmonisation legislation.|e5cc155c92db4b088d2ad3641eb031d8|||
testing and experimentation facilities|union harmonisation legislation|Bodies, groups or laboratories established or accredited pursuant to any relevant Union harmonisation legislation may have access to Testing and Experimentation Facilities when fulfilling tasks in the context of conformity assessment of products or devices covered by that Union harmonisation legislation.|e5cc155c92db4b088d2ad3641eb031d8|||
expert panels|union harmonisation legislation (regulation (eu) 2017/745 and regulation (eu) 2017/746)|Expert panels are established in the field of medical devices pursuant to Regulation (EU) 2017/745 and Regulation (EU) 2017/746 for tasks related to conformity assessment of products or devices covered by that Union harmonisation legislation.|e5cc155c92db4b088d2ad3641eb031d8|||
expert laboratories|union harmonisation legislation (regulation (eu) 2017/745 and regulation (eu) 2017/746)|Expert laboratories are established in the field of medical devices pursuant to Regulation (EU) 2017/745 and Regulation (EU) 2017/746 for tasks related to conformity assessment of products or devices covered by that Union harmonisation legislation.|e5cc155c92db4b088d2ad3641eb031d8|||
reference laboratories|union harmonisation legislation (regulation (eu) 2017/745 and regulation (eu) 2017/746)|Reference laboratories are established in the field of medical devices pursuant to Regulation (EU) 2017/745 and Regulation (EU) 2017/746 for tasks related to conformity assessment of products or devices covered by that Union harmonisation legislation.|e5cc155c92db4b088d2ad3641eb031d8|||
commission|european artificial intelligence board|The Commission establishes the European Artificial Intelligence Board to facilitate a smooth, effective and harmonised implementation of this Regulation.|e5cc155c92db4b088d2ad3641eb031d8|||
european artificial intelligence board|matters related to the implementation of this regulation|The European Artificial Intelligence Board is responsible for advisory tasks, including issuing opinions, recommendations, advice or guidance on matters related to the implementation of this Regulation.|e5cc155c92db4b088d2ad3641eb031d8|||
european artificial intelligence board|technical specifications or existing standards regarding the requirements established in this regulation|The European Artificial Intelligence Board provides advice to and assists the Commission on specific questions related to artificial intelligence regarding technical specifications or existing standards regarding the requirements established in this Regulation.|e5cc155c92db4b088d2ad3641eb031d8|||
member states|application and enforcement of this regulation|Each Member State holds a key role in the application and enforcement of this Regulation.|e5cc155c92db4b088d2ad3641eb031d8|||
national competent authorities|each member state|In this respect, each Member State should designate one or more national competent authorities for the purpose of supervising the application and implementation of this Regulation.|d108139702c743a6b01aa9a70a78c95a|||
national authority|one national authority should be designated as national supervisory authority in each member state|In order to increase organisation efficiency on the side of Member States and to set an official point of contact vis-à-vis the public and other counterparts at Member State and Union levels,...|d108139702c743a6b01aa9a70a78c95a|||
providers|all providers|All providers should have a post-market monitoring system in place.|d108139702c743a6b01aa9a70a78c95a|||
serious incidents|providers|Providers should also be required to have a system in place to report to the relevant authorities any serious incidents...|d108139702c743a6b01aa9a70a78c95a|||
breaches|providers|... Or any breaches to national and Union law protecting fundamental rights resulting from the use of their AI systems.|d108139702c743a6b01aa9a70a78c95a|||
ai systems|union harmonisation legislation|The use of AI systems falls under the scope of Union harmonisation legislation to ensure appropriate and effective enforcement of requirements and obligations set out by this Regulation.|adedc050a2a547ca998ad064b07d1fce|||
national public authorities or bodies|union law protecting fundamental rights|National public authorities or bodies, which supervise the application of Union law protecting fundamental rights, including equality bodies, should have access to any documentation created under this Regulation where necessary for their mandate.|adedc050a2a547ca998ad064b07d1fce|||
union legislation on financial services|ai systems provided or used by regulated and|In order to ensure coherent application and enforcement of obligations under this Regulation and relevant rules and requirements of the Union financial services legislation, authorities responsible for supervision and enforcement of financial services legislation should be designated as competent authorities for the purpose of supervising the implementation of this Regulation, including for market surveillance activities.|adedc050a2a547ca998ad064b07d1fce|||
surveillance activities|ai systems provided or used by regulated and supervised financial institutions|as regards|54b7ca0ea74b4176b97f30932139c750|||
regulation|credit institutions regulated under directive 2013/36/eu of the european parliament and of the council|enhance consistency between|54b7ca0ea74b4176b97f30932139c750|||
conformity assessment procedure|existing obligations and procedures under directive 2013/36/eu|integrate|54b7ca0ea74b4176b97f30932139c750|||
risk management, post marketing monitoring and documentation|existing obligations and procedures under directive 2013/36/eu|enhance consistency between|54b7ca0ea74b4176b97f30932139c750|||
quality management system of providers|monitoring obligation placed on users of high-risk ai systems|limited derogations should also be envisaged in relation to the extent that these apply to credit institutions regulated by Directive 2013/36/EU|54b7ca0ea74b4176b97f30932139c750|||
trustworthy artificial intelligence|larger uptake of trustworthy artificial intelligence in the union|The development of AI systems other than high-risk AI systems in accordance with the requirements of this Regulation may lead to|54b7ca0ea74b4176b97f30932139c750|||
mandatory requirements applicable to high-risk ai systems|codes of conduct intended to foster the voluntary application|Providers of non-high-risk AI systems should be encouraged to create|54b7ca0ea74b4176b97f30932139c750|||
mandatory requirements applicable to high-risk ai systems|additional requirements related, for example, to environmental sustainability, accessibility to persons with disability, stakeholders'|Providers should also be encouraged to apply on a voluntary basis additional requirements related, for example,|54b7ca0ea74b4176b97f30932139c750|||
council57 safety net|trustful and constructive cooperation of competent authorities on union and national level|The Council57 would apply as a safety net to ensure trustful and constructive cooperation of competent authorities on Union and national level.|67e697b96205456f8bd849ae3cd7e755|||
member states|effective, proportionate and dissuasive penalties for infringement|Member States should take all necessary measures to ensure that the provisions of this Regulation are implemented, including by laying down effective, proportionate and dissuasive penalties for their infringement.|67e697b96205456f8bd849ae3cd7e755|||
european data protection supervisor|power to impose fines on union institutions, agencies and bodies falling within the scope of this regulation|The European Data Protection Supervisor should have the power to impose fines on Union institutions, agencies and bodies falling within the scope of this Regulation.|67e697b96205456f8bd849ae3cd7e755|||
annex i techniques and approaches|commission|The power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the techniques and approaches referred to in Annex I to define AI systems,|67e697b96205456f8bd849ae3cd7e755|||
union harmonisation legislation listed in annex ii|commission|The power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the Union harmonisation legislation listed in Annex II,|67e697b96205456f8bd849ae3cd7e755|||
high-risk ai systems listed in annex iii|commission|The power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the high-risk AI systems listed in Annex III,|67e697b96205456f8bd849ae3cd7e755|||
provisions regarding technical documentation listed in annex iv|commission|The power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the provisions regarding technical documentation listed in Annex IV,|67e697b96205456f8bd849ae3cd7e755|||
eu declaration of conformity in annex v|commission|The power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the EU declaration of conformity in Annex V,|67e697b96205456f8bd849ae3cd7e755|||
provisions regarding conformity assessment procedures in annex vi and vii|commission|The power to adopt acts in accordance with Article 290 TFEU should be delegated to the Commission to amend the provisions regarding conformity assessment procedures in Annex VI and VII,|67e697b96205456f8bd849ae3cd7e755|||
conformity assessment procedures in annex vi and vii|high-risk ai systems|The conformity assessment procedures in Annex VI and VII should apply to the high-risk AI systems.|509658938c864f5cbcd25d9459f80b76|||
principles laid down in the interinstitutional agreement of 13 april 2016 on better law-making|appropriate consultations during its preparatory work|The Commission should carry out appropriate consultations during its preparatory work in accordance with the principles laid down in the Interinstitutional Agreement of 13 April 2016 on Better Law-Making.|509658938c864f5cbcd25d9459f80b76|||
european parliament and council|member states' experts|The European Parliament and the Council should receive all documents at the same time as Member States' experts, and their experts should systematically have access to meetings of Commission expert groups dealing with the preparation of delegated acts.|509658938c864f5cbcd25d9459f80b76|||
regulation (eu) no 182/2011 of the european parliament and of the council|implementing powers should be conferred on the commission|Those powers should be exercised in accordance with Regulation (EU) No 182/2011 of the European Parliament and of the Council.|509658938c864f5cbcd25d9459f80b76|||
union|third country|providers located in can be either in Union or a third country, implying some form of interdependence between the two locations|58c1c5ec653648129566cd9fd0bc7dc5|||
users of ai systems|union|located within the Union implies that these users are based in or accessing the Union, indicating a relationship between users and the location they operate from|58c1c5ec653648129566cd9fd0bc7dc5|||
providers and users of ai systems|third country|output produced by system is used in Union, signifying that there is some kind of interaction or exchange between the two locations|58c1c5ec653648129566cd9fd0bc7dc5|||
ai systems|safety components of products or systems, or which are themselves products or systems|falling within the scope of specific acts, implying a regulatory framework and possible compliance requirements between these categories|58c1c5ec653648129566cd9fd0bc7dc5|||
ai systems|military purposes|developed or used exclusively for military purposes, indicating that the use case can affect the nature of AI and its applications|58c1c5ec653648129566cd9fd0bc7dc5|||
public authorities in a third country|union or one or more member states|use AI systems in the framework of international agreements, implying some form of collaboration or exchange between these entities|58c1c5ec653648129566cd9fd0bc7dc5|||
public authorities in a third country|international organizations falling within the scope of this regulation|use AI systems in the framework of international agreements for law enforcement and judicial cooperation, indicating a relationship between these entities|58c1c5ec653648129566cd9fd0bc7dc5|||
this regulation|military purposes|shall not apply to AI systems developed or used exclusively for military purposes, implying an exception or exclusion based on the specific use case|58c1c5ec653648129566cd9fd0bc7dc5|||
public authorities in a third country|union or one or more member states|use AI systems in the framework of international agreements, implying some form of collaboration or exchange between these entities, except for law enforcement and judicial cooperation purposes|58c1c5ec653648129566cd9fd0bc7dc5|||
this regulation|international organizations falling within the scope of this regulation|shall not affect the application of the provisions on the liability of, implying that the regulatory framework is still applicable to international organizations for other purposes|58c1c5ec653648129566cd9fd0bc7dc5|||
artificial intelligence system|provider|An AI system is developed by a provider for a specific set of objectives, and the output generated by the AI system can influence its environment.|7726b254b5114f16b3eb58ea8076d276|||
directive 2000/31/ec|digital services act|The provisions on the liability of intermediary service providers set out in Directive 2000/31/EC will be replaced by corresponding provisions of the Digital Services Act.|7726b254b5114f16b3eb58ea8076d276|||
union|member states|This Regulation applies to the Union or one or more Member States.|7726b254b5114f16b3eb58ea8076d276|||
small-scale provider|provider|A small-scale provider is a type of provider as defined by Commission Recommendation 2003/361/EC, which falls under the category of micro or small enterprises.|c364227c17784f1d98b2b1c96ffcc73a|||
user|ai system|A user is a natural or legal person, public authority, agency or other body who uses an AI system within their authority, except for personal non-professional activities.|c364227c17784f1d98b2b1c96ffcc73a|||
authorised representative|provider|An authorised representative is a natural or legal person established in the Union who has been given a written mandate by a provider to perform and carry out their obligations and procedures under this Regulation.|c364227c17784f1d98b2b1c96ffcc73a|||
importer|provider|An importer is a natural or legal person established in the Union who places an AI system on the market or puts it into service and uses the name or trademark of a natural or legal person established outside the Union.|c364227c17784f1d98b2b1c96ffcc73a|||
distributor|provider|A distributor is any natural or legal person in the supply chain, other than the provider or importer, who makes an AI system available on the Union market without affecting its properties.|c364227c17784f1d98b2b1c96ffcc73a|||
operator|provider|An operator is any of the following: the provider, the user, the authorised representative, the importer or the distributor.|c364227c17784f1d98b2b1c96ffcc73a|||
`placing on the market'|ai system|First making available of an AI system on the Union market.|46d943dd34b345e4b8d5ebb89981960c|||
`making available on the market'|ai system|Supply of an AI system for distribution or use on the Union market in the course of a commercial activity, whether in return for payment or free of charge.|46d943dd34b345e4b8d5ebb89981960c|||
`putting into service'|ai system|Supply of an AI system for first use directly to the user or for own use on the Union market for its intended purpose.|46d943dd34b345e4b8d5ebb89981960c|||
`intended purpose'|provider|Use for which an AI system is intended by the provider, including the specific context and conditions of use, as specified in the information supplied by the provider in the instructions for use, promotional or sales materials and statements, as well as in the technical documentation.|46d943dd34b345e4b8d5ebb89981960c|||
`intended purpose'|ai system|The intended use of an AI system as specified by its provider.|46d943dd34b345e4b8d5ebb89981960c|||
`reasonably foreseeable misuse'|ai system|Use of an AI system in a way that is not in accordance with its intended purpose, but which may result from reasonably foreseeable human behaviour or interaction with other systems.|46d943dd34b345e4b8d5ebb89981960c|||
`safety component of a product or system'|product or system|A component of a product or of a system which fulfils a safety function for that product or system or the failure or malfunctioning of which endangers the health and safety of persons or property.|46d943dd34b345e4b8d5ebb89981960c|||
`instructions for use'|ai system|Information provided by the provider to inform the user of in particular an AI system's intended purpose and proper use, inclusive of the|46d943dd34b345e4b8d5ebb89981960c|||
user|intended purpose|The user of an AI system is intended to benefit from its specific geographical, behavioural or functional setting within which it is used.|7a694c19bf51431caff88bc19aec2120|||
user|proper use|The user's appropriate handling of the AI system falls under its intended purpose and proper use.|7a694c19bf51431caff88bc19aec2120|||
recall|provider of an ai system|When a user requests recall from the provider, it is aimed at achieving the return of the AI system.|7a694c19bf51431caff88bc19aec2120|||
withdrawal|distribution, display and offer of an ai system|Preventing distribution, display and offer of an AI system falls under the measure aimed at preventing it.|7a694c19bf51431caff88bc19aec2120|||
performance|intended purpose|The ability of an AI system to achieve its intended purpose is called its performance.|7a694c19bf51431caff88bc19aec2120|||
notifying authority|national authority|The national authority responsible for setting up and carrying out the necessary procedures for the assessment, designation and notification of conformity assessment bodies is known as the notifying authority.|7a694c19bf51431caff88bc19aec2120|||
conformity assessment|requirements set out in title iii, chapter 2 of this regulation relating to an ai system|The process of verifying whether the requirements related to an AI system have been fulfilled is called conformity assessment.|7a694c19bf51431caff88bc19aec2120|||
conformity assessment body|third-party conformity assessment activities|A body that performs third-party conformity assessment activities, including testing, certification and inspection is called a conformity assessment body.|7a694c19bf51431caff88bc19aec2120|||
assessment activities|testing|Assessment activities include testing, which is a conformity assessment activity as per this Regulation and other relevant Union harmonisation legislation.|75cfb37ded954a7ea08451d754c73cb2|||
assessment activities|certification|Assessment activities include certification, which is a conformity assessment activity as per this Regulation and other relevant Union harmonisation legislation.|75cfb37ded954a7ea08451d754c73cb2|||
assessment activities|inspection|Assessment activities include inspection, which is a conformity assessment activity as per this Regulation and other relevant Union harmonisation legislation.|75cfb37ded954a7ea08451d754c73cb2|||
notified body|conformity assessment body|A notified body is a conformity assessment body designated in accordance with this Regulation and other relevant Union harmonisation legislation.|75cfb37ded954a7ea08451d754c73cb2|||
substantial modification|compliance of the ai system with the requirements set out in title iii, chapter 2 of this regulation|A substantial modification affects the compliance of the AI system with the requirements set out in Title III, Chapter 2 of this Regulation.|75cfb37ded954a7ea08451d754c73cb2|||
substantial modification|intended purpose for which the ai system has been assessed|A substantial modification results in a modification to the intended purpose for which the AI system has been assessed.|75cfb37ded954a7ea08451d754c73cb2|||
ce marking of conformity|provider|The CE marking of conformity indicates that an AI system is in conformity with the requirements set out in Title III, Chapter 2 of this Regulation and other applicable Union legislation harmonising the conditions for the marketing of products (`Union harmonisation legislation') providing for its affixing.|75cfb37ded954a7ea08451d754c73cb2|||
post-market monitoring|provider|Providers of AI systems carry out post-market monitoring to proactively collect and review experience gained from the use of AI systems they place on the market or put into service for the purpose of identifying any need to immediately apply any necessary corrective or preventive actions.|75cfb37ded954a7ea08451d754c73cb2|||
market surveillance authority|national authority|The market surveillance authority is the national authority carrying out the activities and taking the measures pursuant to Regulation (EU) 2019/1020.|75cfb37ded954a7ea08451d754c73cb2|||
activities and taking the measures pursuant to regulation (eu) 2019/1020|harmonised standard|Regulation (EU) 2019/1020 specifies activities and taking measures, which involve the use of harmonised standards as defined in Article 2(1)(c) of Regulation (EU) No 1025/2012.|6ac9262ea391463a9fc90e557fae4907|||
harmonised standard|common specifications|A harmonised standard is a type of European standard that can be used to meet the requirements and obligations established under this Regulation, which may also be documented in common specifications.|6ac9262ea391463a9fc90e557fae4907|||
training data|ai system|Training data is used for training an AI system by fitting its learnable parameters, including the weights of a neural network.|6ac9262ea391463a9fc90e557fae4907|||
validation data|trained ai system|Validation data is used to provide an evaluation of the trained AI system and for tuning its non-learnable parameters and its learning process to prevent overfitting.|6ac9262ea391463a9fc90e557fae4907|||
testing data|trained and validated ai system|Testing data is used to provide an independent evaluation of the trained and validated AI system before its placement on the market or putting into service to confirm expected performance.|6ac9262ea391463a9fc90e557fae4907|||
input data|ai system|Input data is provided to, or directly acquired by, an AI system based on which it produces an output.|6ac9262ea391463a9fc90e557fae4907|||
biometric data|personal data|Biometric data is a type of personal data resulting from specific technical processing.|6ac9262ea391463a9fc90e557fae4907|||
output|system|The system produces an output.|017158f87beb4ca4a558c51d2afbae50|||
biometric data|natural person|Biometric data results from specific technical processing relating to the physical, physiological or behavioural characteristics of a natural person.|017158f87beb4ca4a558c51d2afbae50|||
emotion recognition system|biometric data|An AI system for the purpose of identifying or inferring emotions or intentions of natural persons on the basis of their biometric data.|017158f87beb4ca4a558c51d2afbae50|||
biometric categorisation system|natural person|An AI system for the purpose of assigning natural persons to specific categories on the basis of their biometric data.|017158f87beb4ca4a558c51d2afbae50|||
remote biometric identification system|natural person|An AI system for the purpose of identifying natural persons at a distance through the comparison of a person's biometric data with the biometric data contained in a reference database, and without prior knowledge of the user of the AI system whether the person will be present and can be identified.|017158f87beb4ca4a558c51d2afbae50|||
real-time remote biometric identification system|remote biometric identification system|The capturing of biometric data, the comparison and the identification all occur without a significant delay. This comprises not only instant identification, but also limited short delays in order to avoid circumvention.|017158f87beb4ca4a558c51d2afbae50|||
law enforcement|publicly accessible space|Law enforcement authorities carry out activities for the prevention, investigation, detection or prosecution of criminal offences and the execution of criminal penalties in publicly accessible spaces to safeguard against and prevent threats to public security.|182e91c06b0941acb48d879d4e121a40|||
law enforcement|law enforcement authority|Law enforcement authorities include public authorities competent for the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, as well as other bodies or entities entrusted by Member State law to exercise public authority and public powers for these purposes.|182e91c06b0941acb48d879d4e121a40|||
post' remote biometric identification system|real-time' remote biometric identification system|A `post' remote biometric identification system is a type of remote biometric identification system that allows for instant identification, while a `real-time' remote biometric identification system provides limited short delays to avoid circumvention.|182e91c06b0941acb48d879d4e121a40|||
subliminal techniques beyond a person's consciousness|physical or psychological harm|deploys subliminal techniques beyond a person's consciousness in order to materially distort a person's behaviour in a manner that causes or is likely to cause that person or another person physical or psychological harm|cbc9e2a3d2ec437eb4bd4357261daff2|||
specific group of persons|physical or psychological harm|exploits any of the vulnerabilities of a specific group of persons due to their age, physical or mental disability, in order to materially distort the behaviour of a person pertaining to that group in a manner that causes or is likely to cause that person or another person physical or psychological harm|cbc9e2a3d2ec437eb4bd4357261daff2|||
social behaviour|trustworthiness of natural persons|AI systems by public authorities or on their behalf for the evaluation or classification of the trustworthiness of natural persons over a certain period of time based on their social behaviour|cbc9e2a3d2ec437eb4bd4357261daff2|||
social contexts|detrimental or unfavourable treatment|leading to either or both of the following: (i) detrimental or unfavourable treatment of certain natural persons or whole groups thereof in social contexts which are unrelated to the contexts in which the data was originally generated or collected;|cbc9e2a3d2ec437eb4bd4357261daff2|||
unrelated social contexts|data originally generated or collected|groups thereof are being extracted from social contexts that are not related to the original data contexts.|936e0c117cf74b958b1177d3c90ff7d0|||
unjustified or disproportionate treatment|social behavior or its gravity|is being inflicted on natural persons or whole groups thereof in unrelated social contexts.|936e0c117cf74b958b1177d3c90ff7d0|||
real-time remote biometric identification systems|publicly accessible spaces for the purpose of law enforcement|are being used, unless it is strictly necessary for specific objectives such as searching for potential victims, preventing a specific threat or detecting criminals punishable by custodial sentences.|936e0c117cf74b958b1177d3c90ff7d0|||
real-time remote biometric identification systems|law enforcement|The use of 'real-time' remote biometric identification systems in publicly accessible spaces for the purpose of law enforcement is mentioned.|e35a1fe360e74261b7d1d008763566c9|||
harm caused|use of system|The nature of the situation giving rise to the possible use, in particular the seriousness, probability and scale of the harm caused in the absence of the use of the system is considered.|e35a1fe360e74261b7d1d008763566c9|||
rights and freedoms|use of system|The consequences of the use of the system for the rights and freedoms of all persons concerned, in particular the seriousness, probability and scale of those consequences is considered.|e35a1fe360e74261b7d1d008763566c9|||
judicial authority|use of system|Each individual use for the purpose of law enforcement of a 'real-time' remote biometric identification system in publicly accessible spaces shall be subject to a prior authorisation granted by a judicial authority or by an independent administrative authority of the Member State in which the use is to take place.|e35a1fe360e74261b7d1d008763566c9|||
reasoned request|use of system|Each individual use for the purpose of law enforcement of a 'real-time' remote biometric identification system in publicly accessible spaces shall be subject to a prior authorisation granted by a judicial authority or by an independent administrative authority of the Member State in which the use is to take place, issued upon a reasoned request.|e35a1fe360e74261b7d1d008763566c9|||
national law|use of system|Each individual use for the purpose of law enforcement of a 'real-time' remote biometric identification system in publicly accessible spaces shall be subject to a prior authorisation granted by a judicial authority or by an independent administrative authority of the Member State in which the use is to take place, issued upon a reasoned request and in accordance with the detailed rules of national law referred to in paragraph 4.|e35a1fe360e74261b7d1d008763566c9|||
urgency|use of system|In a duly justified situation of urgency, the use of the system may be commenced without an authorisation granted by a judicial authority or by an independent administrative authority of the Member State in which the use is to take place.|e35a1fe360e74261b7d1d008763566c9|||
high-risk ai systems|ai systems referred to in annex iii|Both high-risk AI systems and AI systems referred to in Annex III are classified based on specific conditions as per the given context.|199f59a0cf254b3a81b08573e6801881|||
high-risk ai systems|ai system itself as a product|A high-risk AI system can be considered a safety component of a product or itself a product, as per the given context.|199f59a0cf254b3a81b08573e6801881|||
high-risk ai systems|product whose safety component is the ai system|A high-risk AI system can be used as a safety component of a product covered by the Union harmonisation legislation listed in Annex II, as per the given context.|199f59a0cf254b3a81b08573e6801881|||
union harmonisation legislation listed in annex ii|product required to undergo a third-party conformity assessment with a view to placing on the market or putting into service of that product|The Union harmonisation legislation listed in Annex II requires products covered by it to undergo a third-party conformity assessment with a view to their placement on the market or putting into service, as per the given context.|199f59a0cf254b3a81b08573e6801881|||
high-risk ai systems referred to in paragraph 1|ai systems referred to in annex iii|The high-risk AI systems mentioned in paragraph 1 are also considered as high-risk AI systems as per the conditions defined in Article 7.|695fc65809904a14b8f041547911d7c9|||
commission|ai system|The Commission is empowered to add new high-risk AI systems to Annex III through delegated acts, provided they meet the specified conditions as mentioned in Article 7.|695fc65809904a14b8f041547911d7c9|||
areas listed in points 1 to 8 of annex iii|high-risk ai systems|High-risk AI systems are intended for use in any of the areas listed in Annex III.|695fc65809904a14b8f041547911d7c9|||
health and safety|adverse impact on fundamental rights|High-risk AI systems pose a risk of harm to health and safety or an adverse impact on fundamental rights, equivalent to or greater than the risk posed by the high-risk AI systems already referred to in Annex III.|695fc65809904a14b8f041547911d7c9|AI system||
commission|ai system|When assessing whether a new AI system meets the conditions for inclusion in Annex III, the Commission will consider factors such as intended purpose, use history, and potential harm to health and safety or fundamental rights.|695fc65809904a14b8f041547911d7c9|||
harm to health and safety|risk of adverse impact on fundamental rights|When assessing whether a new AI system meets the conditions for inclusion in Annex III, the Commission will consider whether the potential harm to health and safety or adverse impact on fundamental rights is equivalent to or greater than that posed by the high-risk AI systems already referred to in Annex III.|695fc65809904a14b8f041547911d7c9|||
harm|significant concerns|The use of an AI system has already caused harm or given rise to significant concerns in relation to materialisation of such harm or adverse impact, as demonstrated by reports or documented allegations submitted to national competent authorities.|ef3359621cd349d69e7ee43e0dcde022|||
harm|potential extent of such harm|The potential extent of such harm in terms of its intensity and its ability to affect a plurality of persons.|ef3359621cd349d69e7ee43e0dcde022|||
adverse impact|significant concerns|The use of an AI system has already caused harm or given rise to significant concerns in relation to materialisation of such harm or adverse impact, as demonstrated by reports or documented allegations submitted to national competent authorities.|ef3359621cd349d69e7ee43e0dcde022|||
adverse impact|potential extent of such adverse impact|The potential extent of such adverse impact, in particular in terms of its intensity and its ability to affect a plurality of persons.|ef3359621cd349d69e7ee43e0dcde022|||
persons|potentially harmed or adversely impacted persons|Potentially harmed or adversely impacted persons because for practical or legal reasons it is not reasonably possible to opt-out from that outcome.|ef3359621cd349d69e7ee43e0dcde022|||
persons|vulnerable position in relation to the user of an ai system|Potentially harmed or adversely impacted persons who are dependent on the outcome produced with an AI system, in particular because for practical or legal reasons it is not reasonably possible to opt-out from that outcome.|ef3359621cd349d69e7ee43e0dcde022|||
persons|vulnerable position in relation to the user of an ai system|Potentially harmed or adversely impacted persons who are in a vulnerable position in relation to the user of an AI system, in particular due to an imbalance of power, knowledge, economic or social circumstances, or age.|ef3359621cd349d69e7ee43e0dcde022|||
outcome|easily reversible|Outcomes having an impact on the health or safety of persons shall not be considered as easily reversible.|ef3359621cd349d69e7ee43e0dcde022|||
existing union legislation|effective measures of redress|Existing Union legislation provides for: effective measures of redress in relation to the risks posed by an AI system, with the exclusion of claims for damages.|ef3359621cd349d69e7ee43e0dcde022|||
existing union legislation|effective measures to prevent or substantially minimise those risks|Existing Union legislation provides for: effective measures to prevent or substantially minimise those risks.|ef3359621cd349d69e7ee43e0dcde022|||
high-risk ai systems|risk management system|compliance with requirements established in this Chapter is ensured through the establishment, implementation, documentation, and maintenance of a risk management system for high-risk AI systems. This iterative process requires regular systematic updating and consists of identifying and analyzing known and foreseeable risks, estimating and evaluating emerging risks when using the system as intended or under misuse conditions, and evaluating other possible arising risks based on post-market monitoring system data.|2e7c9cdf4aa44a048b85436c68c16ccf|||
high-risk ai systems|identification and analysis of known and foreseeable risks|is a step in the risk management system process for high-risk AI systems, which involves identifying and analyzing the known and foreseeable risks associated with each high-risk AI system.|2e7c9cdf4aa44a048b85436c68c16ccf|||
high-risk ai systems|estimation and evaluation of emerging risks|is a step in the risk management system process for high-risk AI systems, which involves estimating and evaluating the risks that may emerge when using the high-risk AI system as intended or under misuse conditions.|2e7c9cdf4aa44a048b85436c68c16ccf|||
high-risk ai systems|evaluation of other possibly arising risks|is a step in the risk management system process for high-risk AI systems, which involves evaluating other possible arising risks based on the analysis of data gathered from the post-market monitoring system referred to in Article 61.|2e7c9cdf4aa44a048b85436c68c16ccf|||
high-risk ai systems|post-market monitoring system|is a reference made in one of the steps of the risk management system process for high-risk AI systems, which involves analyzing data gathered from this post-market monitoring system to evaluate other possible arising risks.|2e7c9cdf4aa44a048b85436c68c16ccf|||
post-market monitoring system|high-risk ai systems|From the post-market monitoring system referred to in Article 61, suitable risk management measures shall be adopted for high-risk AI systems in accordance with the provisions of this Chapter 2.|8cd0cf71d06c4a1bac3047a4016b8f4b|||
adoption of suitable risk management measures|effects and possible interactions resulting from the combined application of the requirements set out in this chapter 2|The risk management measures referred to in paragraph 2, point (d) shall give due consideration to the effects and possible interactions resulting from the combined application of the requirements set out in this Chapter 2.|8cd0cf71d06c4a1bac3047a4016b8f4b|||
suitable risk management measures|generally acknowledged state of the art, including as reflected in relevant harmonised standards or common specifications|The risk management measures referred to in paragraph 2, point (d) shall take into account the generally acknowledged state of the art, including as reflected in relevant harmonised standards or common specifications.|8cd0cf71d06c4a1bac3047a4016b8f4b|||
risk management measures|residual risk associated with each hazard as well as the overall residual risk of high-risk ai systems|The risk management measures referred to in paragraph 2, point (d) shall be such that any residual risk associated with each hazard as well as the overall residual risk of high-risk AI systems is judged acceptable, provided that the high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse.|8cd0cf71d06c4a1bac3047a4016b8f4b|||
elimination or reduction of risks|adequate design and development|In identifying the most appropriate risk management measures, the following shall be ensured: (a) elimination or reduction of risks as far as possible through adequate design and development;|8cd0cf71d06c4a1bac3047a4016b8f4b|||
mitigation and control measures|risks that cannot be eliminated|In identifying the most appropriate risk management measures, the following shall be ensured: (b) where appropriate, implementation of adequate mitigation and control measures in relation to risks that cannot be eliminated;|8cd0cf71d06c4a1bac3047a4016b8f4b|||
adequate information|risks referred to in paragraph 2, point (b) of this article|In identifying the most appropriate risk management measures, the following shall be ensured: (c) provision of adequate information pursuant to Article 13, in particular as regards the risks referred to in paragraph 2, point (b) of this Article, and, where|8cd0cf71d06c4a1bac3047a4016b8f4b|||
children|high-risk ai system|When implementing the risk management system described in paragraphs 1 to 7, specific consideration shall be given to whether the high-risk AI system is likely to be accessed by or have an impact on children.|16e5e8419c8a4d888e1640769ecfdbe2|||
directive 2013/36/eu|high-risk ai system|For credit institutions regulated by Directive 2013/36/EU, the aspects described in paragraphs 1 to 8 shall be part of the risk management procedures established by those institutions pursuant to Article 74 of that Directive.|16e5e8419c8a4d888e1640769ecfdbe2|||
training|testing data sets|High-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5.|16e5e8419c8a4d888e1640769ecfdbe2|||
validation|testing data sets|High-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5.|16e5e8419c8a4d888e1640769ecfdbe2|||
data governance|training, validation, testing data sets|High-risk AI systems which make use of techniques involving the training of models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5. Data governance and management practices shall concern...|16e5e8419c8a4d888e1640769ecfdbe2|||
data collection|data governance|Data governance and management practices shall concern, (a)   the relevant design choices; (b)   data collection;|16e5e8419c8a4d888e1640769ecfdbe2|||
relevant data preparation processing operations|data governance|Data governance and management practices shall concern, (c)   relevant data preparation processing operations, such as annotation, labelling, cleaning, enrichment and aggregation;|16e5e8419c8a4d888e1640769ecfdbe2|||
formulation of relevant assumptions|data governance|Data governance and management practices shall concern, (d)   the formulation of relevant assumptions, notably with respect to the information that the data are supposed to measure and represent;|16e5e8419c8a4d888e1640769ecfdbe2|||
availability|data governance|Data governance and management practices shall concern, (e)   a prior assessment of the availability, quantity and suitability of the data sets that are needed;|16e5e8419c8a4d888e1640769ecfdbe2|||
possible biases|data governance|Data governance and management practices shall concern, (f)   examination in view of possible biases;|16e5e8419c8a4d888e1640769ecfdbe2|||
data gaps or shortcomings|data governance|Data governance and management practices shall concern, (g)   the identification of any possible data gaps or shortcomings, and how those gaps and shortcomings can be addressed.|16e5e8419c8a4d888e1640769ecfdbe2|||
data gaps or shortcomings|addressing data gaps and shortcomings|The identification of any possible data gaps or shortcomings is relevant in addressing them, as it helps to understand the issues that need to be addressed. The characteristics or elements particular to the specific geographical, behavioural or functional setting within which the high-risk AI system is intended to be used should also be taken into account.|2dbe7bbb6ae946fb887bdbdd243217c3|||
training data sets|validation data sets|Both training and validation data sets are necessary for ensuring the performance of a high-risk AI system. The characteristics or elements particular to the specific geographical, behavioural or functional setting within which the high-risk AI system is intended to be used should also be taken into account.|2dbe7bbb6ae946fb887bdbdd243217c3|||
testing data sets|training and validation data sets|Testing data sets are distinct from both training and validation data sets, as they are used to evaluate the performance of a high-risk AI system on new, unseen data. Both training and validation data sets should be relevant, representative, free of errors and complete, with appropriate statistical properties where applicable.|2dbe7bbb6ae946fb887bdbdd243217c3|||
special categories of personal data|bias monitoring, detection and correction|The processing of special categories of personal data for the purposes of ensuring bias monitoring, detection and correction is strictly necessary for high-risk AI systems. Appropriate safeguards for fundamental rights and freedoms should be applied, along with technical limitations on re-use and use, state-of-the-art security and privacy-preserving measures.|2dbe7bbb6ae946fb887bdbdd243217c3|||
high-risk ai systems|paragraph 2|high-risk AI systems are required to comply with the requirements set out in paragraph 2|6da81e1aa78642fa8eccbae802ca6815|||
technical documentation|before placement on market or put into service|technical documentation is required to be drawn up before placing high-risk AI systems on the market or putting them into service|6da81e1aa78642fa8eccbae802ca6815|||
high-risk ai systems|annex iv|high-risk AI systems are required to contain the elements set out in Annex IV|6da81e1aa78642fa8eccbae802ca6815|||
technical documentation|annex iv|technical documentation is required to contain the elements set out in Annex IV|6da81e1aa78642fa8eccbae802ca6815|||
high-risk ai systems|annex ii, section a|high-risk AI systems related to products subject to legal acts listed in Annex II, section A are required to have a single technical documentation containing all the information set out in Annex IV as well as information required under those legal acts|6da81e1aa78642fa8eccbae802ca6815|||
annex iv|legal acts|required under those legal acts|b6b1796c8c734a9ba5f3b11e5a7eca70|||
annex iv|technical documentation|ensures that, in the light of technical progress, provides all the necessary information to assess the compliance of the system with the requirements set out in this Chapter.|b6b1796c8c734a9ba5f3b11e5a7eca70|||
annex iv|technical progress|ensures that, in the light of technical progress,|b6b1796c8c734a9ba5f3b11e5a7eca70|||
commission|delegated acts|empowered to adopt delegated acts in accordance with Article 73|b6b1796c8c734a9ba5f3b11e5a7eca70|||
commission|annex iv|amend where necessary|b6b1796c8c734a9ba5f3b11e5a7eca70|||
high-risk ai systems|logging capabilities|designed and developed with capabilities enabling the automatic recording of events (`logs') while the high-risk AI systems is operating.|b6b1796c8c734a9ba5f3b11e5a7eca70|||
high-risk ai systems|logging standards or common specifications|shall conform to recognised standards or common specifications.|b6b1796c8c734a9ba5f3b11e5a7eca70|||
logging capabilities|traceability of the ai system's functioning throughout its lifecycle|shall enable the monitoring of the operation of the high-risk AI system with respect to the occurrence of situations that may result in the AI system presenting a risk within the meaning of Article 65(1) or lead to a substantial modification,|b6b1796c8c734a9ba5f3b11e5a7eca70|||
logging capabilities|post-market monitoring referred to in article 61|shall facilitate the post-market monitoring referred to in Article 61.|b6b1796c8c734a9ba5f3b11e5a7eca70|||
high-risk ai systems referred to in paragraph 1, point (a) of annex iii|logging capabilities|shall provide, at a minimum:|b6b1796c8c734a9ba5f3b11e5a7eca70|||
high-risk ai systems referred to in paragraph 1, point (a) of annex iii|start date and time and end date and time of each use||b6b1796c8c734a9ba5f3b11e5a7eca70|||
high-risk ai systems|transparency in operation|Ensured by designing and developing high-risk AI systems to enable users to interpret system's output and use it appropriately, as required by Article 13(1) of the regulation.|931242dd0f144dd98f1e8074cd6af93f|||
high-risk ai systems|instructions for use|Accompanied by concise, complete, correct and clear information about the system's characteristics, capabilities, and limitations of performance, as required by Article 13(2) of the regulation.|931242dd0f144dd98f1e8074cd6af93f|||
high-risk ai systems|provider's identity and contact details|Specified in the instructions for use, as required by Article 13(2) of the regulation.|931242dd0f144dd98f1e8074cd6af93f|||
high-risk ai systems|authorized representative's identity and contact details|Specified in the instructions for use, where applicable, as required by Article 13(2) of the regulation.|931242dd0f144dd98f1e8074cd6af93f|||
high-risk ai systems|references databases against which input data has been checked by the system|One of the factors that could be relevant and specified in the instructions for use, as required by Article 13(2) of the regulation.|931242dd0f144dd98f1e8074cd6af93f|||
high-risk ai systems|natural persons involved in verification of results|Specified in the instructions for use, as required by Article 13(2) and Article 14(5) of the regulation.|931242dd0f144dd98f1e8074cd6af93f|||
authorised representative|high-risk ai system|The authorised representative is responsible for ensuring the conformity and safety of the high-risk AI system.|6addbb69034c42f5bdb3137d03c3c318|||
intended purpose|high-risk ai system|The intended purpose of the high-risk AI system refers to its specific function or application.|6addbb69034c42f5bdb3137d03c3c318|||
level of accuracy, robustness and cybersecurity|high-risk ai system|This refers to the reliability, consistency, and security measures that have been tested and validated for the high-risk AI system against specific conditions.|6addbb69034c42f5bdb3137d03c3c318|||
known or foreseeable circumstances|high-risk ai system|These are situations that could potentially impact the expected level of accuracy, robustness and cybersecurity of the high-risk AI system.|6addbb69034c42f5bdb3137d03c3c318|||
specifications for input data|high-risk ai system|This refers to the specific requirements that need to be met by the data used as inputs into the high-risk AI system, in line with its intended purpose.|6addbb69034c42f5bdb3137d03c3c318|||
persons or groups of persons|high-risk ai system|These are the individuals or groups for whom the high-risk AI system is designed to be used.|6addbb69034c42f5bdb3137d03c3c318|||
pre-determined changes|high-risk ai system|This refers to specific modifications that have been planned for the high-risk AI system and its performance at the time of initial conformity assessment.|6addbb69034c42f5bdb3137d03c3c318|||
human oversight measures|high-risk ai system|These are actions taken by human operators to ensure safe and responsible use of the high-risk AI system, including technical measures to aid interpretation of its outputs.|6addbb69034c42f5bdb3137d03c3c318|||
high-risk ai system|individuals to whom human oversight is assigned|enable the individuals to fully understand the capacities and limitations of the high-risk AI system and be able to duly monitor its operation, so that signs of anomalies, dysfunctions and unexpected performance can be detected and addressed as soon as possible|9f88902f59b2414c942813e8300f0f5a|||
high-risk ai system|automation bias|remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system|9f88902f59b2414c942813e8300f0f5a|||
high-risk ai system|interpretation tools and methods available|be able to correctly interpret the high-risk AI system's output, taking into account in particular the characteristics of the system|9f88902f59b2414c942813e8300f0f5a|||
high-risk ai system|output of the high-risk ai system|decide, in any particular situation, not to use the high-risk AI system or otherwise disregard, override or reverse the output of the high-risk AI system|9f88902f59b2414c942813e8300f0f5a|||
high-risk ai system|stop button or a similar procedure|be able to intervene on the operation of the high-risk AI system or interrupt the system through a 'stop' button or a similar procedure|9f88902f59b2414c942813e8300f0f5a|||
accuracy|cybersecurity|High-risk AI systems shall be designed and developed in such a way that they achieve an appropriate level of accuracy and cybersecurity throughout their lifecycle, in the light of their intended purpose.|5ce9d4f05487422f897612da847a8d65|||
accuracy|robustness|High-risk AI systems shall be designed and developed to achieve an appropriate level of accuracy and perform consistently in terms of robustness throughout their lifecycle.|5ce9d4f05487422f897612da847a8d65|||
accuracy|reliability|The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanying instructions of use.|5ce9d4f05487422f897612da847a8d65|||
cybersecurity|confidentiality|High-risk AI systems shall be designed and developed to ensure that no action or decision is taken by the user on the basis of the identification resulting from the system unless this has been verified and confirmed by at least two natural persons.|5ce9d4f05487422f897612da847a8d65|||
cybersecurity|integrity|High-risk AI systems shall be designed and developed to ensure that possibly biased outputs due to outputs used as an input for future operations (`feedback loops') are duly considered.|5ce9d4f05487422f897612da847a8d65|||
cybersecurity|availability|High-risk AI systems shall be resilient as regards errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems.|5ce9d4f05487422f897612da847a8d65|||
cybersecurity|resilience|The robustness of high-risk AI systems may be achieved through technical redundancy solutions, which may include backup or fail-safe plans.|5ce9d4f05487422f897612da847a8d65|||
procedure|conformity assessment procedures|providers of high-risk ai systems shall put a quality management system in place that ensures compliance with this regulation. This system shall include a strategy for regulatory compliance, which encompasses compliance with conformity assessment procedures.|889e3c79974b41b4908adfaa8f16bda5|||
conformity assessment procedures|regulatory compliance|this strategy for regulatory compliance is an essential aspect of the quality management system that providers of high-risk ai systems shall implement as part of their commitment to fulfilling this regulation's requirements.|889e3c79974b41b4908adfaa8f16bda5|||
high-risk ai system|modifications|the quality management system should also account for procedures for managing modifications to the high-risk ai system, as required by this regulation.|889e3c79974b41b4908adfaa8f16bda5|||
assessment procedures|procedures for the management of modifications to the high-risk ai system|Involve systematic actions and techniques used in both procedures to ensure compliance with requirements set out in Chapter 2 of this Title.|649b73d3811142f29ac405665131d238|||
techniques, procedures and systematic actions|design, design control and design verification of the high-risk ai system|Are used for the development, quality control and quality assurance of the high-risk AI system.|649b73d3811142f29ac405665131d238|||
examination, test and validation procedures|before, during and after the development of the high-risk ai system|Are carried out frequently to ensure compliance with requirements set out in Chapter 2 of this Title.|649b73d3811142f29ac405665131d238|||
technical specifications|including standards|Are applied and, where relevant harmonised standards are not applied in full, means to ensure compliance with requirements set out in Chapter 2 of this Title are used.|649b73d3811142f29ac405665131d238|||
systems and procedures for data management|including data collection, analysis, labelling, storage, filtration, mining, aggregation, retention and any other operation regarding the data|Are performed before and for the purposes of placing on the market or putting into service of high-risk AI systems.|649b73d3811142f29ac405665131d238|||
risk management system|referred to in article 9|Is included in the ontology.|649b73d3811142f29ac405665131d238|||
post-market monitoring system|in accordance with article 61|Is included in the ontology.|649b73d3811142f29ac405665131d238|||
procedures related to the reporting of serious incidents and of malfunctioning||Are included in the ontology.|649b73d3811142f29ac405665131d238|||
system|article 61|In accordance with Article 61, the implementation of aspects referred to in paragraph 1 shall be proportionate to the size of the provider's organization.|3d29b29cb788401ebc499b50f7ae44f9|||
procedures related to the reporting of serious incidents and of malfunctioning|article 61|In accordance with Article 61, procedures related to the reporting of serious incidents and of malfunctioning are implemented.|3d29b29cb788401ebc499b50f7ae44f9|||
handling of communication with national competent authorities, competent authorities, including sectoral ones, providing or supporting the access to data, notified bodies, other operators, customers or other interested parties|article 62|In accordance with Article 62, handling of communication with national competent authorities, competent authorities, including sectoral ones, providing or supporting the access to data, notified bodies, other operators, customers or other interested parties is implemented.|3d29b29cb788401ebc499b50f7ae44f9|||
handling of communication with national competent authorities, competent authorities, including sectoral ones, providing or supporting the access to data, notified bodies, other operators, customers or other interested parties|article 62|Handling of communication with national competent authorities, competent authorities, including sectoral ones, providing or supporting the access to data, notified bodies, other operators, customers or other interested parties is also implemented in accordance with Article 61.|3d29b29cb788401ebc499b50f7ae44f9|||
handling of communication with national competent authorities, competent authorities, including sectoral ones, providing or supporting the access to data, notified bodies, other operators, customers or other interested parties|article 62|Additionally, handling of communication with national competent authorities, competent authorities, including sectoral ones, providing or supporting the access to data, notified bodies, other operators, customers or other interested parties is implemented for providers that are credit institutions regulated by Directive 2013/36/EU.|3d29b29cb788401ebc499b50f7ae44f9|||
quality management system|article 74 of directive 2013/36/eu|For providers that are credit institutions regulated by Directive 2013/36/EU, the obligation to put a quality management system in place shall be deemed to be fulfilled by complying with the rules on internal governance arrangements, processes and mechanisms pursuant to Article 74 of that Directive. In that context, any harmonized standards referred to in Article 40 of this Regulation shall be taken into account.|3d29b29cb788401ebc499b50f7ae44f9|||
internal governance arrangements, processes and mechanisms|article 74 of directive 2013/36/eu|For providers that are credit institutions regulated by Directive 2013/36/EU, implementing quality management system is fulfilled by complying with internal governance arrangements, processes and mechanisms pursuant to Article 74 of that Directive.|3d29b29cb788401ebc499b50f7ae44f9|||
harmonized standards referred to in article 40 of this regulation|article 74 of directive 2013/36/eu|For providers that are credit institutions regulated by Directive 2013/36/EU, any harmonized standards referred to in Article 40 of this Regulation shall be taken into account while implementing quality management system and internal governance arrangements, processes and mechanisms pursuant to Article 74 of that Directive.|3d29b29cb788401ebc499b50f7ae44f9|||
resource management, including security of supply related measures|paragraph 1|In accordance with Paragraph 1, resource management, including security of supply related measures are implemented.|3d29b29cb788401ebc499b50f7ae44f9|||
technical documentation|annex iv|Providers of high-risk AI systems shall draw up the technical documentation referred to in Article 11 in accordance with Annex IV.|71c6ade6c55c433f9ebf7f6f00eee50d|||
credit institutions regulated by directive 2013/36/eu|technical documentation|Providers that are credit institutions regulated by Directive 2013/36/EU shall maintain the technical documentation as part of the documentation concerning internal governance, arrangements, processes and mechanisms pursuant to Article 74 of that Directive.|71c6ade6c55c433f9ebf7f6f00eee50d|||
technical documentation|ce marking of conformity|High-risk AI systems shall undergo the relevant conformity assessment procedure in accordance with Article 43, prior to their placing on the market or putting into service. Where the compliance of the AI systems with the requirements set out in Chapter 2 of this Title has been demonstrated following that conformity assessment, the providers shall draw up an EU declaration of conformity in accordance with Article 48 and affix the CE marking of conformity in accordance with Article 49.|71c6ade6c55c433f9ebf7f6f00eee50d|||
providers|high-risk ai systems referred to in point 5(b) of annex iii|For high-risk AI systems referred to in point 5(b) of Annex III that are placed on the market or put into service by providers that are credit institutions regulated by Directive 2013/36/EU, the conformity assessment shall be carried out as part of the procedure referred to in Articles 97 to101 of that Directive.|71c6ade6c55c433f9ebf7f6f00eee50d|||
providers|logs automatically generated by ai systems|Providers of high-risk AI systems shall keep the logs automatically generated by AI systems.|71c6ade6c55c433f9ebf7f6f00eee50d|||
high-risk ai systems|logs automatically generated by high-risk ai systems|Providers of high-risk AI systems shall keep the logs automatically generated by their high-risk AI systems, to the extent such logs are under their control by virtue of a contractual arrangement with the user or otherwise by law.|7c215a4a77bd411d810b69254f45861f|||
providers|high-risk ai systems|Providers of high-risk AI systems shall maintain the logs automatically generated by their high-risk AI systems as part of the documentation under Articles 74 of Directive 2013/36/EU.|7c215a4a77bd411d810b69254f45861f|||
high-risk ai system|corrective actions|Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system which they have placed on the market or put into service is not in conformity with this Regulation shall immediately take the necessary corrective actions to bring that system into conformity, to withdraw it or to recall it, as appropriate.|7c215a4a77bd411d810b69254f45861f|||
high-risk ai system|duty of information|Where the high-risk AI system presents a risk within the meaning of Article 65(1) and that|7c215a4a77bd411d810b69254f45861f|||
product|manufactured in accordance with those legal acts|The product is manufactured in accordance with those legal acts.|ae81a80662f9434e8bce431d664e0aad|||
product|under the name of the product|The product is marketed under its own name.|ae81a80662f9434e8bce431d664e0aad|||
manufacturer|the manufacturer of the product|The entity responsible for manufacturing the product is referred to as 'the manufacturer'.|ae81a80662f9434e8bce431d664e0aad|||
provider|where an importer cannot be identified, providers established outside the union shall, by written mandate, appoint an authorised representative which is established in the union.|When a provider is unable to identify an importer, they are required to appoint an authorized representative who is based in the EU through a written agreement.|ae81a80662f9434e8bce431d664e0aad|||
provider|the responsibility of the compliance with this regulation and, as far as the ai system is concerned, have the same obligations imposed by the present regulation on the provider.|The provider is responsible for ensuring compliance with this regulation, specifically in relation to any AI systems they provide.|ae81a80662f9434e8bce431d664e0aad|||
ai system|high-risk ai system|The AI system being referred to is classified as 'high-risk'.|ae81a80662f9434e8bce431d664e0aad|||
authorised representative|prior to making their systems available on the union market, where an importer cannot be identified, providers established outside the union shall, by written mandate, appoint an authorised representative which is established in the union.|Before putting high-risk AI systems on the EU market, providers based outside the EU are required to designate a representative who is based within the EU through a written agreement, if they cannot identify an importer.|ae81a80662f9434e8bce431d664e0aad|||
authorised representative|which is established in the union|The authorized representative designated by the provider must be based within the EU.|ae81a80662f9434e8bce431d664e0aad|||
authorised representative|shall perform the tasks specified in the mandate received from the provider.|The authorized representative is responsible for carrying out the duties outlined in the appointment agreement provided by the provider.|ae81a80662f9434e8bce431d664e0aad|||
authorised representative|keep a copy of the eu declaration of conformity and the technical documentation at the disposal of the national competent authorities and national authorities referred to in article 63(7);|The authorized representative is required to maintain copies of the EU Declaration of Conformity and relevant technical documentation, which can be accessed by national competent authorities and other specified bodies.|ae81a80662f9434e8bce431d664e0aad|||
authorised representative|provide a national competent authority, upon a reasoned request, with all the information and documentation necessary to demonstrate the conformity of a high-risk ai system with the requirements set out in chapter 2 of this title, including access to the logs automatically generated by the high-risk ai system|The authorized representative is expected to provide national competent authorities with all necessary information and documentation demonstrating compliance with the requirements for high-risk AI systems as stipulated in Chapter 2, which may include access to the system's logs.|ae81a80662f9434e8bce431d664e0aad|||
high-risk ai system|conformity marking|required for placing the high-risk AI system on the market, as outlined in Article 26.1(c) of this Regulation.|fa64e354638d4a159b15debddbb0e643|||
provider|appropriate conformity assessment procedure|must be carried out by the provider of a high-risk AI system before it is placed on the market, in accordance with Article 26.1(a) of this Regulation.|fa64e354638d4a159b15debddbb0e643|||
technical documentation|annex iv|must be drawn up by the provider of a high-risk AI system in accordance with Article 26.1(b) of this Regulation.|fa64e354638d4a159b15debddbb0e643|||
importer|high-risk ai system|before placing the system on the market, as outlined in Article 26.1 of this Regulation.|fa64e354638d4a159b15debddbb0e643|||
market surveillance authorities|importer|if the importer considers or has reason to consider that a high-risk AI system is not in conformity with this Regulation, they shall inform these authorities to that effect, as outlined in Article 26.2 of this Regulation.|fa64e354638d4a159b15debddbb0e643|||
provider|importer|if the importer considers or has reason to consider that a high-risk AI system is not in conformity with this Regulation, they shall inform the provider of the AI system to that effect, as outlined in Article 26.2 of this Regulation.|fa64e354638d4a159b15debddbb0e643|||
high-risk ai system|risk|where the high-risk AI system presents a risk within the meaning of Article 65(1) of this Regulation, the importer shall inform the provider and market surveillance authorities to that effect, as outlined in Article 26.2 of this Regulation.|fa64e354638d4a159b15debddbb0e643|||
high-risk ai system|ce conformity marking|A high-risk AI system with the required CE conformity marking is compliant with the requirements set out in Chapter 2 of this Title.|81c283f0a67e40bdb86537ff5ec3c016|||
high-risk ai system|documentation and instruction of use|The required documentation and instruction of use must accompany a high-risk AI system before it is made available on the market.|81c283f0a67e40bdb86537ff5ec3c016|||
high-risk ai system|distributors|Distributors are responsible for verifying that a high-risk AI system meets the requirements set out in Chapter 2 of this Title, including having the required CE conformity marking and accompanying documentation and instruction of use.|81c283f0a67e40bdb86537ff5ec3c016|||
high-risk ai system|storage or transport conditions|While a high-risk AI system is under the responsibility of importers, storage or transport conditions must not jeopardize its compliance with the requirements set out in Chapter 2 of this Title.|81c283f0a67e40bdb86537ff5ec3c016|||
high-risk ai system|national competent authorities|Importers shall provide national competent authorities, upon a reasoned request, with all necessary information and documentation to demonstrate the conformity of a high-risk AI system with the requirements set out in Chapter 2 of this Title. This includes access to logs automatically generated by the high-risk AI system, where applicable.|81c283f0a67e40bdb86537ff5ec3c016|||
high-risk ai system|cooperation|Importers shall also cooperate with national competent authorities on any action they take in relation to a high-risk AI system.|81c283f0a67e40bdb86537ff5ec3c016|||
high-risk ai system|requirements set out in chapter 2 of this title|conforms to|27f45552b38a4797a14ed1e89485263f|||
distributor|storage or transport conditions|does not jeopardize|27f45552b38a4797a14ed1e89485263f|||
high-risk ai system|conformity with requirements set out in chapter 2 of this title|maintained while under responsibility of distributor|27f45552b38a4797a14ed1e89485263f|||
distributor|corrective actions necessary to bring system into conformity|takes if considers or has reason to consider system is not in conformity|27f45552b38a4797a14ed1e89485263f|||
high-risk ai system|presenting a risk within meaning of article 65(1)|informs provider or importer of the system, as applicable, to that effect|27f45552b38a4797a14ed1e89485263f|||
high-risk ai system|national competent authorities of member states|shall immediately inform the national competent authorities of the Member States in which it has made the product available to that effect, giving details, in particular, of the non-compliance and of any corrective actions taken.|f25e9dc02b4649759a25b98991303978|||
distributor|national competent authorities of member states|shall immediately inform the national competent authorities of the Member States in which it has made the product available to that effect, giving details, in particular, of the non-compliance and of any corrective actions taken.|f25e9dc02b4649759a25b98991303978|||
high-risk ai system|distributor|represents a risk within the meaning of Article 65(1)|f25e9dc02b4649759a25b98991303978|||
national competent authorities of member states|distributor|shall immediately inform, giving details, in particular, of the non-compliance and of any corrective actions taken|f25e9dc02b4649759a25b98991303978|||
importer|high-risk ai system|distributor under Article 16|f25e9dc02b4649759a25b98991303978|||
user|high-risk ai system|distributor under Article 16|f25e9dc02b4649759a25b98991303978|||
third-party|high-risk ai system|distributor under Article 16|f25e9dc02b4649759a25b98991303978|||
high-risk ai systems|users of high-risk ai systems|Users shall use such systems in accordance with the instructions of use accompanying the systems, pursuant to paragraphs 2 and 5. This obligation is without prejudice to other user obligations under Union or national law and to the user's discretion in organising its own resources and activities for the purpose of implementing the human oversight measures indicated by the provider.|bea2229190e24f739df4458ab335a7e1|||
high-risk ai systems|input data|To the extent the user exercises control over the input data, that user shall ensure that input data is relevant in view of the intended purpose of the high-risk AI system.|bea2229190e24f739df4458ab335a7e1|||
high-risk ai systems|providers or distributors|When users have reasons to consider that the use in accordance with the instructions of use may result in the AI system presenting a risk within the meaning of Article 65(1), they shall inform the provider or distributor and suspend the use of the system.|bea2229190e24f739df4458ab335a7e1|||
high-risk ai systems|providers or distributors|Users shall also inform the provider or distributor when they|bea2229190e24f739df4458ab335a7e1|||
ai system|user|The user has the responsibility to inform the provider or distributor when they have identified any serious incident or any malfunctioning within the meaning of Article 62 and interrupt the use of the AI system.|5ed3912d4add4b46bf83b53aab483682|||
ai system|provider/distributor|In case the user is not able to reach the provider, Article 62 shall apply mutatis mutandis.|5ed3912d4add4b46bf83b53aab483682|||
ai system|high-risk ai system|The monitoring obligation set out in the first subparagraph shall be deemed to be fulfilled by complying with the rules on internal governance arrangements, processes and mechanisms pursuant to Article 74 of Directive 2013/36/EU for users that are credit institutions regulated by Directive 2013/36/EU.|5ed3912d4add4b46bf83b53aab483682|||
logs|high-risk ai system|Users of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system, to the extent such logs are under their control.|5ed3912d4add4b46bf83b53aab483682|||
logs|credit institutions regulated by directive 2013/36/eu|Users that are credit institutions regulated by Directive 2013/36/EU shall maintain the logs as part of the documentation concerning internal governance arrangements, processes and mechanisms pursuant to Article 74 of that Directive.|5ed3912d4add4b46bf83b53aab483682|||
information|user|Users of high-risk AI systems shall use the information provided under Article 13 to comply with their obligation to carry out a data protection impact assessment under Article 35 of Regulation (EU) 2016/679 or Article 27 of Directive (EU) 2016/680, where applicable.|5ed3912d4add4b46bf83b53aab483682|||
article 30|member states|In paragraph 1, Member States are required to designate or establish a notifying authority responsible for the assessment, designation, and notification of conformity assessment bodies.|9a60eb0f1e5a4082a9e8eda4d825378f|||
article 30|national accreditation body referred to in regulation (ec) no 765/2008|In paragraph 2, Member States may designate a national accreditation body as a notifying authority.|9a60eb0f1e5a4082a9e8eda4d825378f|||
notifying authorities|conformity assessment bodies|In paragraphs 3 and 4, notifying authorities are established and organized in such a way that decisions relating to the notification of conformity assessment bodies are taken by competent persons different from those who carried out the assessment of those bodies.|9a60eb0f1e5a4082a9e8eda4d825378f|||
notifying authorities|confidentiality|In paragraph 6, notifying authorities shall safeguard the confidentiality of the information they obtain.|9a60eb0f1e5a4082a9e8eda4d825378f|||
notifying authorities|conformity assessment bodies and commercial/competitive basis|In paragraphs 5, notifying authorities shall not offer or provide any activities that conformity assessment bodies perform or any consultancy services on a commercial or competitive basis.|9a60eb0f1e5a4082a9e8eda4d825378f|||
conformity assessment bodies|notifying authority|A conformity assessment body applies for notification to the notifying authority in the Member State where it is established.|5620ebb251d14b7e9576bc6fb710e3a9|||
conformity assessment activities|member state's notifying authority|The application for notification submitted by a conformity assessment body includes details of the conformity assessment activities it carries out.|5620ebb251d14b7e9576bc6fb710e3a9|||
conformity assessment module or modules|member state's notifying authority|The application for notification submitted by a conformity assessment body specifies the conformity assessment modules for which it claims competence.|5620ebb251d14b7e9576bc6fb710e3a9|||
artificial intelligence technologies|member state's notifying authority|The application for notification submitted by a conformity assessment body indicates the artificial intelligence technologies for which it is qualified.|5620ebb251d14b7e9576bc6fb710e3a9|||
accreditation certificate|member state's notifying authority|The application for notification submitted by a conformity assessment body must include an accreditation certificate issued by the national accreditation body, where one exists, attesting its compliance with the requirements specified in Article 33.|5620ebb251d14b7e9576bc6fb710e3a9|||
existing designations of the applicant notified body|member state's notifying authority|Any valid document related to existing designations of the applicant notified body under any other Union harmonisation legislation should be included in the application for notification.|5620ebb251d14b7e9576bc6fb710e3a9|||
notifying authority|conformity assessment bodies|The Member State's notifying authority is responsible for reviewing applications submitted by conformity assessment bodies.|5620ebb251d14b7e9576bc6fb710e3a9|||
member state's notifying authority|conformity assessment body|If a conformity assessment body meets the requirements specified in Article 33, the Member State's notifying authority can designate it as a notified body.|5620ebb251d14b7e9576bc6fb710e3a9|||
member state's notifying authority|conformity assessment body|The Member State's notifying authority has the power to revoke a designation if the conformity assessment body no longer meets the requirements.|5620ebb251d14b7e9576bc6fb710e3a9|||
member state's notifying authority|competent personnel|The Member State's notifying authority should ensure that notified bodies have a sufficient number of competent personnel available for their tasks.|5620ebb251d14b7e9576bc6fb710e3a9|||
member state's notifying authority|proportionate manner|The Member State's notifying authority should ensure that conformity assessments are carried out in a proportionate manner, avoiding unnecessary burdens for providers.|5620ebb251d14b7e9576bc6fb710e3a9|||
member state's notifying authority|notified bodies|The Member State's notifying authority should ensure that notified bodies perform their activities taking due account of the size of an undertaking, the sector in which it operates, its structure and the degree of complexity of the AI system in question.|5620ebb251d14b7e9576bc6fb710e3a9|||
member state's notifying authority|confidentiality|The Member State's notifying authority should safeguard the confidentiality of the information it obtains.|5620ebb251d14b7e9576bc6fb710e3a9|||
existing designations of the applicant notified body under any other union harmonisation legislation|notifying authority|The conformity assessment body provides documentation necessary for verification, recognition and regular monitoring of compliance with requirements laid down in Article 33 to the notifying authority. This supports the designation procedure under this Regulation for notified bodies designated under any other Union harmonisation legislation.|935732932c1d4f42bc79b6866de7a1c1|||
conformity assessment body concerned|notifying authority|The notifying authority may notify only conformity assessment bodies that have satisfied the requirements laid down in Article 33. The notification includes full details of conformity assessment activities, conformity assessment modules or modules, and artificial intelligence technologies concerned.|935732932c1d4f42bc79b6866de7a1c1|||
conformity assessment body concerned|commission and other member states|The notifying authority shall notify the Commission and other Member States using the electronic notification tool developed and managed by the Commission.|935732932c1d4f42bc79b6866de7a1c1|||
conforming assessment activities, conformity assessment modules or modules, and artificial intelligence technologies concerned|commission and other member states|The notification shall include full details of conforming assessment activities, conformity assessment modules or modules, and artificial intelligence technologies concerned.|935732932c1d4f42bc79b6866de7a1c1|||
conformity assessment body concerned|commission and other member states|The conformity assessment body may perform the activities of a notified body only where no objections are raised by the Commission or the other Member States.|935732932c1d4f42bc79b6866de7a1c1|||
body only where no objections are raised by the commission or the other member states|conformity assessment procedures referred to in article 43|Notified bodies shall verify the conformity of high-risk AI system in accordance with the conformity assessment procedures referred to in Article 43, where no objections are raised by the Commission or the other Member States.|fba48ded90bd44a3bb7f3b729332ad91|||
high-risk ai system|notified bodies|Notified bodies shall verify the conformity of high-risk AI system in accordance with the conformity assessment procedures referred to in Article 43.|fba48ded90bd44a3bb7f3b729332ad91|||
subsequent relevant changes|commission and the other member states|Notifying authorities shall notify the Commission and the other Member States of any subsequent relevant changes to the notification.|fba48ded90bd44a3bb7f3b729332ad91|||
organisational structure|reporting lines|The organisational structure, allocation of responsibilities, reporting lines and operation of notified bodies shall be such as to ensure that there is confidence in the performance by and in the results of the conformity assessment activities that the notified bodies conduct.|fba48ded90bd44a3bb7f3b729332ad91|||
independence|provider of a high-risk ai system|Notified bodies shall be independent of the provider of a high-risk AI system in relation to which it performs conformity assessment activities.|fba48ded90bd44a3bb7f3b729332ad91|||
economic interest|other operator having an economic interest in the high-risk ai system that is assessed|Notified bodies shall also be independent of any other operator having an economic interest in the high-risk AI system that is assessed, as well as of any competitors of the provider.|fba48ded90bd44a3bb7f3b729332ad91|||
independence|activities|Notified bodies shall be organised and operated so as to safeguard the independence, objectivity and impartiality of their activities.|fba48ded90bd44a3bb7f3b729332ad91|||
documentation|implementation|Notified bodies shall document and implement a structure and procedures to safeguard impartiality and to promote and ensure the quality management required by this Regulation.|fba48ded90bd44a3bb7f3b729332ad91|||
impartiality|notified bodies|Notified bodies shall have documented procedures in place ensuring that their personnel, committees, subsidiaries, subcontractors and any associated body or personnel of external bodies respect the confidentiality of the information which comes into their possession during the performance of conformity assessment activities, except when disclosure is required by law. The staff of notified bodies shall be bound to observe professional secrecy with regard to all information obtained in carrying out their tasks under this Regulation, except in relation to the notifying authorities of the Member State in which their activities are carried out.|3b1f3213639c4bc79fca4926e3810946|||
impartiality|personnel|Notified bodies shall have documented procedures in place ensuring that their personnel respect the confidentiality of the information which comes into their possession during the performance of conformity assessment activities, except when disclosure is required by law.|3b1f3213639c4bc79fca4926e3810946|||
impartiality|committees|Notified bodies shall have documented procedures in place ensuring that their committees respect the confidentiality of the information which comes into their possession during the performance of conformity assessment activities, except when disclosure is required by law.|3b1f3213639c4bc79fca4926e3810946|||
impartiality|subsidiaries|Notified bodies shall have documented procedures in place ensuring that their subsidiaries respect the confidentiality of the information which comes into their possession during the performance of conformity assessment activities, except when disclosure is required by law.|3b1f3213639c4bc79fca4926e3810946|||
impartiality|subcontractors|Notified bodies shall have documented procedures in place ensuring that their subcontractors respect the confidentiality of the information which comes into their possession during the performance of conformity assessment activities, except when disclosure is required by law.|3b1f3213639c4bc79fca4926e3810946|||
impartiality|associated body or personnel of external bodies|Notified bodies shall have documented procedures in place ensuring that the associated body or personnel of external bodies respect the confidentiality of the information which comes into their possession during the performance of conformity assessment activities, except when disclosure is required by law.|3b1f3213639c4bc79fca4926e3810946|||
impartiality|staff|The staff of notified bodies shall be bound to observe professional secrecy with regard to all information obtained in carrying out their tasks under this Regulation, except in relation to the notifying authorities of the Member State in which their activities are carried out.|3b1f3213639c4bc79fca4926e3810946|||
notified bodies|appropriate liability insurance|Notified bodies shall take out appropriate liability insurance for their conformity assessment activities, unless liability is assumed by the Member State concerned in accordance with national law or that Member State is directly responsible for the conformity assessment.|3b1f3213639c4bc79fca4926e3810946|||
notified bodies|conformity assessment activities|Notified bodies shall be capable of carrying out all the tasks falling to them under this Regulation.|3b1f3213639c4bc79fca4926e3810946|||
conformity assessment|notified bodies|Notified bodies shall be capable of carrying out all the tasks falling to them under this Regulation with the highest degree of professional integrity and the requisite competence in the specific field, whether those tasks are carried out by notified bodies themselves or on their behalf and under their responsibility.|e51424717e624320bd68a73f9c1b0f7e|||
notified bodies|sufficient internal competences|Notified bodies shall have sufficient internal competences to be able to effectively evaluate the tasks conducted by external parties on their behalf. To that end, at all times and for each conformity assessment procedure and each type of high-risk AI system in relation to which they have been designated, the notified body shall have permanent availability of sufficient administrative, technical and scientific personnel who possess experience and knowledge relating to the relevant artificial intelligence technologies, data and data computing and to the requirements set out in Chapter 2 of this Title.|e51424717e624320bd68a73f9c1b0f7e|||
notified bodies|permanent availability|For each conformity assessment procedure and each type of high-risk AI system in relation to which they have been designated, the notified body shall have permanent availability of sufficient administrative, technical and scientific personnel who possess experience and knowledge relating to the relevant artificial intelligence technologies, data and data computing and to the requirements set out in Chapter 2 of this Title.|e51424717e624320bd68a73f9c1b0f7e|||
notified bodies|participation in coordination activities|Notified bodies shall participate in coordination activities as referred to in Article 38.|e51424717e624320bd68a73f9c1b0f7e|||
notified bodies|awareness and up-to-dateness|They shall also take part directly or be represented in European standardisation organisations, or ensure that they are aware and up to date in respect of relevant standards.|e51424717e624320bd68a73f9c1b0f7e|||
notified bodies|submission upon request|Notified bodies shall make available and submit upon request all relevant documentation.|e51424717e624320bd68a73f9c1b0f7e|||
notified bodies|relevant documentation|Notified bodies shall make available and submit upon request all relevant documentation, including the providers' documentation, to the notifying authority referred to in Article 30 to allow it to conduct its assessment, designation, notification, monitoring and surveillance activities and to facilitate the assessment outlined in this Chapter.|dc83c48adc5348bda42e8a7cf7ed07ed|||
notified bodies|subcontractors or subsidiaries|Where a notified body subcontracts specific tasks connected with the conformity assessment or has recourse to a subsidiary, it shall ensure that the subcontractor or the subsidiary meets the requirements laid down in Article 33 and shall inform the notifying authority accordingly.|dc83c48adc5348bda42e8a7cf7ed07ed|||
notified bodies|provider|Notified bodies shall take full responsibility for the tasks performed by subcontractors or subsidiaries wherever these are established.|dc83c48adc5348bda42e8a7cf7ed07ed|||
subcontractors or subsidiaries|notifying authority|Activities may be subcontracted or carried out by a subsidiary only with the agreement of the provider. Notified bodies shall keep at the disposal of the notifying authority the relevant documents concerning the assessment of the qualifications of the subcontractor or the subsidiary and the work carried out by them under this Regulation.|dc83c48adc5348bda42e8a7cf7ed07ed|||
notified bodies|identification number|The Commission shall assign an identification number to notified bodies. It shall assign a single number, even where a body is notified under several Union acts.|dc83c48adc5348bda42e8a7cf7ed07ed|||
single number|union acts|Assigned in context of bodies notified under multiple Union acts.|d350adb012cf424baa76cee32fea8b76|||
bodies notified|list|Made publicly available by Commission as part of notification process.|d350adb012cf424baa76cee32fea8b76|||
commission|list|Responsible for keeping list up to date.|d350adb012cf424baa76cee32fea8b76|||
notifying authority|notified body|Investigates suspicions or information of failure to meet requirements or fulfill obligations.|d350adb012cf424baa76cee32fea8b76|||
notifying authority|objections raised|Informs notified body concerned about objections.|d350adb012cf424baa76cee32fea8b76|||
notifying authority|restriction, suspension or withdrawal of notification|Appropriate action taken based on seriousness of failure.|d350adb012cf424baa76cee32fea8b76|||
notifying authority|commission and other member states|Immediately informs upon restriction, suspension or withdrawal of notification.|d350adb012cf424baa76cee32fea8b76|||
commission|notifying member states accordingly.|The Commission is responsible for ensuring that Member States follow the required procedures in line with the regulation.|0fbe24f85b1f4534a3b3e37a06962672|||
restriction, suspension, or withdrawal of notification|notification body has ceased its activity.|In certain circumstances, the Commission and Member States have the authority to restrict, suspend, or withdraw notifications granted to a notified body if it fails to meet the requirements.|0fbe24f85b1f4534a3b3e37a06962672|||
notifying authority|other member states accordingly.|When a restriction, suspension, or withdrawal of notification occurs, the notifying authority is responsible for ensuring that files related to the notified body are either transferred to another notified body or made available upon request from other responsible authorities.|0fbe24f85b1f4534a3b3e37a06962672|||
commission|notified body complies with requirements laid down in article 33.|The Commission has the authority to investigate instances where doubts arise about a notified body's compliance with the necessary requirements.|0fbe24f85b1f4534a3b3e37a06962672|||
notifying authority|all relevant information relating to the notification of the notified body concerned.|Upon request from the Commission, Member States are required to provide all pertinent data concerning the notified body under investigation.|0fbe24f85b1f4534a3b3e37a06962672|||
commission|confidential information obtained in the course of its investigations pursuant to this article is treated confidentially.|The Commission guarantees that any confidential data gathered during its investigations shall be safeguarded and preserved.|0fbe24f85b1f4534a3b3e37a06962672|||
commission|ascertains that a notified body does not meet or no longer meets the requirements laid down in article 33.|If, during its investigations, the Commission determines that a notified body fails to meet or no longer complies with the stipulated requirements, it has the authority to request corrective action from the responsible Member State.|0fbe24f85b1f4534a3b3e37a06962672|||
commission|implementing act shall be adopted in accordance with the examination procedure referred to in article 74(2).|If corrective action is not taken, the Commission may withdraw notification as an ultimate measure.|0fbe24f85b1f4534a3b3e37a06962672|||
commission|coordination of notified bodies.|The Commission plays a vital role in coordinating activities and ensuring compliance with regulatory requirements among notified bodies across the EU.|0fbe24f85b1f4534a3b3e37a06962672|||
conformity assessment bodies|third countries|According to Article 39, conformity assessment bodies established under the law of a third country with which the Union has concluded an agreement may be authorised to carry out the activities of notified Bodies under this Regulation.|c2f89d41cfbd441e8de1a5ab1ac9237f|||
harmonised standards|high-risk ai systems|High-risk AI systems which are in conformity with harmonised standards or parts thereof the references of which have been published in the Official Journal of the European Union shall be presumed to be in conformity with the requirements set out in Chapter 2 of this Title, to the extent those standards cover those requirements (Article 40).|c2f89d41cfbd441e8de1a5ab1ac9237f|||
sectoral group of notified bodies|notified bodies active|The Commission shall ensure that, with regard to the areas covered by this Regulation, appropriate coordination and cooperation between notified bodies active in the conformity assessment procedures of AI systems pursuant to this Regulation are put in place and properly operated in the form of a sectoral group of notified bodies (Article 38, paragraph 1).|c2f89d41cfbd441e8de1a5ab1ac9237f|||
notified bodies|member states|Member States shall ensure that the bodies notified by them participate in the work of that group, directly or by means of designated representatives (Article 38, paragraph 2).|c2f89d41cfbd441e8de1a5ab1ac9237f|||
high-risk ai systems|common specifications referred to in article 41 paragraph 1|High-risk AI systems which are in conformity with the common specifications referred to in paragraph 1 of Article 41 shall be presumed to be in conformity with the requirements set out in Chapter 2 of this Title, to the extent those common specifications cover those requirements.|5f5475abd467402dae655d2bdb9fc3cc|||
high-risk ai systems|providers|Where providers do not comply with the common specifications referred to in paragraph 1 of Article 41, they shall duly justify that they have adopted technical solutions that are at least equivalent thereto.|5f5475abd467402dae655d2bdb9fc3cc|||
high-risk ai systems|intended purpose|Taking into account their intended purpose, high-risk AI systems that have been trained and tested on data concerning the specific geographical, behavioural and|5f5475abd467402dae655d2bdb9fc3cc|||
high-risk ai systems|cybersecurity requirements set out in article 15 of this regulation|If the cybersecurity certificate or statement of conformity covers those requirements for high-risk AI systems listed in point 1 of Annex III, then they are presumed to be in compliance with the cybersecurity requirements.|59e9aff5ef4e47ee87ff2aedb9ba500c|||
high-risk ai systems|conformity assessment procedures based on internal control or assessment of the quality|In demonstrating compliance with the requirements set out in Chapter 2 of this Title for high-risk AI systems listed in point 1 of Annex III, the provider can follow either the conformity assessment procedure based on internal control referred to in Annex VI or the conformity assessment procedure based on assessment of the quality.|59e9aff5ef4e47ee87ff2aedb9ba500c|||
high-risk ai systems|harmonised standards referred to in article 40 or common specifications referred to in article 41|For high-risk AI systems listed in point 1 of Annex III, the provider can demonstrate compliance with the requirements set out in Chapter 2 of this Title by applying harmonised standards referred to in Article 40 or common specifications referred to in Article 41.|59e9aff5ef4e47ee87ff2aedb9ba500c|||
annex vi|conformity assessment procedure based on assessment of quality management system and technical documentation with involvement of notified body referred to in annex vii|is related to|886032f3fc454eb3b547a300f5a3ce3d|||
high-risk ai systems referred to in points 2 to 8 of annex iii|conformity assessment procedure based on internal control as referred to in annex vi|is related to (for high-risk AI systems)|886032f3fc454eb3b547a300f5a3ce3d|||
annex vii|provider may choose any of the notified bodies|is related to|886032f3fc454eb3b547a300f5a3ce3d|||
annex vii|market surveillance authority referred to in article 63(5) or (6), as applicable, acts as a notified body for high-risk ai systems intended to be put into service by law enforcement, immigration or asylum authorities as well as eu institutions, bodies or agencies|is related to|886032f3fc454eb3b547a300f5a3ce3d|||
high-risk ai systems referred to in point 5(b) of annex iii|conformity assessment procedure based on internal control as referred to in annex vi|is related to (for high-risk AI systems)|886032f3fc454eb3b547a300f5a3ce3d|||
credit institutions regulated by directive 2013/36/eu|high-risk ai systems referred to in point 5(b) of annex iii (placed on the market or put into service by)|is related to|886032f3fc454eb3b547a300f5a3ce3d|||
high-risk ai systems referred to in points 2 to 8 of annex iii|conformity assessment procedure referred to in annex vii or parts thereof|subjected to conformity assessment procedure due to technical progress and potential risks to health, safety, and fundamental rights|3ba3517ee9a54778b5acdf5602a0d7b9|||
annex iii|high-risk ai systems|defined in Annex III based on technical progress and potential risks|3ba3517ee9a54778b5acdf5602a0d7b9|||
points 2 to 8 of annex iii|high-risk ai systems referred to in points 2 to 8 of annex iii|specified within the definition of high-risk AI systems in Annex III|3ba3517ee9a54778b5acdf5602a0d7b9|||
annex vii or parts thereof|conformity assessment procedure referred to in annex vii or parts thereof|specified in Annex VII or its relevant parts for high-risk AI systems|3ba3517ee9a54778b5acdf5602a0d7b9|||
annex vi|effectiveness of the conformity assessment procedure based on internal control referred to in annex vi|determines the effectiveness of the internal control conformity assessment procedure for high-risk AI systems|3ba3517ee9a54778b5acdf5602a0d7b9|||
member state|official union language determined by the member state|specifies an official Union language for certificates issued by notified bodies based on Member States' preferences or requirements|3ba3517ee9a54778b5acdf5602a0d7b9|||
five years|validity of certificates|sets the initial validity period for certificates issued by notified bodies, limited to five years|3ba3517ee9a54778b5acdf5602a0d7b9|||
further periods|validity of certificates|allows for extensions of certificate validity based on re-assessment in accordance with applicable conformity assessment procedures, each not exceeding five years|3ba3517ee9a54778b5acdf5602a0d7b9|||
ai system|meets the requirements set out in chapter 2 of this title|When a notified body finds that an AI system no longer meets the requirements set out in Chapter 2 of this Title, it shall suspend or withdraw the certificate issued or impose any restrictions on it, unless compliance with those requirements is ensured by appropriate corrective action taken by the provider of the system within an appropriate deadline set by the notified body. This decision follows the principle of proportionality.|c93ee19afc49488f9d7eb36c63b33992|||
ai system|notified body|A notified body is responsible for assessing whether an AI system meets the requirements set out in Chapter 2 of this Title and issuing a Union technical documentation assessment certificate. This decision follows the principle of proportionality.|c93ee19afc49488f9d7eb36c63b33992|||
union technical documentation assessment certificate|refusal, restriction, suspension or withdrawal|When a notified body refuses, restricts, suspends, or withdraws a Union technical documentation assessment certificate issued in accordance with the requirements of Annex VII, it shall give reasons for its decision.|c93ee19afc49488f9d7eb36c63b33992|||
union technical documentation assessment certificate|quality management system approvals issued in accordance with the requirements of annex vii|When a notified body issues a Union technical documentation assessment certificate or a quality management system approval, it shall inform the notifying authority of these decisions.|c93ee19afc49488f9d7eb36c63b33992|||
appeal procedure|parties having a legitimate interest in that decision|Member States shall ensure that an appeal procedure against decisions of notified bodies is available to parties having a legitimate interest in that decision.|c93ee19afc49488f9d7eb36c63b33992|||
notified body|information obligations of notified bodies|Notified bodies shall inform the notifying authority of various decisions and circumstances as outlined in Article 46.|c93ee19afc49488f9d7eb36c63b33992|||
annex vii|requirements for notification|in accordance with|95a32b731ee94a03b4d4791dd2a1cc3c|||
circumstances affecting scope of or conditions for notification|notification|affecting|95a32b731ee94a03b4d4791dd2a1cc3c|||
request for information from market surveillance authorities|notified body|regarding conformity assessment activities|95a32b731ee94a03b4d4791dd2a1cc3c|||
conformity assessment activities performed within scope of notification|notified body|on request, including cross-border activities and subcontracting|95a32b731ee94a03b4d4791dd2a1cc3c|||
quality management system approvals refused, suspended, or withdrawn|notified body|shall inform other notified bodies|95a32b731ee94a03b4d4791dd2a1cc3c|||
eu technical documentation assessment certificates or supplements thereto refused, withdrawn, suspended, or restricted|notified body|shall inform other notified bodies upon request|95a32b731ee94a03b4d4791dd2a1cc3c|||
relevant information on issues relating to negative and positive conformity assessment results|other notified bodies carrying out similar conformity assessment activities covering the same artificial intelligence technologies|shall provide upon request|95a32b731ee94a03b4d4791dd2a1cc3c|||
market surveillance authority|placing on the market or putting into service of specific high-risk ai|may authorise by way of derogation from Article 43|95a32b731ee94a03b4d4791dd2a1cc3c|||
authorise the placing on the market or putting into service of specific high-risk ai systems|exceptional reasons of public security or the protection of life and health of persons, environmental protection and the protection of key industrial and infrastructural assets.|required for specific high-risk AI systems due to exceptional reasons related to public security, protection of life and health of persons, environmental protection and protection of key industrial and infrastructural assets.|2e99c7e2badb4648a58a8cf6a328c74c|||
market surveillance authority|high-risk ai system complies with the requirements of chapter 2 of this title.|evaluated by market surveillance authority to ensure compliance with requirements for high-risk AI systems as specified in Chapter 2.|2e99c7e2badb4648a58a8cf6a328c74c|||
market surveillance authority|commission and the other member states|required to inform Commission and other Member States of any authorisation issued pursuant to paragraph 1.|2e99c7e2badb4648a58a8cf6a328c74c|||
information referred to in paragraph 2|no objection raised by either a member state or the commission in respect of an authorisation issued by a market surveillance authority of a member state in accordance with paragraph 1.|authorisation is deemed justified after 15 calendar days if no objections are raised.|2e99c7e2badb4648a58a8cf6a328c74c|||
high-risk ai systems|devices covered by regulation (eu) 2017/745 and regulation (eu) 2017/746|High-risk AI systems intended to be used as safety components of devices, or which are themselves devices, covered by Regulation (EU) 2017/745 and Regulation (EU) 2017/746 shall apply also with regard to the derogation from the conformity assessment of the compliance with the requirements set out in Chapter 2 of this Title.|9c8c52f8e66e4e6b9bf1c71c93333a20|||
member state|commission|When objections are raised by a Member State against an authorisation issued by a market surveillance authority of another Member State, or where the Commission considers the authorisation to be contrary to Union law or the conclusion of the Member States regarding the compliance of the system as referred to in paragraph 2 to be unfounded, the Commission shall without delay enter into consultation with the relevant Member State; the operator(s) concerned shall be consulted and have the possibility to present their views. In view thereof, the Commission shall decide whether the authorisation is justified or not. The Commission shall address its decision to the Member State concerned and the relevant operator or operators.|9c8c52f8e66e4e6b9bf1c71c93333a20|||
commission|authorisation|If the authorisation is considered unjustified, this shall be withdrawn by the market surveillance authority of the Member State concerned.|9c8c52f8e66e4e6b9bf1c71c93333a20|||
ai system|high-risk ai system|AI system and high-risk AI system are related as high-risk AI systems fall under the category of AI systems.|27a4012944a34671a059000a21388c59|||
eu declaration of conformity|high-risk ai system|A EU declaration of conformity is required for each high-risk AI system and contains information about whether the system meets the requirements set out in Chapter 2 of this Title.|27a4012944a34671a059000a21388c59|||
eu declaration of conformity|ai system|A EU declaration of conformity may also be required for AI systems that are not classified as high-risk, if they fall under other Union harmonisation legislation.|27a4012944a34671a059000a21388c59|||
national competent authorities|eu declaration of conformity|The provider is required to keep a copy of the EU declaration of conformity for each AI system at the disposal of national competent authorities for 10 years after placement on the market or putting into service.|27a4012944a34671a059000a21388c59|||
relevant national competent authorities|eu declaration of conformity|A copy of the EU declaration of conformity shall be given to the relevant national competent authorities upon request.|27a4012944a34671a059000a21388c59|||
union harmonisation legislation|declaration relates|The context mentions the need to identify Union harmonisation legislation that a declaration relates to.|91466d1b3a2e42d99123e334ed35871a|||
provider|compliance with requirements set out in chapter 2 of this title|By drawing up an EU declaration of conformity, the provider assumes responsibility for compliance with the requirements set out in Chapter 2 of this Title.|91466d1b3a2e42d99123e334ed35871a|||
provider|up-to-date as appropriate|The context mentions that the provider should keep the EU declaration of conformity up-to-date as appropriate.|91466d1b3a2e42d99123e334ed35871a|||
commission|update content of eu declaration of conformity|The Commission is empowered to adopt delegated acts in accordance with Article 73 to update the content of the EU declaration of conformity.|91466d1b3a2e42d99123e334ed35871a|||
ce marking|conformity assessment procedures set out in article 43|Where applicable, the CE marking should be followed by the identification number of the notified body responsible for conformity assessment procedures.|91466d1b3a2e42d99123e334ed35871a|||
ce marking|promotional material|The CE marking should also be indicated in any promotional material that mentions the high-risk AI system fulfils the requirements for CE marking.|91466d1b3a2e42d99123e334ed35871a|||
high-risk ai system|ce requirements|Article 50 mentions that the high-risk AI system fulfils the requirements for CE.|447abaeb119c458380782556fb50e584|||
provider|registration in eu database|Before placing on the market or putting into service a high-risk AI system, the provider or authorised representative shall register that system in the EU database referred to in Article 60.|447abaeb119c458380782556fb50e584|||
ai system|interaction with natural persons|Providers shall ensure that AI systems intended to interact with natural persons are designed and developed in such a way that natural persons are informed that they are interacting with an AI system, unless this is obvious from the circumstances and the|447abaeb119c458380782556fb50e584|||
ai system|user|interacting with an AI system, unless this is obvious from the circumstances and the context of use.|2afb0be0ded041c08d13d432b6dcd80e|||
emotion recognition system|natural persons exposed thereto|Users of an emotion recognition system or a biometric categorisation system shall inform of the operation of the system the natural persons exposed thereto.|2afb0be0ded041c08d13d432b6dcd80e|||
biometric categorisation system|natural persons exposed thereto|This obligation shall not apply to AI systems used for biometric categorisation, which are permitted by law to detect, prevent and investigate criminal offences.|2afb0be0ded041c08d13d432b6dcd80e|||
deep fake|content|AI system that generates or manipulates image, audio or video content that appreciably resembles existing persons, objects, places or other entities or events and would falsely appear to a person to be authentic or truthful (`deep fake'), shall disclose that the content has been artificially generated or manipulated.|2afb0be0ded041c08d13d432b6dcd80e|||
charter of fundamental rights of the eu|use|The first subparagraph shall not apply where the use is authorised by law to detect, prevent, investigate and prosecute criminal offences or it is necessary for the exercise of the right to freedom of expression and the right to freedom of the arts and sciences guaranteed in the Charter of Fundamental Rights of the EU, and subject to appropriate safeguards for the rights and freedoms of third parties.|2afb0be0ded041c08d13d432b6dcd80e|||
ai regulatory sandboxes|member states competent authorities or the european data protection supervisor|established by one or more Member States competent authorities or the European Data Protection Supervisor shall provide a controlled environment that facilitates the development, testing and validation of innovative AI systems for a limited time before their placement on the market or putting into service pursuant to a specific plan.|0df63552c3424f19a835897b73ba80ac|||
innovative ai systems|ai regulatory sandboxes|shall provide a controlled environment that facilitates the development, testing and validation of for a limited time before their placement on the market or putting into service pursuant to a specific plan.|0df63552c3424f19a835897b73ba80ac|||
innovative ai systems|member states competent authorities or the european data protection supervisor|direct supervision and guidance by the competent authorities with a view to ensuring compliance with the requirements of this Regulation and, where relevant, other Union and Member States legislation supervised within the sandbox.|0df63552c3424f19a835897b73ba80ac|||
innovative ai systems|national data protection authorities|associated to the operation of the AI regulatory sandbox where the innovative AI systems involve the processing of personal data or otherwise fall under the supervisory remit of other national authorities or competent authorities providing or supporting access to data.|0df63552c3424f19a835897b73ba80ac|||
ai regulatory sandboxes|supervisory and corrective powers|shall not affect the supervisory and corrective powers of national data protection authorities and those other national authorities associated to the operation of the AI regulatory sandbox.|0df63552c3424f19a835897b73ba80ac|||
ai regulatory sandbox|operation|The AI regulatory sandboxes shall not affect the supervisory and corrective powers of the competent authorities. Any significant risks to health and safety and fundamental rights identified during the development and testing of such systems shall result in immediate mitigation and, failing that, in the suspension of the development and testing process until such mitigation takes place.|a059e1686bfd47a48ab57c991a31a676|||
ai regulatory sandbox|participants|Participants in the AI regulatory sandbox shall remain liable under applicable Union and Member States liability legislation for any harm inflicted on third parties as a result from the experimentation taking place in the sandbox.|a059e1686bfd47a48ab57c991a31a676|||
ai regulatory sandbox|member states' competent authorities|Member States' competent authorities that have established AI regulatory sandboxes shall coordinate their activities and cooperate within the framework of the European Artificial Intelligence Board.|a059e1686bfd47a48ab57c991a31a676|||
ai regulatory sandbox|eligibility criteria|The modalities and the conditions of the operation of the AI regulatory sandboxes, including the eligibility criteria and the procedure for the application, selection, participation and exiting from the sandbox,|a059e1686bfd47a48ab57c991a31a676|||
ai regulatory sandbox|procedure for application|The modalities and the conditions of the operation of the AI regulatory sandboxes, including the eligibility criteria and the procedure for the application, selection, participation and exiting from the sandbox,|a059e1686bfd47a48ab57c991a31a676|||
ai regulatory sandbox|selection|The modalities and the conditions of the operation of the AI regulatory sandboxes, including the eligibility criteria and the procedure for the application, selection, participation and exiting from the sandbox,|a059e1686bfd47a48ab57c991a31a676|||
ai regulatory sandbox|participation|The modalities and the conditions of the operation of the AI regulatory sandboxes, including the eligibility criteria and the procedure for the application, selection, participation and exiting from the sandbox,|a059e1686bfd47a48ab57c991a31a676|||
ai regulatory sandbox|exiting from the sandbox|The modalities and the conditions of the operation of the AI regulatory sandboxes, including the eligibility criteria and the procedure for the application, selection, participation and exiting from the sandbox,|a059e1686bfd47a48ab57c991a31a676|||
ai regulatory sandbox|rights and obligations of participants|Participants in the AI regulatory sandbox shall remain liable under applicable Union and Member States liability legislation for any harm inflicted on third parties as a result from the experimentation taking place in the sandbox.|a059e1686bfd47a48ab57c991a31a676|||
ai regulatory sandbox|annual reports|Member States' competent authorities that have established AI regulatory sandboxes shall submit annual reports to the Board and the Commission on the results from the implementation of those scheme, including good practices, lessons learnt and recommendations on their setup and, where relevant, on the application of this Regulation and other Union legislation supervised within the sandbox.|a059e1686bfd47a48ab57c991a31a676|||
ai regulatory sandbox|coordination and cooperation|Member States' competent authorities that have established AI regulatory sandboxes shall coordinate their activities and cooperate within the framework of the European Artificial Intelligence Board.|a059e1686bfd47a48ab57c991a31a676|||
participation and exiting from the sandbox|developing certain ai systems in the public interest in the ai regulatory sandbox|According to Article 54, personal data collected for other purposes can be processed for the purpose of developing innovative AI systems in the sandbox under specific conditions. These systems are developed for safeguarding substantial public interests in areas such as preventing criminal offences, ensuring public safety and health, and improving environmental protection.|27c18b4f9b93423eaed24ab67975fc3d|||
personal data|developing certain ai systems in the public interest in the ai regulatory sandbox|The processing of personal data is permitted for the purpose of developing innovative AI systems in the sandbox, provided that they are necessary for complying with requirements related to criminal offences, public safety and health, or environmental protection, as specified in Title III, Chapter 2.|27c18b4f9b93423eaed24ab67975fc3d|||
personal data|participation and exiting from the sandbox|In the context of the sandbox, personal data collected for other purposes may also be processed for developing certain AI systems in the public interest, as outlined in Article 54.|27c18b4f9b93423eaed24ab67975fc3d|||
ai regulatory sandbox|developing certain ai systems in the public interest|The AI regulatory sandbox provides a controlled environment for developing and testing innovative AI systems that serve substantial public interests, as described in Article 54.|27c18b4f9b93423eaed24ab67975fc3d|||
ai system testing|technical documentation|The complete and detailed description of the process and rationale behind the training, testing, and validation of the AI system is kept together with the testing results as part of the technical documentation in Annex IV.|460d8d7337c54ef4b82f2607b929b618|||
ai project objectives|short summary of ai project developed in sandbox|Published on the website of the competent authorities.|460d8d7337c54ef4b82f2607b929b618|||
small-scale providers and start-ups|priority access to ai regulatory sandboxes|Provided by Member States to the extent that they fulfil the eligibility conditions, as mentioned in Article 55(1)(a) of the Regulation.|460d8d7337c54ef4b82f2607b929b618|||
small-scale providers and users|specific awareness raising activities about ai|Tailored to their needs, as mentioned in Article 55(1)(b) of the Regulation.|460d8d7337c54ef4b82f2607b929b618|||
small-scale providers and users|dedicated channel for communication with small-scale providers and user and other innovators|Established by Member States, as mentioned in Article 55(1)(c) of the Regulation.|460d8d7337c54ef4b82f2607b929b618|||
small-scale providers|guidance and response to queries about implementation|when setting fees for conformity assessment under Article 43, their specific interests and needs shall be taken into account proportionately to their size and market size (as per Title VI, Chapter 1, EUROPEAN ARTIFICIAL INTELLIGENCE BOARD, Article 56, Paragraph 2, Point a)|838ac67a0fc444d5955f174e756d12e3|||
european artificial intelligence board|advice and assistance to the commission (as per title vi, chapter 1, european artificial intelligence board, article 56, paragraph 2)||838ac67a0fc444d5955f174e756d12e3|||
european artificial intelligence board|contribution to guidance and analysis by the commission and national supervisory authorities (as per title vi, chapter 1, european artificial intelligence board, article 56, paragraph 2, point b)||838ac67a0fc444d5955f174e756d12e3|||
european artificial intelligence board|assistance in ensuring consistent application of this regulation (as per title vi, chapter 1, european artificial intelligence board, article 56, paragraph 2, point c)||838ac67a0fc444d5955f174e756d12e3|||
national supervisory authorities|composition of the board (as per title vi, chapter 1, european artificial intelligence board, article 57)||838ac67a0fc444d5955f174e756d12e3|||
board|national supervisory authorities|The Board shall be composed of national supervisory authorities represented by their heads or equivalent high-level officials.|2a5010f8f0b248eda0314ea8ffc2fa66|||
board|european data protection supervisor|The Board shall also include the European Data Protection Supervisor.|2a5010f8f0b248eda0314ea8ffc2fa66|||
meetings|national authorities|Other national authorities may be invited to meetings where the issues discussed are of relevance for them.|2a5010f8f0b248eda0314ea8ffc2fa66|||
board|commission|The Board shall be chaired by the Commission.|2a5010f8f0b248eda0314ea8ffc2fa66|||
meetings|commission|The Commission shall convene meetings and prepare the agenda in accordance with the tasks of the Board pursuant to this Regulation and its rules of procedure.|2a5010f8f0b248eda0314ea8ffc2fa66|||
board|external experts and observers|The Board may invite external experts and observers to attend its meetings.|2a5010f8f0b248eda0314ea8ffc2fa66|||
board|interested third parties|The Board may hold exchanges with interested third parties to inform its activities to an appropriate extent.|2a5010f8f0b248eda0314ea8ffc2fa66|||
commission|board|The Commission shall provide administrative and analytical support for the activities of the Board pursuant to this Regulation.|2a5010f8f0b248eda0314ea8ffc2fa66|||
article 58|board|The Board is mentioned in Article 58 of this context.|28c2e30c8f2947bcbdb40959832ad599|||
article 58|tasks of the board|The Tasks of the Board are specified in Article 58 of this context.|28c2e30c8f2947bcbdb40959832ad599|||
board|article 56(2)|The Board provides advice and assistance to the Commission in the context of Article 56(2) as mentioned in this context.|28c2e30c8f2947bcbdb40959832ad599|||
tasks of the board|article 53|The Board contributes to uniform administrative practices for the functioning of regulatory sandboxes referred to in Article 53 as mentioned in this context.|28c2e30c8f2947bcbdb40959832ad599|||
tasks of the board|(a)|The Board collects and shares expertise and best practices among Member States as specified under (a) in this context.|28c2e30c8f2947bcbdb40959832ad599|||
tasks of the board|(b)|The Board contributes to uniform administrative practices in the Member States as specified under (b) in this context.|28c2e30c8f2947bcbdb40959832ad599|||
tasks of the board|(c)|The Board issues opinions, recommendations or written contributions on matters related to the implementation of this Regulation as specified under (c) in this context.|28c2e30c8f2947bcbdb40959832ad599|||
tasks of the board|(i)|The Board provides opinions on technical specifications or existing standards regarding the requirements set out in Title III, Chapter 2 as specified under (i) in this context.|28c2e30c8f2947bcbdb40959832ad599|||
tasks of the board|(ii)|The Board provides opinions on the use of harmonised standards or common specifications referred to in Articles 40 and 41 as specified under (ii) in this context.|28c2e30c8f2947bcbdb40959832ad599|||
tasks of the board|(iii)|The Board provides opinions on the preparation of guidance documents, including the guidelines concerning the setting of administrative fines referred to in Article 71 as specified under (iii) in this context.|28c2e30c8f2947bcbdb40959832ad599|||
article 59|national competent authorities|The establishment or designation of national competent authorities is mentioned in Article 59 of this context.|28c2e30c8f2947bcbdb40959832ad599|||
national competent authorities|each member state|Each Member State establishes or designates national competent authorities as specified in this context.|28c2e30c8f2947bcbdb40959832ad599|||
national competent authorities|organisation|The organisation of national competent authorities is mentioned in this context.|28c2e30c8f2947bcbdb40959832ad599|||
regulation|objectivity and impartiality|National competent authorities shall be organised so as to safeguard the objectivity and impartiality of their activities and tasks.|a636a274b968418fb2c49fc5a8c84c95|||
member state|national supervisory authority|Each Member State shall designate a national supervisory authority among the national competent authorities. The national supervisory authority shall act as notifying authority and market surveillance authority unless a Member State has organisational and administrative reasons to designate more than one authority.|a636a274b968418fb2c49fc5a8c84c95|||
member state|designations|Member States shall inform the Commission of their designation or designations and, where applicable, the reasons for designating more than one authority.|a636a274b968418fb2c49fc5a8c84c95|||
national competent authorities|financial and human resources|Member States shall ensure that national competent authorities are provided with adequate financial and human resources to fulfil their tasks under this Regulation.|a636a274b968418fb2c49fc5a8c84c95|||
national competent authorities|personnel|National competent authorities shall have a sufficient number of personnel permanently available whose competences and expertise shall include an in-depth understanding of artificial intelligence technologies, data and data computing, fundamental rights, health and safety risks and knowledge of existing standards and legal requirements.|a636a274b968418fb2c49fc5a8c84c95|||
commission|reporting|Member States shall report to the Commission on an annual basis on the status of the financial and human resources of the national competent authorities with an assessment of their adequacy.|a636a274b968418fb2c49fc5a8c84c95|||
commission|transmission|The Commission shall transmit that information to the Board for discussion and possible recommendations.|a636a274b968418fb2c49fc5a8c84c95|||
eu database for stand-alone high-risk ai systems|high-risk ai systems referred to in article 6(2)|The EU database contains information regarding the registered high-risk AI systems.|59bf10160be14d23942cd35fc4b6a037|||
commission|eu database for stand-alone high-risk ai systems|The Commission is responsible for setting up and maintaining the EU database for stand-alone high-risk AI systems in collaboration with the Member States.|59bf10160be14d23942cd35fc4b6a037|||
high-risk ai systems referred to in article 6(2)|registration in accordance with article 51|High-risk AI systems referred to in Article 6(2) must be registered in the EU database for stand-alone high-risk AI systems.|59bf10160be14d23942cd35fc4b6a037|||
article 51|registration requirement for high-risk ai systems|Article 51 outlines the registration requirement for high-risk AI systems in the EU database for stand-alone high-risk AI systems.|59bf10160be14d23942cd35fc4b6a037|||
member states|eu database for stand-alone high-risk ai systems|The Member States collaborate with the Commission in setting up and maintaining the EU database for stand-alone high-risk AI systems.|59bf10160be14d23942cd35fc4b6a037|||
high-risk ai systems referred to in article 6(2)|national competent authorities|The registration of high-risk AI systems referred to in Article 6(2) is the responsibility of national competent authorities.|59bf10160be14d23942cd35fc4b6a037|||
high-risk ai systems referred to in article 6(2)|implementation of this regulation|The implementation of this Regulation regarding high-risk AI systems referred to in Article 6(2) is guided and advised by national competent authorities, as appropriate.|59bf10160be14d23942cd35fc4b6a037|||
union institutions, agencies and bodies|eu database for stand-alone high-risk ai systems|When Union institutions, agencies and bodies fall within the scope of this Regulation, the European Data Protection Supervisor acts as their competent authority for supervision in the EU database for stand-alone high-risk AI systems.|59bf10160be14d23942cd35fc4b6a037|||
article 6(2)|ai systems|Referred to in Article 6(2)|b8672f6fef3a40a2be27b981385d8321|||
eu database|information contained in the eu database|Shall be accessible to the public|b8672f6fef3a40a2be27b981385d8321|||
commission|controller of the eu database||b8672f6fef3a40a2be27b981385d8321|||
providers|natural persons who are responsible for registering the system and have the legal authority to represent the provider|Contained in the EU database|b8672f6fef3a40a2be27b981385d8321|||
providers|post-market monitoring system|Established and documented by providers|b8672f6fef3a40a2be27b981385d8321|||
high-risk ai systems|post-market monitoring system|Shall be established and documented by providers in a manner that is proportionate to the nature of the artificial intelligence technologies and the risks of the high-risk AI system|b8672f6fef3a40a2be27b981385d8321|||
providers|post-market monitoring plan for high-risk ai systems||b8672f6fef3a40a2be27b981385d8321|||
providers|post-market monitoring system|Shall actively and systematically collect,|b8672f6fef3a40a2be27b981385d8321|||
providers|data listed in annex viii|Shall be entered into the EU database by the providers|b8672f6fef3a40a2be27b981385d8321|||
serious incident|providers of high-risk ai systems placed on the union market|Providers of high-risk AI systems placed on the Union market shall report any serious incident related to their systems to market surveillance authorities in Member States where the incident occurred immediately after establishing a causal link or within 15 days of becoming aware.|a3b9f89b60c549aeb72e3c9c34a2e956|||
malfunctioning|providers of high-risk ai systems placed on the union market|Providers of high-risk AI systems placed on the Union market shall report any malfunctioning of those systems which constitutes a breach of obligations under Union law intended to protect fundamental rights to market surveillance authorities in Member States where that incident occurred immediately after establishing a causal link or within 15 days of becoming aware.|a3b9f89b60c549aeb72e3c9c34a2e956|||
serious incident|market surveillance authorities|Providers shall report any serious incident related to high-risk AI systems to market surveillance authorities in Member States where the incident occurred immediately after establishing a causal link or within 15 days of becoming aware.|a3b9f89b60c549aeb72e3c9c34a2e956|||
malfunctioning|market surveillance authorities|Providers shall report any malfunctioning of high-risk AI systems which constitutes a breach of obligations under Union law intended to protect fundamental rights to market surveillance authorities in Member States where that incident occurred immediately after establishing a causal link or within 15 days of becoming aware.|a3b9f89b60c549aeb72e3c9c34a2e956|||
market surveillance authorities|national public authorities or bodies referred to in article 64(3)|Upon receiving a notification related to a breach of obligations under Union law intended to protect fundamental rights, market surveillance authorities shall inform national public authorities or bodies referred to in Article 64(3).|a3b9f89b60c549aeb72e3c9c34a2e956|||
high-risk ai systems|providers that are credit institutions regulated by directive 2013/36/eu|For high-risk AI systems referred to in point 5(b) of Annex III which are placed on the market or put into service by providers that are credit institutions regulated by Directive 2013/36/EU|a3b9f89b60c549aeb72e3c9c34a2e956|||
high-risk ai systems|safety components of|For high-risk AI systems which are safety components of|a3b9f89b60c549aeb72e3c9c34a2e956|||
directive 2013/36/eu|union law intended to protect fundamental rights|Constitutes a breach of obligations under Union law|08aa6cf91da046a09555a219c13ffff0|||
ai systems covered by directive 2013/36/eu|ai systems falling within the scope of this regulation|Same as per Article 63.1|08aa6cf91da046a09555a219c13ffff0|||
ai systems covered by directive 2013/36/eu|ai systems covered by regulation (eu) 2019/1020|Same as per Article 63.1|08aa6cf91da046a09555a219c13ffff0|||
operators identified in title iii, chapter 3 of this regulation|economic operators under regulation (eu) 2019/1020|Same as per Article 63.1(a)|08aa6cf91da046a09555a219c13ffff0|||
products under regulation (eu) 2019/1020|ai systems falling within the scope of this regulation|Same as per Article 63.1(b)|08aa6cf91da046a09555a219c13ffff0|||
national supervisory authority|commission|Reports to the Commission on a regular basis|08aa6cf91da046a09555a219c13ffff0|||
national supervisory authority|relevant national competition authorities|Reports, without delay, to the Commission and relevant national competition authorities any information identified in the course of market surveillance activities that may be of potential interest for the application of Union law|08aa6cf91da046a09555a219c13ffff0|||
union law on competition rules|high-risk ai systems related to products in annex ii, section a|For these high-risk AI systems, the market surveillance authority for the purposes of this Regulation shall be the authority responsible for market surveillance activities designated under those legal acts listed in Annex II, section A.|84428023b7824ce2b7b94c05f0d1b5f3|||
financial institutions regulated by union legislation on financial services|ai systems placed on the market, put into service or used by these institutions|For AI systems placed on the market, put into service or used by financial institutions regulated by Union legislation on financial services, the market surveillance authority for the purposes of this Regulation shall be the relevant authority responsible for the financial supervision of those institutions under that legislation.|84428023b7824ce2b7b94c05f0d1b5f3|||
ai systems listed in point 1(a)|law enforcement purposes|For AI systems listed in point 1(a) in so far as the systems are used for law enforcement purposes, points 6 and 7 of Annex III, Member States shall designate as market surveillance authorities for the purposes of this Regulation either the competent data protection supervisory authorities under Directive (EU) 2016/680, or Regulation 2016/679 or the national competent authorities supervising the activities of the law enforcement, immigration or asylum authorities putting into service or using those systems.|84428023b7824ce2b7b94c05f0d1b5f3|||
union institutions, agencies and bodies|this regulation|Where Union institutions, agencies and bodies fall within the scope of this Regulation, the European Data Protection Supervisor shall act as their market surveillance authority.|84428023b7824ce2b7b94c05f0d1b5f3|||
market surveillance authority|member states|The European Data Protection Supervisor shall act as their market surveillance authority, indicating a regulatory relationship between the two entities.|13fb4cc3fefd4e8cac0fee0536c59464|||
market surveillance authorities designated under this regulation|other relevant national authorities or bodies which supervise the application of union harmonisation legislation listed in annex ii or other union legislation that might be relevant for the high-risk ai systems referred to in annex iii.|The context suggests a coordination and facilitation relationship between these entities.|13fb4cc3fefd4e8cac0fee0536c59464|||
market surveillance authorities|national public authorities or bodies which supervise or enforce the respect of obligations under union law protecting fundamental rights in relation to the use of high-risk ai systems referred to in annex iii.|The context suggests a request and access relationship between these entities.|13fb4cc3fefd4e8cac0fee0536c59464|||
market surveillance authorities|training, validation and testing datasets used by the provider|Full access to training, validation and testing datasets is granted to market surveillance authorities through application programming interfaces (`API') or other appropriate technical means and tools enabling remote access.|13fb4cc3fefd4e8cac0fee0536c59464|||
market surveillance authorities|source code of the ai system|Where necessary to assess the conformity of the high-risk AI system with the requirements set out in Title III, Chapter 2, market surveillance authorities shall be granted access to the source code of the AI system upon a reasoned request.|13fb4cc3fefd4e8cac0fee0536c59464|||
high-risk ai systems|documentation created or maintained under this regulation|necessary for the fulfilment of competences under their mandate within the limits of their jurisdiction|eb6672649a55475f880bbb00454dcb82|||
relevant public authority or body|member state concerned|shall inform the market surveillance authority of any such request|eb6672649a55475f880bbb00454dcb82|||
each member state|publicly available on the website of the national supervisory authority|by 3 months after the entering into force of this Regulation|eb6672649a55475f880bbb00454dcb82|||
documentation referred to in paragraph 3|insufficient to ascertain whether a breach of obligations under union law intended to protect fundamental rights has occurred|the public authority or body referred to paragraph 3 may make a reasoned request|eb6672649a55475f880bbb00454dcb82|||
market surveillance authority|organise testing of the high-risk ai system through technical means|the close involvement of the requesting public authority or body|eb6672649a55475f880bbb00454dcb82|||
ai systems presenting a risk|risks to the health or safety or to the protection of fundamental rights of persons|As referred to in paragraph 1, AI systems presenting a risk defined in Article 3, point 19 of Regulation (EU) 2019/1020 are concerned with risks to the health or safety or to the protection of fundamental rights of persons.|7f09871ea0194164a139981686f35332|||
market surveillance authority|relevant national public authorities or bodies referred to in article 64(3)|When risks to the protection of fundamental rights are present, the market surveillance authority shall inform the relevant national public authorities or bodies referred to in Article 64(3) as necessary with cooperation from relevant operators.|7f09871ea0194164a139981686f35332|||
market surveillance authorities|other national public authorities or bodies referred to in article 64(3)|referenced in the context|a0a88466513142829cbfbe17e76fbaab|||
ai system|operator of an ai system|owned and operated by operator as mentioned in the context|a0a88466513142829cbfbe17e76fbaab|||
market surveillance authority|relevant notified body|informs relevant notified body according to Article 18 of Regulation (EU) 2019/1020|a0a88466513142829cbfbe17e76fbaab|||
market surveillance authority|commission and the other member states|informs in case of non-compliance spanning beyond national territory|a0a88466513142829cbfbe17e76fbaab|||
operator|market surveillance authority|required to take corrective actions for non-compliance by market surveillance authority|a0a88466513142829cbfbe17e76fbaab|||
operator|relevant notified body|notifies relevant notified body of corrective actions required by market surveillance authority|a0a88466513142829cbfbe17e76fbaab|||
period referred to in paragraph 2|ai system's being made available on its national market|The period referred to in paragraph 2 refers to a specific time frame during which the AI system is being sold or offered for sale within a particular country.|f0006aadcc614bf9a077bf00f44ee31f|||
market surveillance authority|ai system's being made available on its national market|The market surveillance authority has the responsibility to take appropriate provisional measures, such as prohibiting or restricting the AI system's sale or withdrawal from the national market, if it fails to meet the requirements set out in Title III, Chapter 2.|f0006aadcc614bf9a077bf00f44ee31f|||
commission|market surveillance authority|The market surveillance authority shall inform the Commission of any measures taken to prohibit or restrict the sale of non-compliant AI systems within their national market.|f0006aadcc614bf9a077bf00f44ee31f|||
other member states|market surveillance authority|The market surveillance authority shall also inform other Member States of any measures taken to prohibit or restrict the sale of non-compliant AI systems within their national market.|f0006aadcc614bf9a077bf00f44ee31f|||
non-compliant ai system|ai system's being made available on its national market|If an AI system does not meet the requirements set out in Title III, Chapter 2 or falls short of the harmonised standards or common specifications referred to in Articles 40 and 41 conferring a presumption of conformity, it becomes a non-compliant AI system and may be subject to measures taken by the market surveillance authority.|f0006aadcc614bf9a077bf00f44ee31f|||
shortcomings|non-compliant ai system|The shortcomings that result in a non-compliant AI system may be due to one or more of the following: failure to meet requirements set out in Title III, Chapter 2 or shortcomings in the harmonised standards or common specifications referred to in Articles 40 and 41 conferring a presumption of conformity.|f0006aadcc614bf9a077bf00f44ee31f|||
union safeguard procedure|article 66|Article 66 is a part of the Union safeguard procedure.|cee9fd85122346eb8dba4161ccf52f86|||
article 65(5)|notification|The notification referred to in Article 65(5) is required for the Union safeguard procedure.|cee9fd85122346eb8dba4161ccf52f86|||
national measure|measure taken by another member state|A national measure is a measure taken by another Member State in accordance with Article 66.|cee9fd85122346eb8dba4161ccf52f86|||
commission|consultation|The Commission enters into consultation with the relevant Member State and operator or operators as per Article 66.|cee9fd85122346eb8dba4161ccf52f86|||
objections|member state|Objections against a measure taken by another Member State can be raised by a Member State under Article 66.|cee9fd85122346eb8dba4161ccf52f86|||
contrary to union law|commission|The Commission considers the measure to be contrary to Union law as per Article 66.|cee9fd85122346eb8dba4161ccf52f86|||
ai system|risk to health or safety of persons|Where, having performed an evaluation under Article 65, the market surveillance authority of a Member State finds that although an AI system is in compliance with this Regulation, it presents a risk to the health or safety of persons, to the compliance with obligations under Union or national law intended to protect fundamental rights or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that the AI system concerned, when placed on the market, does not pose such risks.|444db3946cfa4b3fa29641db48523934|||
ai system|compliant with this regulation|The national measure is considered justified and the non-compliance of the AI system is attributed to shortcomings in the harmonised standards or common specifications referred to in Articles 40 and 41 of this Regulation, the Commission shall apply the procedure provided for in Article 11 of Regulation (EU) No 1025/2012.|444db3946cfa4b3fa29641db48523934|||
ai system|withdrawn from market|If the national measure is considered justified, all Member States shall take the measures necessary to ensure that the non-compliant AI system is withdrawn from their market, and shall inform the Commission accordingly.|444db3946cfa4b3fa29641db48523934|||
national measure|unjustified|If the national measure is considered unjustified, the Member State concerned shall withdraw the measure.|444db3946cfa4b3fa29641db48523934|||
decision|9 months from notification|The Commission shall decide whether the national measure is justified or not within 9 months from the notification referred to in Article 65(5) and notify such decision to the Member State concerned.|444db3946cfa4b3fa29641db48523934|||
ai system|risk|When an AI system presents a risk, as per Article 53(1), the Member State referred to in paragraph 1 has the power to take all appropriate measures to ensure that the AI system concerned no longer presents that risk. The nature of the risk involved and the national measures taken should be immediately informed to the Commission and other Member States (Article 53(3)). If necessary, the Commission will propose appropriate measures after evaluating the national measures taken through consultation with the Member States and relevant operator (Article 53(4)).|4ad48abb1a2f4844b568d3c740558953|||
ai system|corrective action|The provider or other relevant operators shall ensure that corrective action is taken in respect of all the AI systems concerned that they have made available on the market throughout the Union (Article 53(2)).|4ad48abb1a2f4844b568d3c740558953|||
ai system|market|When placed on the market or put into service, an AI system should not present any risk. If it presents a risk, it should be withdrawn from the market or recalled within a reasonable period commensurate with the nature of the risk (Article 53(1)).|4ad48abb1a2f4844b568d3c740558953|||
ai system|origin|When informing about national measures taken, all available details including data necessary for the identification of the AI system concerned and the origin and supply chain of the AI system should be provided (Article 53(3)).|4ad48abb1a2f4844b568d3c740558953|||
formal non-compliance|require the relevant provider to put an end to the non-compliance concerned|In instances of formal non-compliance, the market surveillance authority requires the relevant provider to rectify the issue.|ad4db888c2b6496580565c314c7453db|||
formal non-compliance|restrict or prohibit the high-risk ai being made available on the market or ensure that it is recalled or withdrawn from the market|If formal non-compliance persists, member states may take stringent measures to mitigate risks associated with high-risk AI.|ad4db888c2b6496580565c314c7453db|||
codes of conduct|title iii, chapter 2 requirements on ai systems|Codes of conduct can be used to promote voluntary compliance with specific requirements related to AI systems other than high-risk ones.|ad4db888c2b6496580565c314c7453db|||
codes of conduct|appropriate means of fulfilling the requirements|The use of technical specifications and solutions can help achieve compliance with Title III, Chapter 2 requirements through codes of conduct for AI systems.|ad4db888c2b6496580565c314c7453db|||
ai systems|codes of conduct|AI systems can be covered by codes of conduct drawn up by individual providers, organizations representing them, or both, including with the involvement of users and any interested stakeholders and their representative organizations.|b6dd60a255b647e9b83a1def3234f5d3|||
small-scale providers and start-ups|confidentiality|The Commission and the Board shall take into account the specific interests and needs of small-scale providers and start-ups when encouraging and facilitating the drawing up of codes of conduct. This is because confidentiality is important for these types of organizations.|b6dd60a255b647e9b83a1def3234f5d3|||
codes of conduct|environmental sustainability|Codes of conduct can foster the voluntary application to AI systems of requirements related to environmental sustainability, as encouraged and facilitated by the Commission and the Board.|b6dd60a255b647e9b83a1def3234f5d3|||
codes of conduct|accessibility for persons with a disability|Codes of conduct can foster the voluntary application to AI systems of requirements related to accessibility for persons with a disability, as encouraged and facilitated by the Commission and the Board.|b6dd60a255b647e9b83a1def3234f5d3|||
codes of conduct|stakeholders participation in the design and development of ai systems|Codes of conduct can foster the voluntary application to AI systems of requirements related to stakeholders participation in the design and development of AI systems, as encouraged and facilitated by the Commission and the Board.|b6dd60a255b647e9b83a1def3234f5d3|||
codes of conduct|diversity of development teams|Codes of conduct can foster the voluntary application to AI systems of requirements related to diversity of development teams, as encouraged and facilitated by the Commission and the Board.|b6dd60a255b647e9b83a1def3234f5d3|||
codes of conduct|objectives and key performance indicators|Codes of conduct may include clear objectives and key performance indicators to measure the achievement of those objectives, as encouraged and facilitated by the Commission and the Board.|b6dd60a255b647e9b83a1def3234f5d3|||
technical specifications|compliance with requirements|The basis of technical specifications is to ensure compliance with such requirements, as mentioned in the context.|b6dd60a255b647e9b83a1def3234f5d3|||
information and data obtained in carrying out their tasks and activities|intellectual property rights, and confidential business information or trade secrets of a natural or legal person, including source code|should be protected to maintain confidentiality.|ef222180a8f24575961f355dc97b7326|||
information and data obtained in carrying out their tasks and activities|effective implementation of this regulation|should be protected to facilitate inspections, investigations or audits.|ef222180a8f24575961f355dc97b7326|||
information exchanged on a confidential basis between national competent authorities and between national competent authorities and the commission |public and national security interests|should be protected to ensure confidentiality without jeopardising public and national security.|ef222180a8f24575961f355dc97b7326|||
information exchanged on a confidential basis between national competent authorities and between national competent authorities and the commission |high-risk ai systems referred to in points 1, 6 and 7 of annex iii|should be protected when used by law enforcement, immigration or asylum authorities without jeopardising public and national security interests.|ef222180a8f24575961f355dc97b7326|||
immigration or asylum authorities|law enforcement|When disclosure of information would jeopardize public and national security interests, both immigration or asylum authorities and law enforcement authorities are providers of high-risk AI systems referred to in points 1, 6, and 7 of Annex III. This technical documentation referred to in Annex IV shall remain within the premises of these authorities, but upon request, market surveillance authorities can immediately access this documentation or obtain a copy.|7505f83b539e421caddd1750a9d00276|||
immigration or asylum authorities|market surveillance authorities|When immigration or asylum authorities are providers of high-risk AI systems referred to in points 1, 6, and 7 of Annex III, market surveillance authorities can immediately access the technical documentation referred to in Annex IV or obtain a copy upon request. This is subject to the condition that only staff holding the appropriate level of security clearance shall be allowed to access this documentation.|7505f83b539e421caddd1750a9d00276|||
commission|member states|The rights and obligations of the Commission, Member States, and notified bodies with regard to the exchange of information and dissemination of warnings under criminal law of the Member States are unaffected by paragraphs 1 and 2.|7505f83b539e421caddd1750a9d00276|notified bodies||
commission|member states|The Commission and Member States may exchange confidential information with regulatory authorities of third countries that have concluded bilateral or multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality.|7505f83b539e421caddd1750a9d00276|||
non-compliance with the prohibition of the artificial intelligence practices referred to in article 5|administrative fines of up to 30 000 000 eur or, if the offender is a company, up to 6 % of its total worldwide annual turnover for the preceding financial year, whichever is higher|Infringement and penalty for non-compliance with the prohibition of the artificial intelligence practices referred to in Article 5|bf3648d7f37d48ff8c9f32349e9f86f5|||
non-compliance of the ai system with the requirements laid down in article 10|administrative fines of up to 30 000 000 eur or, if the offender is a company, up to|Infringement and penalty for non-compliance of the AI system with the requirements laid down in Article 10|bf3648d7f37d48ff8c9f32349e9f86f5|||
non-compliance of the ai system with any requirements or obligations under this regulation, other than those laid down in articles 5 and 10|administrative fines of up to 20 000 000 eur|Infringement and penalty for non-compliance of the AI system with any requirements or obligations under this Regulation, other than those laid down in Articles 5 and 10|bf3648d7f37d48ff8c9f32349e9f86f5|||
small-scale providers|start-up and their economic viability|Small-scale providers and start-ups' interests considered while setting penalties|bf3648d7f37d48ff8c9f32349e9f86f5|||
administrative fines|up to 20 000 000 eur|when a fine is imposed for an administrative infringement, it cannot exceed 20 million euros in accordance with the regulation|d23b12b1f5ee49619e5f8fe5b9039a16|||
administrative fines|up to 4 % of its total worldwide annual turnover|if the offender is a company, the administrative fine for an infringement cannot exceed 4% of its total worldwide annual turnover in the preceding financial year|d23b12b1f5ee49619e5f8fe5b9039a16|||
supply of incorrect information|administrative fines|the supply of incorrect, incomplete or misleading information to notified bodies and national competent authorities in reply to a request is subject to administrative fines of up to 10 million euros for individuals and up to 2% of its total worldwide annual turnover for the preceding financial year for companies|d23b12b1f5ee49619e5f8fe5b9039a16|||
nature, gravity and duration|administrative fines|when deciding on the amount of the administrative fine in each individual case, all relevant circumstances of the specific situation shall be taken into account, including the nature, gravity and duration of the infringement and of its consequences|d23b12b1f5ee49619e5f8fe5b9039a16|||
administrative fines|already applied by other market surveillance authorities|the rules on administrative fines may take into account whether administrative fines have been already applied by other market surveillance authorities to the same operator for the same infringement|d23b12b1f5ee49619e5f8fe5b9039a16|||
size and market share|administrative fines|the rules on administrative fines may also take into account the size and market share of the operator committing the infringement|d23b12b1f5ee49619e5f8fe5b9039a16|||
rules on administrative fines|each member state|each Member State shall lay down rules on whether and to what extent administrative fines may be imposed on public authorities and bodies established in that Member State|d23b12b1f5ee49619e5f8fe5b9039a16|||
rules on administrative fines|depending on the legal system of the member states|the rules on administrative fines may be applied in such a manner that the fines are imposed by competent national courts or bodies as applicable in those Member States|d23b12b1f5ee49619e5f8fe5b9039a16|||
non-compliance with the prohibition of the artificial intelligence practices referred to in article 5|non-compliance of the ai system with the requirements laid down in article 10|Both types of non-compliance fall under the scope of administrative fines imposed by this Regulation, which range from 0 to 250 000 EUR according to their severity.|d9062e2403234b8ea6ffa6ca707735f6|||
european data protection supervisor|union institution, agency or body which is the subject of the proceedings conducted by the european data protection supervisor|The European Data Protection Supervisor shall give the latter the opportunity to be heard on the matter regarding the possible infringement before taking decisions pursuant to this Article.|d9062e2403234b8ea6ffa6ca707735f6|||
european data protection supervisor|complainants, if any|The European Data Protection Supervisor shall associate the latter closely with the proceedings.|d9062e2403234b8ea6ffa6ca707735f6|||
parties concerned|rights of defense|The rights of defense of the parties concerned shall be fully respected in the proceedings.|d9062e2403234b8ea6ffa6ca707735f6|||
european data protection supervisor's file|legitimate interest of individuals or undertakings in the protection of their personal data or business secrets|The parties concerned shall be entitled to have access to the European Data Protection Supervisor's file subject to the legitimate interest of individuals or undertakings in the protection of their personal data or business secrets.|d9062e2403234b8ea6ffa6ca707735f6|||
funds collected by imposition of fines|general budget of the union|Funds collected by imposition of fines in this Article shall be the income of the general budget of the Union.|d9062e2403234b8ea6ffa6ca707735f6|||
article 73|commission|The power to adopt delegated acts is conferred on the Commission subject to the conditions laid down in this Article (Article 73).|eaf55c71f2f34b2695b4d728dc709e78|||
delegation of power|commission|The delegation of power referred to in Article 4, Article 7(1), Article 11(3), Article 43(5) and (6) and Article 48(5) shall be conferred on the Commission for an indeterminate period of time from [entering into force of the Regulation] (Article 73).|eaf55c71f2f34b2695b4d728dc709e78|||
european parliament|delegation of power|The delegation of power referred to in Article 4, Article 7(1), Article 11(3), Article 43(5) and (6) and Article 48(5) may be revoked at any time by the European Parliament or by the Council (Article 73).|eaf55c71f2f34b2695b4d728dc709e78|||
council|delegation of power|The delegation of power referred to in Article 4, Article 7(1), Article 11(3), Article 43(5) and (6) and Article 48(5) may be revoked at any time by the European Parliament or by the Council (Article 73).|eaf55c71f2f34b2695b4d728dc709e78|||
delegated act|commission|As soon as it adopts a delegated act, the Commission shall notify it simultaneously to the European Parliament and to the Council (Article 73).|eaf55c71f2f34b2695b4d728dc709e78|||
european parliament|delegated act|Any objection raised by the European Parliament regarding a delegated act adopted pursuant to Article 4, Article 7(1), Article 11(3), Article 43(5) and (6) and Article 48(5) shall result in the revocation of the said delegated act (Article 73).|eaf55c71f2f34b2695b4d728dc709e78|||
council|delegated act|Any objection raised by the Council regarding a delegated act adopted pursuant to Article 4, Article 7(1), Article 11(3), Article 43(5) and (6) and Article 48(5) shall result in the revocation of the said delegated act (Article 73).|eaf55c71f2f34b2695b4d728dc709e78|||
delegated act|european parliament|If no objection is raised by the European Parliament regarding a delegated act adopted pursuant to Article 4, Article 7(1), Article 11(3), Article 43(5) and (6) and Article 48(5), it shall enter into force (Article 73).|eaf55c71f2f34b2695b4d728dc709e78|||
delegated act|council|If no objection is raised by the Council regarding a delegated act adopted pursuant to Article 4, Article 7(1), Article 11(3), Article 43(5) and (6) and Article 48(5), it shall enter into force (Article 73).|eaf55c71f2f34b2695b4d728dc709e78|||
commission|general budget of the union.|The text is related to the general budget of the Union.|eaf55c71f2f34b2695b4d728dc709e78|||
article 48(5)|european parliament|expressed objection to within a period of three months of notification of that act|6090d8de76ab4ab7a995f6343d65d31a|||
article 48(5)|council|expressed objection to within a period of three months of notification of that act|6090d8de76ab4ab7a995f6343d65d31a|||
commission|committee procedure|assisted by a committee in accordance with Regulation (EU) No 182/2011|6090d8de76ab4ab7a995f6343d65d31a|||
regulation (ec) no 300/2008|article 4(3)|amended in accordance with Article 75 to take into account requirements set out in Chapter 2, Title III of Regulation (EU) YYY/XX on Artificial Intelligence|6090d8de76ab4ab7a995f6343d65d31a|||
regulation (eu) no 167/2013|||6090d8de76ab4ab7a995f6343d65d31a|||
regulation (eu) yyy/xx [on artificial intelligence]|delegated acts pursuant to the first subparagraph concerning artificial intelligence systems|In Article 76 and 77 of Regulation (EU) No 167/2013 and 168/2013, respectively, the requirements set out in Title III, Chapter 2 of Regulation (EU) YYY/XX [on Artificial Intelligence] are taken into account.|86f45d9c8b374f5cb907ef02ca5bf64d|||
regulation (eu) yyy/xx [on artificial intelligence]|artificial intelligence systems which are safety components in the meaning of regulation (eu) yyy/xx [on artificial intelligence] of the european parliament and of the council|When adopting delegated acts pursuant to the first subparagraph concerning these systems, the requirements set out in Title III, Chapter 2 of Regulation (EU) YYY/XX [on Artificial Intelligence] are taken into account.|86f45d9c8b374f5cb907ef02ca5bf64d|||
directive 2014/90/eu|regulation (eu) yyy/xx [on artificial intelligence]|In Article 8 of Directive 2014/90/EU, the requirements set out in Regulation (EU) YYY/XX [on Artificial Intelligence] are taken into account.|86f45d9c8b374f5cb907ef02ca5bf64d|||
directive 2014/90/eu|article 8|contains a new paragraph regarding Artificial Intelligence systems as safety components in Regulation (EU) YYY/XX|358c53fe7a0b4913a3a4cc0339d8e2da|||
directive (eu) 2016/797|article 5|contains a new paragraph regarding Artificial Intelligence systems as safety components in Regulation (EU) YYY/XX, which should be taken into account when adopting delegated and implementing acts|358c53fe7a0b4913a3a4cc0339d8e2da|||
regulation (eu) 2018/858|article 5|contains a new paragraph regarding Artificial Intelligence systems as safety components in Regulation (EU) YYY/XX, which should be taken into account|358c53fe7a0b4913a3a4cc0339d8e2da|||
directive 2014/90/eu|commission|when carrying out activities and adopting technical specifications and testing standards in accordance with paragraphs 2 and 3, should take into account the requirements set out in Title III, Chapter 2 of Regulation (EU) YYY/XX when dealing with Artificial Intelligence systems as safety components|358c53fe7a0b4913a3a4cc0339d8e2da|||
directive (eu) 2016/797|commission|when adopting delegated acts regarding Artificial Intelligence systems as safety components in the meaning of Regulation (EU) YYY/XX, should take into account the requirements set out in Title III, Chapter 2 of that Regulation|358c53fe7a0b4913a3a4cc0339d8e2da|||
regulation (eu) 2018/858|commission|when adopting delegated and implementing acts regarding Artificial Intelligence systems as safety components in the meaning of Regulation (EU) YYY/XX, should take into account the requirements set out in Title III, Chapter 2 of that Regulation|358c53fe7a0b4913a3a4cc0339d8e2da|||
regulation (eu) yyy/xx [on artificial intelligence]|article 78|referred to in the new paragraph added to Directive 2014/90/EU regarding Artificial Intelligence systems as safety components|358c53fe7a0b4913a3a4cc0339d8e2da|||
regulation (eu) yyy/xx [on artificial intelligence]|commission|when carrying out activities and adopting technical specifications and testing standards in accordance with paragraphs 2 and 3, the requirements set out in Title III, Chapter 2 of that Regulation should be taken into account regarding Artificial Intelligence systems as safety components|358c53fe7a0b4913a3a4cc0339d8e2da|||
regulation (eu) yyy/xx [on artificial intelligence]|commission|when adopting delegated and implementing acts regarding Artificial Intelligence systems as safety components, the requirements set out in Title III, Chapter 2 of that Regulation should be taken into account|358c53fe7a0b4913a3a4cc0339d8e2da|||
directive (eu) 2016/797|regulation (eu) yyy/xx [on artificial intelligence]|referred to in the new paragraph added to Article 5 of Directive (EU) 2016/797 regarding Artificial Intelligence systems as safety components|358c53fe7a0b4913a3a4cc0339d8e2da|||
article 80|regulation (eu) yyy/xx [on artificial intelligence]|The amendment made to Article 80 in this context refers to Regulation (EU) YYY/XX [on Artificial Intelligence] for safety components when adopting delegated acts pursuant to paragraph 3 concerning Artificial Intelligence systems.|4712128e1de94a00939be5ac6ae0ed06|||
article 5|regulation (eu) yyy/xx [on artificial intelligence]|The requirements set out in Title III, Chapter 2 of Regulation (EU) YYY/XX [on Artificial Intelligence] shall be taken into account when adopting delegated acts pursuant to paragraph 3 concerning Artificial Intelligence systems as specified in Article 5.|4712128e1de94a00939be5ac6ae0ed06|||
article 81|regulation (eu) yyy/xx [on artificial intelligence]|The amendment made to Article 81 in this context refers to Regulation (EU) YYY/XX [on Artificial Intelligence] for safety components when adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems.|4712128e1de94a00939be5ac6ae0ed06|||
article 17|regulation (eu) yyy/xx [on artificial intelligence]|The requirements set out in Title III, Chapter 2 of Regulation (EU) YYY/XX [on Artificial Intelligence] shall be taken into account when adopting implementing acts pursuant to paragraph 3 concerning Artificial Intelligence systems as specified in Article 17.|4712128e1de94a00939be5ac6ae0ed06|||
article 19|regulation (eu) yyy/xx [on artificial intelligence]|The requirements set out in Title III, Chapter 2 of Regulation (EU) YYY/XX [on Artificial Intelligence] shall be taken into account when adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems as specified in Article 19.|4712128e1de94a00939be5ac6ae0ed06|||
regulation (eu) yyyy/xx [on artificial intelligence]|artificial intelligence systems which are safety components|When adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.|0e09255116504e3d904ad2a3a008b542|||
artificial intelligence systems which are safety components|requirements set out in title iii, chapter 2 of that regulation|When adopting implementing acts pursuant to paragraph 1 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.|0e09255116504e3d904ad2a3a008b542|||
artificial intelligence systems which are safety components|delegated acts pursuant to paragraphs 1 and 2 concerning artificial intelligence systems which are safety components in the meaning of regulation (eu) yyyy/xx [on artificial intelligence]|When adopting delegated acts pursuant to paragraphs 1 and 2 concerning Artificial Intelligence systems which are safety components in the meaning of Regulation (EU) YYYY/XX [on Artificial Intelligence], the requirements set out in Title III, Chapter 2 of that Regulation shall be taken into account.|0e09255116504e3d904ad2a3a008b542|||
article 43|implementing acts pursuant to paragraph 1 concerning artificial intelligence systems which are safety components in the meaning of regulation (eu) yyyy/xx [on artificial intelligence]|In Article 43, the following paragraph is added:|0e09255116504e3d904ad2a3a008b542|||
article 47|delegated acts pursuant to paragraphs 1 and 2 concerning artificial intelligence systems which are safety components in the meaning of regulation (eu) yyyy/xx [on artificial intelligence]|In Article 47, the following paragraph is added:|0e09255116504e3d904ad2a3a008b542|||
article 57|implementing acts concerning artificial intelligence systems which are safety components in the meaning of regulation (eu) yyyy/xx [on artificial intelligence]|In Article 57, the following paragraph is added:|0e09255116504e3d904ad2a3a008b542|||
article 58|delegated acts pursuant to paragraphs 1 and 2 concerning artificial intelligence systems which are safety components in the meaning of regulation (eu) yyyy/xx [on artificial intelligence]|In Article 58, the following paragraph is added:|0e09255116504e3d904ad2a3a008b542|||
high-risk ai systems|large-scale it systems|Applicable in the evaluation of each large-scale IT systems established by the legal acts listed in Annex IX to be undertaken as provided for in those respective acts. The requirements laid down in this Regulation shall be taken into account, where applicable.|0883e505b2134df9a07d8373811c36c5|||
high-risk ai systems|significant changes in their design or intended purpose|From the date of application of this Regulation referred to in Article 85(2), only if, from that date, those systems are subject to significant changes in their design or intended purpose.|0883e505b2134df9a07d8373811c36c5|||
evaluation and review|commission|The Commission shall assess the need for amendment of the list in Annex III once a year following the entry into force of this Regulation.|0883e505b2134df9a07d8373811c36c5|||
evaluation and review|commission|By [three years after the date of application of this Regulation referred to in Article 85(2)] and every four years thereafter, the Commission shall submit a report on the evaluation and review of this Regulation to the European Parliament and to the Council.|0883e505b2134df9a07d8373811c36c5|||
evaluation and review|reports|The reports referred to in paragraph 2 shall be made public.|0883e505b2134df9a07d8373811c36c5|||
evaluation and review|reports|The reports referred to in paragraph 2 shall devote specific attention to the following:|0883e505b2134df9a07d8373811c36c5|||
evaluation and review|(a)|the status of the financial and human resources of the national competent authorities in order to effectively perform the tasks assigned to them under this Regulation;|0883e505b2134df9a07d8373811c36c5|||
information society|european parliament|The article mentions the concept of 'information society' and its progress in relation to the European Parliament, indicating a potential legislative action related to this topic.|8896114f0cf34ec88ecc61557374c841|||
european parliament|council|Both the European Parliament and Council are mentioned as entities involved in the legislation process.|8896114f0cf34ec88ecc61557374c841|||
proposal/initiative|policy area(s) concerned|The proposal/initiative is directly related to a specific policy area(s).|8896114f0cf34ec88ecc61557374c841|||
proposal/initiative|specific objective(s)|The proposal/initiative has one or more specific objectives.|8896114f0cf34ec88ecc61557374c841|||
expected result(s) and impact|indicators of performance|To achieve its objective(s), the proposal/initiative must deliver expected results and impacts, which can be measured using specific indicators.|8896114f0cf34ec88ecc61557374c841|||
proposal/initiative|requirement(s) to be met in the short or long term|The proposal/initiative must meet certain requirements in a specified timeframe, as outlined in this section.|8896114f0cf34ec88ecc61557374c841|||
regulation of the european parliament and of the council laying down harmonised rules on artificial intelligence (artificial intelligence act)|communications networks, content and technology|The policy area Communications Networks, Content and Technology is concerned with the Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act)|9b5cb7aca8ac4b6bbb262de504650f00|||
regulation of the european parliament and of the council laying down harmonised rules on artificial intelligence (artificial intelligence act)|internal market, industry, entrepreneurship and smes|The policy area Internal Market, Industry, Entrepreneurship and SMEs is concerned with the Regulation of the European Parliament and of the Council Laying Down Harmonised Rules on Artificial Intelligence (Artificial Intelligence Act)|9b5cb7aca8ac4b6bbb262de504650f00|||
estimated financial impact of the proposal on appropriations|line(s) affected|The estimated financial impact of the proposal on appropriations is related to Line(s) affected|9b5cb7aca8ac4b6bbb262de504650f00|||
heading(s) of the multiannual financial framework and expenditure budget|line(s) affected|The Heading(s) of the multiannual financial framework and expenditure budget are related to Line(s) affected|9b5cb7aca8ac4b6bbb262de504650f00|||
estimated financial impact of the proposal on appropriations|third-party contributions|The estimated financial impact of the proposal on appropriations is related to Third-party contributions|9b5cb7aca8ac4b6bbb262de504650f00|||
legislative financial statement|title of the proposal/initiative|The Legislative Financial Statement is related to the Title of the proposal/initiative|9b5cb7aca8ac4b6bbb262de504650f00|||
internal market, industry, entrepreneurship and smes|europe's digital future|The activity 'Shaping Europe's digital future' is related to the policy area of Internal Market, Industry, Entrepreneurship and SMEs.|499748ee22ca473e86afbd746c8c0750|||
ai board|commission|The proposal includes a new task for the Commission, which is supporting the EU AI Board.|499748ee22ca473e86afbd746c8c0750|||
single market|union market|The objective of ensuring legal certainty to facilitate investment and innovation in AI by making it clear what essential requirements, obligations, as well as conformity and compliance procedures must be followed is related to both the 'single market' and the 'Union market'.|499748ee22ca473e86afbd746c8c0750|||
ai systems|existing law on fundamental rights and union values|The objective of ensuring that AI systems placed on the market and used are safe and respect existing law on fundamental rights and Union values is related to 'AI systems' and 'existing law on fundamental rights and Union values'.|499748ee22ca473e86afbd746c8c0750|||
ai systems|union market|The objectives of setting requirements specific to AI systems and obligations on all value chain participants, as well as ensuring legal certainty to facilitate investment and innovation in AI by making it clear what essential requirements, obligations, as well as conformity and compliance procedures must be followed are both related to 'AI systems' and 'Union market'.|499748ee22ca473e86afbd746c8c0750|||
ai suppliers|legal certainty|By providing a minimal but clear set of requirements, AI suppliers will benefit from legal certainty and ensure access to the entire single market.|c347eab82c1d4b9faab1f75ae7d3f9f6|||
ai users|legal certainty|AI users will benefit from legal certainty that the high-risk AI systems they buy comply with European laws and values.|c347eab82c1d4b9faab1f75ae7d3f9f6|||
consumers|reducing risk of violations|Consumers will benefit by reducing the risk of violations of their safety or fundamental rights.|c347eab82c1d4b9faab1f75ae7d3f9f6|||
ai suppliers|access to single market|Creating legal certainty for AI suppliers ensures access to the entire single market.|c347eab82c1d4b9faab1f75ae7d3f9f6|||
consumers|serious incidents or ai performances which constitute a serious incident or a breach of fundamental rights obligations|Reduction in risk of violations of safety or fundamental rights for consumers|058be7ae11f64f9d9bc1c0c6b905e11f|||
indicator 1|serious incidents or ai performances which constitute a serious incident or a breach of fundamental rights obligations|Measures the number of such incidents/performances by fields of applications, as absolute numbers, shares of applications deployed and shares of citizens concerned (semi-annual)|058be7ae11f64f9d9bc1c0c6b905e11f|||
indicator 2|total ai investment in the eu|Relates to indicator for monitoring implementation of proposal/initiative, calculates total AI investment in the EU annually|058be7ae11f64f9d9bc1c0c6b905e11f|||
indicator 2|total ai investment by member state|Relates to indicator for monitoring implementation of proposal/initiative, calculates total AI investment by Member State annually|058be7ae11f64f9d9bc1c0c6b905e11f|||
indicator 2|share of companies using ai|Relates to indicator for monitoring implementation of proposal/initiative, calculated annually through regular company surveys|058be7ae11f64f9d9bc1c0c6b905e11f|||
indicator 2|share of smes using ai|Relates to indicator for monitoring implementation of proposal/initiative, calculated annually through regular company surveys|058be7ae11f64f9d9bc1c0c6b905e11f|||
1.5.1|requirement(s) to be met in the short or long term including a detailed timeline for roll-out of implementation|Describes grounds for proposal/initiative, includes requirement(s) and detailed timeline for roll-out|058be7ae11f64f9d9bc1c0c6b905e11f|||
1.5.1|member states shall have appointed existing authorities and/or established new authorities performing the tasks set out in the legislation earlier|Describes grounds for proposal/initiative, specifies actions to be taken by Member States|058be7ae11f64f9d9bc1c0c6b905e11f|||
1.5.1|the eu ai board should be set-up and effective|Describes grounds for proposal/initiative, specifies actions to be taken by the European Commission|058be7ae11f64f9d9bc1c0c6b905e11f|||
board|european database of ai systems|By the time of applicability, Board should ensure that European database of AI systems is fully operative.|5aed5c0be6644d05bc6bd985a8a11d33|||
european database of ai systems|regulation enters into force|Development of the European database of AI systems should come to an end when the regulation enters into force.|5aed5c0be6644d05bc6bd985a8a11d33|||
union involvement|added value of union involvement|It may result from different factors, e.g. Coordination gains, legal certainty, greater effectiveness or complementarities.|5aed5c0be6644d05bc6bd985a8a11d33|||
seamless provision of ai systems|emerging patchy framework of potentially divergent national rules|An emerging patchy framework of potentially divergent national rules will hamper the seamless provision of AI systems across the EU and is ineffective in ensuring the safety and protection of fundamental rights and Union values across the different Member States.|5aed5c0be6644d05bc6bd985a8a11d33|||
common eu legislative action|european industry|A common EU legislative action on AI could boost the internal market and has great potential to provide European industry with a competitive edge at the global scene and economies of scale that cannot be achieved by individual Member States alone.|5aed5c0be6644d05bc6bd985a8a11d33|||
e-commerce directive 2000/31/ec|similar experiences in the past|The E-commerce Directive 2000/31/EC provides the core framework for the|5aed5c0be6644d05bc6bd985a8a11d33|||
e-commerce directive 2000/31/ec|cooperation mechanism among member states for digital services|The E-commerce Directive 2000/31/EC provides a core framework for the functioning of the single market and sets a basic structure for general cooperation mechanism among Member States for digital services, covering in principle all requirements applicable to digital services. This mechanism has shortcomings in procedural aspects such as lack of clear timeframes for response from Member States and lack of responsiveness to requests from their counterparts, leading to a lack of trust between Member States in addressing concerns about providers offering digital services cross-border.|812bea03e32d4f3e93ee88d5c2753f2b|||
e-commerce directive 2000/31/ec|evaluation of directive|The evaluation of the E-commerce Directive 2000/31/EC pointed to shortcomings in several aspects of the cooperation mechanism among Member States for digital services, including important procedural aspects such as lack of clear timeframes for response from Member States and a general lack of responsiveness to requests from their counterparts.|812bea03e32d4f3e93ee88d5c2753f2b|||
e-commerce directive 2000/31/ec|need for differentiated set of rules and requirements at european level|The evaluation of the E-commerce Directive 2000/31/EC showed the need to define a differentiated set of rules and requirements at European level due to shortcomings in several aspects of the cooperation mechanism among Member States for digital services.|812bea03e32d4f3e93ee88d5c2753f2b|||
implementation of specific obligations|cooperation mechanism at eu level|The implementation of the specific obligations laid down in this Regulation would require a specific cooperation mechanism at EU level, with a governance structure ensuring coordination of specific responsible bodies at EU level.|812bea03e32d4f3e93ee88d5c2753f2b|||
regulation laying down harmonised rules on artificial intelligence and amending certain union legislative acts|eu legislative acts|The Regulation Laying Down Harmonised Rules on Artificial Intelligence and Amending Certain Union Legislative Acts defines a new common framework for AI, which may have synergies with other appropriate instruments compatible with the Multiannual Financial Framework.|812bea03e32d4f3e93ee88d5c2753f2b|||
ai systems|trustworthy ai|The new common framework of requirements applicable to AI systems, as defined in this proposal, contributes directly to ensuring trustworthy AI. This proposal establishes a new national and European regulatory and coordination function due to the fact that Amending Certain Union Legislative Acts goes well beyond the framework provided by existing legislation. Therefore, a new national and European regulatory and coordination function needs to be established with this proposal.|ac40080e9aa540a09e809d943a12f00c|||
trustworthy ai|digital europe|The proposal contributes directly to one key objective of Digital Europe, which is promoting the diffusion of AI. By increasing trust in AI and thus encouraging investment in development and adoption of AI, it complements Digital Europe.|ac40080e9aa540a09e809d943a12f00c|||
ai systems|digital europe|The proposal accelerates AI development and deployment in Europe by ensuring trustworthy AI. Therefore, by increasing trust in AI and thus encouraging investment in development and adoption of AI, it complements Digital Europe.|ac40080e9aa540a09e809d943a12f00c|||
amending certain union legislative acts|existing legislation|The new common framework of requirements applicable to AI systems, as defined in this proposal, goes well beyond the framework provided by existing legislation.|ac40080e9aa540a09e809d943a12f00c|||
amending certain union legislative acts|digital europe|Promoting the diffusion of AI is one of five priorities of Digital Europe, and the proposal complements this priority by accelerating AI development and deployment in Europe.|ac40080e9aa540a09e809d943a12f00c|||
financing options|dep|The staff will be redeployed, and other costs will be supported from the DEP. The objective of this regulation ensuring trustworthy AI contributes directly to one key objective of Digital Europe accelerating AI development and deployment in Europe, and therefore, it has a financial impact on the DEP.|ac40080e9aa540a09e809d943a12f00c|||
proposal|initiative|The proposal/initiative has a limited duration from [DD/MM]YYYY to [DD/MM]YYYY, and the financial impact is from YYYY to YYYY for commitment appropriations and from YYYY to YYYY for payment appropriations.|ac40080e9aa540a09e809d943a12f00c|||
existing legislation|amending certain union legislative acts|Amending Certain Union Legislative Acts provides a new common framework of requirements applicable to AI systems, which goes well beyond the framework provided by existing legislation.|ac40080e9aa540a09e809d943a12f00c|||
existing legislation|notifying authorities|The role of notifying authorities at national level can be performed by national authorities fulfilling similar functions under other EU regulations due to the possible synergies with other appropriate instruments.|ac40080e9aa540a09e809d943a12f00c|||
direct management by the commission|management mode(s) planned|According to the context, 'Direct management by the Commission' is a specific type of management mode that falls under the category 'Management mode(s) planned'. This relationship can be seen in section 1.7.|f3a4a36577754517b2699e438a3f4212|||
departments|direct management by the commission|The context explains that 'departments' are a part of 'Direct management by the Commission'. This relationship is evident in section 1.7.|f3a4a36577754517b2699e438a3f4212|||
staff in the union delegations|direct management by the commission|As per the context, 'staff in the Union delegations' are involved in 'Direct management by the Commission'. This relationship is mentioned in section 1.7.|f3a4a36577754517b2699e438a3f4212|||
executive agencies|direct management by the commission|The context reveals that 'executive agencies' are also included under 'Direct management by the Commission'. This relationship can be observed in section 1.7.|f3a4a36577754517b2699e438a3f4212|||
third countries or bodies they have designated|indirect management by entrusting budget implementation tasks to|In the context, 'third countries or bodies they have designated' are a type of entity that 'Indirect management by entrusting budget implementation tasks to' applies to. This relationship can be seen in section 1.7.|f3a4a36577754517b2699e438a3f4212|||
international organisations and their agencies (to be specified)|indirect management by entrusting budget implementation tasks to|According to the context, 'international organisations and their agencies (to be specified)' fall under 'Indirect management by entrusting budget implementation tasks to'. This relationship is mentioned in section 1.7.|f3a4a36577754517b2699e438a3f4212|||
eib and the european investment fund|indirect management by entrusting budget implementation tasks to|As per the context, 'EIB and the European Investment Fund' come under 'Indirect management by entrusting budget implementation tasks to'. This relationship can be seen in section 1.7.|f3a4a36577754517b2699e438a3f4212|||
bodies referred to in articles 70 and 71 of the financial regulation|indirect management by entrusting budget implementation tasks to|According to the context, 'bodies referred to in Articles 70 and 71 of the Financial Regulation' are included under 'Indirect management by entrusting budget implementation tasks to'. This relationship is evident in section 1.7.|f3a4a36577754517b2699e438a3f4212|||
public law bodies|indirect management by entrusting budget implementation tasks to|As per the context, 'public law bodies' are a type of entity that 'Indirect management by entrusting budget implementation tasks to' applies to. This relationship is visible in section 1.7.|f3a4a36577754517b2699e438a3f4212|||
bodies governed by private law with a public service mission|indirect management by entrusting budget implementation tasks to|The context explains that 'bodies governed by private law with a public service mission' are covered under 'Indirect management by entrusting budget implementation tasks to'. This relationship is visible in section 1.7.|f3a4a36577754517b2699e438a3f4212|||
bodies governed by the private law of a member state|indirect management by entrusting budget implementation tasks to|According to the context, 'bodies governed by the private law of a Member State' fall under 'Indirect management by entrusting budget implementation tasks to'. This relationship can be seen in section 1.7.|f3a4a36577754517b2699e438a3f4212|||
persons entrusted with the implementation of specific actions in the cfsp|indirect management by entrusting budget implementation tasks to|As per the context, 'persons entrusted with the implementation of specific actions in the CFSP' are included under 'Indirect management by entrusting budget implementation tasks to'. This relationship is evident in section 1.7.|f3a4a36577754517b2699e438a3f4212|||
management mode(s) planned|shared management with the member states|The context shows that 'Shared management with the Member States' comes under 'Management mode(s) planned'. This relationship is visible in section 1.7.|f3a4a36577754517b2699e438a3f4212|||
budgweb site|management modes and references to financial regulation|Details of management modes and references to the Financial Regulation may be found on the BudgWeb site: http://www.cc.cec/budg/man/budgmanag/budgmanag_en.html|3a9644d740194c4987a2105b1cbfa017|||
en|evaluation of regulation|The Regulation will be reviewed and evaluated five years from the entry into force of the regulation. The Commission will report on the findings of the evaluation to the European Parliament, the Council and the European Economic and Social Committee.|3a9644d740194c4987a2105b1cbfa017|||
en|new policy with regard to harmonised rules for the provision of artificial intelligence systems in the internal market while ensuring the respect of safety and fundamental rights.|The Regulation establishes a new policy with regard to harmonised rules for the provision of artificial intelligence systems in the internal market while ensuring the respect of safety and fundamental rights.|3a9644d740194c4987a2105b1cbfa017|||
en|consistency mechanism for the cross-border application of obligations under this regulation in the form of a new advisory group coordinating the activities of national authorities.|These new rules require a consistency mechanism for the cross-border application of the obligations under this Regulation in the form of a new advisory group coordinating the activities of national authorities.|3a9644d740194c4987a2105b1cbfa017|||
en|enforcement of new regulation estimated to|In order to face these new tasks, it is necessary to appropriately resource the Commission's services. The enforcement of the new Regulation is estimated to|3a9644d740194c4987a2105b1cbfa017|||
commission's services|enforcement of the new regulation|The enforcement of the new Regulation is estimated to require 10 FTE à regime (5 FTE for the support to the activities of the Board and 5 FTE for the European Data Protection Supervisor acting as a notifying body for AI systems deployed by a body of the European Union).|a8f6bf44792a4326bead66a085e72072|||
commission's services|board|The Board should be supported by the administrative structure of the Commission.|a8f6bf44792a4326bead66a085e72072|||
commission's services|expert group|It is foreseen that the Board should be supported by the administrative structure of the Commission and that an expert group be created to provide additional expertise where required.|a8f6bf44792a4326bead66a085e72072|||
commission's services|member of the board|In order to ensure that the members of the Board have the possibility to make informed analysis on the basis of factual evidence,|a8f6bf44792a4326bead66a085e72072|||
enforcement of the new regulation|risk identified|In order to ensure that the members of the Board have the possibility to make informed analysis on the basis of factual evidence, it is foreseen that the Board should be supported by the administrative structure of the Commission and that an expert group be created to provide additional expertise where required. For the meeting expenditure, given the low value per transaction (e.g. refunding travel costs for a delegate for a meeting), standard control procedures seem sufficient. Regarding the development of the database, contract attribution has a strong internal control system in place in DG CNECT through centralised procurement activities.|a8f6bf44792a4326bead66a085e72072|||
enforcement of the new regulation|internal control system(s)|In order to ensure that the members of the Board have the possibility to make informed analysis on the basis of factual evidence, it is foreseen that the Board should be supported by the administrative structure of the Commission and that an expert group be created to provide additional expertise where required. For the meeting expenditure, given the low value per transaction (e.g. refunding travel costs for a delegate for a meeting), standard control procedures seem sufficient. Regarding the development of the database, contract attribution has a strong internal control system in place in DG CNECT through centralised procurement activities.|a8f6bf44792a4326bead66a085e72072|||
enforcement of the new regulation|cost-effectiveness of controls|In order to ensure that the members of the Board have the possibility to make informed analysis on the basis of factual evidence, it is foreseen that the Board should be supported by the administrative structure of the Commission and that an expert group be created to provide additional expertise where required. For the meeting expenditure, given the low value per transaction (e.g. refunding travel costs for a delegate for a meeting), standard control procedures seem sufficient. Regarding the development of the database, contract attribution has a strong internal control system in place in DG CNECT through centralised procurement activities.|a8f6bf44792a4326bead66a085e72072|||
enforcement of the new regulation|expected levels of risk of error|In order to ensure that the members of the Board have the possibility to make informed analysis on the basis of factual evidence, it is foreseen that the Board should be supported by the administrative structure of the Commission and that an expert group be created to provide additional expertise where required. For the meeting expenditure, given the low value per transaction (e.g. refunding travel costs for a delegate for a meeting), standard control procedures seem sufficient. Regarding the development of the database, contract attribution has a strong internal control system in place in DG CNECT through centralised procurement activities.|a8f6bf44792a4326bead66a085e72072|||
commission's services|prevention and protection measures|Specify existing or envisaged prevention and protection measures, e.g. From the Anti-Fraud Strategy.|a8f6bf44792a4326bead66a085e72072|||
existing fraud prevention measures|commission|The existing fraud prevention measures applicable to the Commission will cover the additional appropriations necessary for this Regulation.|5d3434e74f7747bd892de5a0ed406860|||
multiannual financial framework headings|existing budget lines|In order of multiannual financial framework headings and budget lines.|5d3434e74f7747bd892de5a0ed406860|||1.0
heading(s) of the multiannual financial framework|existing budget lines|In order of multiannual financial framework headings and budget lines.|5d3434e74f7747bd892de5a0ed406860|||1.0
budget line|expenditure|Existing budget lines|5d3434e74f7747bd892de5a0ed406860|||1.0
contribution|existing budget lines|Existing budget lines|5d3434e74f7747bd892de5a0ed406860|||1.0
heading of the multiannual financial framework|efta|In order of multiannual financial framework headings and budget lines.|5d3434e74f7747bd892de5a0ed406860|||0.0
heading of the multiannual financial framework|financial framework|In order of multiannual financial framework headings and budget lines.|5d3434e74f7747bd892de5a0ed406860|||0.0
number|difference between number of countries belonging to the respective categories|Type of Budget line expenditure|5d3434e74f7747bd892de5a0ed406860|||1.0
financial regulation|existing budget lines|EU financial regulation|5d3434e74f7747bd892de5a0ed406860|||0.0
candidate|third countries|From third countries|5d3434e74f7747bd892de5a0ed406860|||1.0
meaning of|multiannual financial framework headings|From third countries|5d3434e74f7747bd892de5a0ed406860|||0.0
regulation|administrative expenditure|In context, the regulation is being discussed in relation to administrative expenditure as it may require or not require the use of operational appropriations. This implies that there might be a financial impact on administrative expenditure due to this regulation.|25f9cefc3c1e4a539238442c117ca5d0|||
dep artificial intelligence|digital europe programme|Both DEP Artificial Intelligence and Digital Europe programme are discussed in the same context, indicating that there might be a relation between them. However, no further information is provided about their relationship.|25f9cefc3c1e4a539238442c117ca5d0|||
efta|candidate countries and potential candidate countries from the western balkans|Both EFTA and Candidate countries and potential candidate countries from the Western Balkans are mentioned in the same context, suggesting that they might have some connection. Further details about their relationship remain unclear.|25f9cefc3c1e4a539238442c117ca5d0|||
heading of multiannual financial framework|year|The heading of multiannual financial framework and year are mentioned together, indicating that the regulation might be related to the allocation or distribution of funds in a multiannual financial framework over specific years.|25f9cefc3c1e4a539238442c117ca5d0|||
dg: cnect|total|Both DG: CNECT and TOTAL are discussed in the same context, possibly signifying that there might be a connection between them.|25f9cefc3c1e4a539238442c117ca5d0|||
operational appropriations|commitments|Financial relationship between operational appropriations and commitments, as mentioned in the context.|d3ed44fc1d174703bbeea592b435e1c0|||
commitments (1a)|payments (2a)|Flow of funds from commitments to payments, as shown in the context.|d3ed44fc1d174703bbeea592b435e1c0|||
budget line 02 04 03|operational appropriations|Association between a specific budget line and operational appropriations, as provided in the context.|d3ed44fc1d174703bbeea592b435e1c0|||
payments (2b)|appropriations of an administrative nature financed from the envelope of specific programmes|Flow of funds between payments and appropriations of an administrative nature, as mentioned in the context.|d3ed44fc1d174703bbeea592b435e1c0|||
budget line 02 01 30 01|appropriations of an administrative nature financed from the envelope of specific programmes|Relationship between a specific budget line and appropriations of an administrative nature, as shown in the context.|d3ed44fc1d174703bbeea592b435e1c0|||
total appropriations|operational appropriations + commitments + appropriations of an administrative nature financed from the envelope of specific programmes|Summation of operational appropriations, commitments, and appropriations of an administrative nature financed from the envelope of specific programmes, as explained in the context.|d3ed44fc1d174703bbeea592b435e1c0|||
payments|operational appropriations + payments (2a) + payments (2b)|Relationship between payments and operational appropriations along with payments for administrative expenses, as mentioned in the context.|d3ed44fc1d174703bbeea592b435e1c0|||
indicative and dependent on budget availability|70|Label used to indicate indicative and dependent on budget availability, which is associated with line number 70 in the context.|d3ed44fc1d174703bbeea592b435e1c0|||
according to the official budget nomenclature.|70|Label used to indicate conformity with the official budget classification, which is associated with line number 70 in the context.|d3ed44fc1d174703bbeea592b435e1c0|||
technical and/or administrative assistance and expenditure in support of the implementation of eu programmes and/or actions (former `ba' lines), indirect research, direct research.|71|Label used to describe technical and administrative expenses for implementing EU programmes and direct and indirect research, which is associated with line number 71 in the context.|d3ed44fc1d174703bbeea592b435e1c0|||
total operational appropriations|payments|under HEADING 1 of the multiannual financial framework, TOTAL operational appropriations and Payments are both included in the proposal/initiative.|4b4868942a9e4dc182c37b1e8d8df44e|||
total operational appropriations (all operational headings)|payments (5)|If more than one heading is affected by the proposal/initiative, repeat the section above: TOTAL operational appropriations from all operational headings and Payments from HEADING 1 are both part of the proposal/initiative.|4b4868942a9e4dc182c37b1e8d8df44e|||
total appropriations of an administrative nature financed from the envelope for specific programmes (all operational headings)|total appropriations (4)|under HEADINGS 1 to 6 of the multiannual financial framework, TOTAL appropriations from all operational headings and TOTAL appropriations of an administrative nature financed from the envelope for specific programmes are both included in the proposal/initiative.|4b4868942a9e4dc182c37b1e8d8df44e|||
total appropriations (4)|committments|Commitments and TOTAL appropriations from all operational headings are both part of the proposal/initiative.|4b4868942a9e4dc182c37b1e8d8df44e|||
envelope for specific programmes (all operational headings)|total appropriations of an administrative nature financed from|If more than one heading is affected by the proposal/initiative, repeat the section above: TOTAL appropriations of an administrative nature financed from the envelope for specific programmes and TOTAL appropriations of an administrative nature are both included in the proposal/initiative.|4b4868942a9e4dc182c37b1e8d8df44e|||
heading of multiannual financial framework|total operational appropriations (all operational headings)|under HEADINGS 1 to 6 of the multiannual financial framework, TOTAL operational appropriations from all operational headings and Heading of multiannual financial framework are both part of the proposal/initiative.|4b4868942a9e4dc182c37b1e8d8df44e|||
heading of multiannual financial framework|envelope for specific programmes (all operational headings)|under HEADINGS 1 to 6 of the multiannual financial framework, Heading of multiannual financial framework and Envelope for specific programmes from all operational headings are both included in the proposal/initiative.|4b4868942a9e4dc182c37b1e8d8df44e|||
heading of multiannual financial framework|total appropriations (4)|under HEADINGS 1 to 6 of the multiannual financial framework, Heading of multiannual financial framework and TOTAL appropriations from all operational headings are both part of the proposal/initiative.|4b4868942a9e4dc182c37b1e8d8df44e|||
heading of multiannual financial framework|total appropriations (4)|under HEADINGS 1 to 6 of the multiannual financial framework, Heading of multiannual financial framework and TOTAL appropriations of an administrative nature financed from the envelope for specific programmes are both included in the proposal/initiative.|4b4868942a9e4dc182c37b1e8d8df44e|||
heading of multiannual financial framework|administrative expenditure|The section in the given context is related to the concept 'administrative expenditure' which falls under the heading of multiannual financial framework.|8fbe77b31ab44727940ccd9eb8d02be6|||
year 2023|year 2024|Both years are mentioned in the given context and are related as they appear together in the same line indicating a consecutive time period.|8fbe77b31ab44727940ccd9eb8d02be6|||
year 2025|year 2026|Similar to the previous pair, both years are consecutively mentioned and hence related in the given context.|8fbe77b31ab44727940ccd9eb8d02be6|||
year 2027|total|The concept 'total' is calculated at the end of a specific year, here being year 2027. This indicates a relationship between the year and the total value.|8fbe77b31ab44727940ccd9eb8d02be6|||
dg: cnect|human resources|The concept 'human resources' is related to the Directorate-General (DG) CNECT as it indicates the allocation of personnel for this specific DG.|8fbe77b31ab44727940ccd9eb8d02be6|||
dg: cnect|other administrative expenditure|This expenditure falls under the Directorate-General (DG) CNECT as it is a part of the administrative expenses incurred by this specific DG.|8fbe77b31ab44727940ccd9eb8d02be6|||
european data protection supervisor|human resources|As seen previously, the concept 'human resources' is again related to another entity, i.e., the European Data Protection Supervisor.|8fbe77b31ab44727940ccd9eb8d02be6|||
european data protection supervisor|other administrative expenditure|The concept 'other administrative expenses' is related to the European Data Protection Supervisor as it falls under its administrative expenses.|8fbe77b31ab44727940ccd9eb8d02be6|||
total edps|total appropriations|Both terms represent the total expenditure for education and professional skills, and total appropriations respectively. These terms are related as they both refer to the financial aspects of a budget.|d581eceb30094e2da700ef3711939ec9|||
year 2026|year 2027|Both terms represent years in which expenditure is being made. These terms are related as they both refer to specific points in time.|d581eceb30094e2da700ef3711939ec9|||
year 2022|year 2027|Both terms represent years in which expenditure is being made. These terms are related as they both refer to specific points in time.|d581eceb30094e2da700ef3711939ec9|||
year 2024|year 2025|Both terms represent years in which expenditure is being made. These terms are related as they both refer to specific points in time.|d581eceb30094e2da700ef3711939ec9|||
headings 1 to 7|under headings 1 to 7|Both terms represent a grouping of specific items. These terms are related as they both refer to the categorization of expenditure.|d581eceb30094e2da700ef3711939ec9|||
en|en|Both terms represent the language in which the document is written. These terms are related as they both refer to the linguistic aspect of the document.|d581eceb30094e2da700ef3711939ec9|||
database|meetings- output|In this context, Database and Meetings- Output are both outputs of a specific objective (No 1). While the former refers to a database used for storing and managing data related to the objective, the latter refers to meetings held as part of executing the objective. These outputs are directly related as the meeting notes and decisions could potentially be recorded in the database.|9ab795bccb5a4ff08ca45774de5ca864|||
database|cost|The cost of maintaining and operating the Database is included in the total cost of this specific objective. Therefore, there is a financial relationship between Database and Cost.|9ab795bccb5a4ff08ca45774de5ca864|||
meetings- output|cost|Similarly, the cost of organizing and conducting meetings falls under the total cost of this specific objective. Hence, there is a financial relationship between Meetings- Output and Cost.|9ab795bccb5a4ff08ca45774de5ca864|||
human resources|heading 7 of the multiannual financial framework|Human resources are a part of Heading 7 of the multiannual financial framework.|2f806145f2af49459be8193910df388c|||
heading 7 of the multiannual financial framework|subtotal heading 7 of the multiannual financial framework|HEADING 7 of the multiannual financial framework has a subtotal named 'Subtotal HEADING 7'.|2f806145f2af49459be8193910df388c|||
heading 7 of the multiannual financial framework|total|The total amount required for Heading 7 of the multiannual financial framework is named 'TOTAL'.|2f806145f2af49459be8193910df388c|||
human resources|outside heading 7 of the multiannual financial framework|Human resources can also be found outside Heading 7 of the multiannual financial framework.|2f806145f2af49459be8193910df388c|||
outside heading 7 of the multiannual financial framework|subtotal|There is a subtotal outside Heading 7 of the multiannual financial framework.|2f806145f2af49459be8193910df388c|||
outside heading 7 of the multiannual financial framework|total|The total amount required for expenditure outside Heading 7 of the multiannual financial framework is named 'TOTAL'.|2f806145f2af49459be8193910df388c|||
human resources|other administrative expenditure|Human resources are also a part of other administrative expenditure.|2f806145f2af49459be8193910df388c|||
establishment plan posts|delegations|The Establishment plan posts in the years 2023-2025 include three posts in Delegations.|8b91690ecc3b4e62a12486971e888ddb|||
establishment plan posts|headquarters and commission's representation offices|The Establishment plan posts in the years 2023-2025 include a total of ten posts in Headquarters and Commission's Representation offices.|8b91690ecc3b4e62a12486971e888ddb|||
establishment plan posts|indirect research|The Establishment plan posts in the years 2023-2025 include one post for Indirect research.|8b91690ecc3b4e62a12486971e888ddb|||
establishment plan posts|direct research|The Establishment plan posts in the years 2023-2025 include eleven posts for Direct research.|8b91690ecc3b4e62a12486971e888ddb|||
external staff (fte)|ac, end, int from the `global envelope'|The External staff in Full Time Equivalent unit under AC, END, INT from the `global envelope' for the years 2023-2025 is two.|8b91690ecc3b4e62a12486971e888ddb|||
external staff (fte)|ac, al, end, int and jpd in the delegations|The External staff in Full Time Equivalent unit under AC, AL, END, INT and JPD in the delegations for the years 2023-2025 is three.|8b91690ecc3b4e62a12486971e888ddb|||
edps|european data protection supervisor|identity relationship|b3336401c80e473fa296ca3cd645e567|||
edps|future amendments of the list of high-risk ai applications|involvement in policy work|b3336401c80e473fa296ca3cd645e567|||
edps|eu institutions' ai systems|responsibility for data protection|b3336401c80e473fa296ca3cd645e567|||
dg|management of the action|associated with implementing the action|b3336401c80e473fa296ca3cd645e567|||
dg|staff from the dg|providing resources required for the action|b3336401c80e473fa296ca3cd645e567|||
xx|policy area or budget title concerned|related to policy area or budget title|b3336401c80e473fa296ca3cd645e567|||
ad fte|officials and temporary staff|job positions required for the tasks|b3336401c80e473fa296ca3cd645e567|||
ast fte|officials and temporary staff|job positions required for the tasks|b3336401c80e473fa296ca3cd645e567|||
ac|delegations|part of delegations|b3336401c80e473fa296ca3cd645e567|||
al|delegations|part of delegations|b3336401c80e473fa296ca3cd645e567|||
end|delegations|part of delegations|b3336401c80e473fa296ca3cd645e567|||
int|ac, al, end, jpd in the delegations|related to indirect and direct research|b3336401c80e473fa296ca3cd645e567|||
jpd|delegations|part of delegations|b3336401c80e473fa296ca3cd645e567|||
79|location or place|related to the location at Headquarters|b3336401c80e473fa296ca3cd645e567|||
xx xx xx|budget line|related to budgetary allocation|b3336401c80e473fa296ca3cd645e567|||
edps responsibilites|draft legislation|are reuqired to fulfill the EDPS responsibilites under the draft legislation|94b2179ae08f4b028112d8b30c77815d|||
all figures|programmes|indicative and subject to the continuation of the programmes and availability of appropriations.|94b2179ae08f4b028112d8b30c77815d|||
ac|int|Contract Staff and agency staff|94b2179ae08f4b028112d8b30c77815d|||
al|int||94b2179ae08f4b028112d8b30c77815d|||
end|int||94b2179ae08f4b028112d8b30c77815d|||
jpd|int||94b2179ae08f4b028112d8b30c77815d|||
sub-ceiling for external staff covered by operational appropriations |en||94b2179ae08f4b028112d8b30c77815d|||
3.2.4.|x|can be fully financed through redeployment within the relevant heading of the Multiannual Financial Framework (MFF).|94b2179ae08f4b028112d8b30c77815d|||
3.2.4.|use of the unallocated margin under the relevant heading of the mff and/or use of the special instruments as defined in the mff regulation.||94b2179ae08f4b028112d8b30c77815d|||
3.2.4.|requires a revision of the mff.||94b2179ae08f4b028112d8b30c77815d|||
3.2.5.|x|does not provide for co-financing by third parties|94b2179ae08f4b028112d8b30c77815d|||
co-financing|third parties|The proposal/initiative provides for the co-financing by third parties estimated below.|82da033a783149bc9404afdbc7cda180|||
co-financing|n+1|The proposal/initiative provides for the co-financing by third parties estimated below for Year N+1.|82da033a783149bc9404afdbc7cda180|||
co-financing|n+2|The proposal/initiative provides for the co-financing by third parties estimated below for Year N+2.|82da033a783149bc9404afdbc7cda180|||
co-financing|n+3|The proposal/initiative provides for the co-financing by third parties estimated below for Year N+3.|82da033a783149bc9404afdbc7cda180|||
impact|revenue|The proposal/initiative has the following financial impact: (for instance: 2021).|82da033a783149bc9404afdbc7cda180|||
impact|other revenue|The proposal/initiative has the following financial impact:|82da033a783149bc9404afdbc7cda180|||
impact|other revenue|The proposal/initiative has the following financial impact:|82da033a783149bc9404afdbc7cda180|||
budget revenue line|financial year|Each budget revenue line corresponds to a specific financial year.|65ff964c1b064ce2b7f9930ae67c320a|||
n|current financial year|The current financial year is represented by 'N' in the budget revenue line.|65ff964c1b064ce2b7f9930ae67c320a|||
n|financial year n+1|The financial year after the current financial year is represented by 'N+1' in the budget revenue line.|65ff964c1b064ce2b7f9930ae67c320a|||
n|financial year n+2|The financial year two years after the current financial year is represented by 'N+2' in the budget revenue line.|65ff964c1b064ce2b7f9930ae67c320a|||
n|financial year n+3|The financial year three years after the current financial year is represented by 'N+3' in the budget revenue line. This duration is used to show the duration of the impact.|65ff964c1b064ce2b7f9930ae67c320a|||
article|budget expenditure line(s)|For assigned revenue, specify the budget expenditure line(s) affected by the article.|65ff964c1b064ce2b7f9930ae67c320a|||
